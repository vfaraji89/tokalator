{
  "articles": [
    {
      "slug": "arxiv-engineering-tagging-languages-for-dsls",
      "title": "Engineering Tagging Languages for DSLs",
      "description": "To keep a DSL clean, readable and reusable in different contexts, it is useful to define a separate tagging language. A tag model logically adds information to the tagged DSL model while technically...",
      "source": "arxiv",
      "sourceLabel": "arXiv",
      "sourceColor": "#b31b1b",
      "url": "https://arxiv.org/abs/1606.05112v1",
      "content": "## Abstract\n\nTo keep a DSL clean, readable and reusable in different contexts, it is useful to define a separate tagging language. A tag model logically adds information to the tagged DSL model while technically keeping the artifacts separated. Using a generic tagging language leads to promiscuous tag models, whereas defining a target DSL-specific tag language has a high initial overhead. This paper presents a systematic approach to define a DSL-specific tag language and a corresponding schema language, combining the advantages of both worlds: (a) the tag language specifically fits to the DSL, (b) the artifacts are kept separated and enabling reuse with different tag decorations, (c) the tag language follows a defined type schema, and (d) systematic derivation considerably reduces the effort necessary to implement the tag language. An example shows that it can at least partially be realized by a generator and applied for any kind of DSL. Index Terms Software Engineering, Modeling, MDE, GSE",
      "authors": [
        "Timo Greifenberg",
        "Markus Look",
        "Sebastian Roidl",
        "Bernhard Rumpe"
      ],
      "date": "2016-06-16",
      "tags": [
        "cs.SE"
      ],
      "category": "general"
    },
    {
      "slug": "arxiv-data-engineering-for-scaling-language-models-to-128k-context",
      "title": "Data Engineering for Scaling Language Models to 128K Context",
      "description": "We study the continual pretraining recipe for scaling language models' context lengths to 128K, with a focus on data engineering. We hypothesize that long context modeling, in particular \\textit{the...",
      "source": "arxiv",
      "sourceLabel": "arXiv",
      "sourceColor": "#b31b1b",
      "url": "https://arxiv.org/abs/2402.10171v1",
      "content": "## Abstract\n\nWe study the continual pretraining recipe for scaling language models' context lengths to 128K, with a focus on data engineering. We hypothesize that long context modeling, in particular \\textit{the ability to utilize information at arbitrary input locations}, is a capability that is mostly already acquired through large-scale pretraining, and that this capability can be readily extended to contexts substantially longer than seen during training~(e.g., 4K to 128K) through lightweight continual pretraining on appropriate data mixture. We investigate the \\textit{quantity} and \\textit{quality} of the data for continual pretraining: (1) for quantity, we show that 500 million to 5 billion tokens are enough to enable the model to retrieve information anywhere within the 128K context; (2) for quality, our results equally emphasize \\textit{domain balance} and \\textit{length upsampling}. Concretely, we find that naively upsampling longer data on certain domains like books, a common practice of existing work, gives suboptimal performance, and that a balanced domain mixture is important. We demonstrate that continual pretraining of the full model on 1B-5B tokens of such data is an effective and affordable strategy for scaling the context length of language models to 128K. Our recipe outperforms strong open-source long-context models and closes the gap to frontier models like GPT-4 128K.",
      "authors": [
        "Yao Fu",
        "Rameswar Panda",
        "Xinyao Niu",
        "Xiang Yue",
        "Hannaneh Hajishirzi"
      ],
      "date": "2024-02-15",
      "tags": [
        "cs.CL",
        "cs.AI"
      ],
      "category": "token-optimization"
    },
    {
      "slug": "arxiv-learning-from-failure-integrating-negative-examples-when-fine-tuning-large-langu",
      "title": "Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents",
      "description": "Large language models (LLMs) have achieved success in acting as agents, which interact with environments through tools such as search engines. However, LLMs are optimized for language generation...",
      "source": "arxiv",
      "sourceLabel": "arXiv",
      "sourceColor": "#b31b1b",
      "url": "https://arxiv.org/abs/2402.11651v2",
      "content": "## Abstract\n\nLarge language models (LLMs) have achieved success in acting as agents, which interact with environments through tools such as search engines. However, LLMs are optimized for language generation instead of tool use during training or alignment, limiting their effectiveness as agents. To resolve this problem, previous work has first collected interaction trajectories between LLMs and environments, using only trajectories that successfully finished the task to fine-tune smaller models, making fine-tuning data scarce and acquiring it both difficult and costly. Discarding failed trajectories also leads to significant wastage of data and resources and limits the possible optimization paths during fine-tuning. In this paper, we argue that unsuccessful trajectories offer valuable insights, and LLMs can learn from these trajectories through appropriate quality control and fine-tuning strategies. By simply adding a prefix or suffix that tells the model whether to generate a successful trajectory during training, we improve model performance by a large margin on mathematical reasoning, multi-hop question answering, and strategic question answering tasks. We further analyze the inference results and find that our method provides a better trade-off between valuable information and errors in unsuccessful trajectories. To our knowledge, we are the first to demonstrate the value of negative trajectories and their application in agent-tunning scenarios. Our findings offer guidance for developing better agent-tuning methods and low-resource data usage techniques.",
      "authors": [
        "Renxi Wang",
        "Haonan Li",
        "Xudong Han",
        "Yixuan Zhang",
        "Timothy Baldwin"
      ],
      "date": "2024-02-18",
      "tags": [
        "cs.CL"
      ],
      "category": "tool-use"
    },
    {
      "slug": "arxiv-how-important-is-tokenization-in-french-medical-masked-language-models",
      "title": "How Important Is Tokenization in French Medical Masked Language Models?",
      "description": "Subword tokenization has become the prevailing standard in the field of natural language processing (NLP) over recent years, primarily due to the widespread utilization of pre-trained language...",
      "source": "arxiv",
      "sourceLabel": "arXiv",
      "sourceColor": "#b31b1b",
      "url": "https://arxiv.org/abs/2402.15010v2",
      "content": "## Abstract\n\nSubword tokenization has become the prevailing standard in the field of natural language processing (NLP) over recent years, primarily due to the widespread utilization of pre-trained language models. This shift began with Byte-Pair Encoding (BPE) and was later followed by the adoption of SentencePiece and WordPiece. While subword tokenization consistently outperforms character and word-level tokenization, the precise factors contributing to its success remain unclear. Key aspects such as the optimal segmentation granularity for diverse tasks and languages, the influence of data sources on tokenizers, and the role of morphological information in Indo-European languages remain insufficiently explored. This is particularly pertinent for biomedical terminology, characterized by specific rules governing morpheme combinations. Despite the agglutinative nature of biomedical terminology, existing language models do not explicitly incorporate this knowledge, leading to inconsistent tokenization strategies for common terms. In this paper, we seek to delve into the complexities of subword tokenization in French biomedical domain across a variety of NLP tasks and pinpoint areas where further enhancements can be made. We analyze classical tokenization algorithms, including BPE and SentencePiece, and introduce an original tokenization strategy that integrates morpheme-enriched word segmentation into existing tokenization methods.",
      "authors": [
        "Yanis Labrak",
        "Adrien Bazoge",
        "Beatrice Daille",
        "Mickael Rouvier",
        "Richard Dufour"
      ],
      "date": "2024-02-22",
      "tags": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "category": "token-optimization"
    },
    {
      "slug": "arxiv-token-weighting-for-long-range-language-modeling",
      "title": "Token Weighting for Long-Range Language Modeling",
      "description": "Many applications of large language models (LLMs) require long-context understanding, but models continue to struggle with such tasks. We hypothesize that conventional next-token prediction training...",
      "source": "arxiv",
      "sourceLabel": "arXiv",
      "sourceColor": "#b31b1b",
      "url": "https://arxiv.org/abs/2503.09202v1",
      "content": "## Abstract\n\nMany applications of large language models (LLMs) require long-context understanding, but models continue to struggle with such tasks. We hypothesize that conventional next-token prediction training could contribute to this, because each token is assigned equal weight. Yet, intuitively, the amount of context needed to predict the next token accurately varies greatly across different data. To reflect this, we propose various novel token-weighting schemes that assign different weights to each training token in the loss, thereby generalizing existing works. For this, we categorize token-weighting methods using a two-step framework which compares the confidences of a long-context and short-context model to score tokens. We evaluate all methods on multiple long-context understanding tasks and show that non-uniform loss weights are helpful to improve the long-context abilities of LLMs. Different short-context models can be used effectively for token scoring, including models that are much smaller than the long-context model that is trained. All in all, this work contributes to a better understanding of the trade-offs long-context language modeling faces and provides guidelines for model steering via loss-weighting based on empirical evidence. The code can be found on Github.",
      "authors": [
        "Falko Helm",
        "Nico Daheim",
        "Iryna Gurevych"
      ],
      "date": "2025-03-12",
      "tags": [
        "cs.CL"
      ],
      "category": "general"
    },
    {
      "slug": "arxiv-on-the-solution-existence-and-stability-of-polynomial-optimization-problems",
      "title": "On the solution existence and stability of polynomial optimization problems",
      "description": "This paper introduces and investigates a regularity condition in the asymptotic sense for optimization problems whose objective functions are polynomial. Under this regularity condition, the...",
      "source": "arxiv",
      "sourceLabel": "arXiv",
      "sourceColor": "#b31b1b",
      "url": "https://arxiv.org/abs/1808.06100v6",
      "content": "## Abstract\n\nThis paper introduces and investigates a regularity condition in the asymptotic sense for optimization problems whose objective functions are polynomial. Under this regularity condition, the normalization argument in asymptotic analysis enables us to see the solution existence as well as the solution stability of these problems. We prove a Frank-Wolfe type theorem for regular optimization problems and an Eaves type theorem for non-regular pseudoconvex optimization problems. Moreover, we show results on the stability such as upper semicontinuity and local upper-Hölder stability of the solution map of polynomial optimization problems. At the end of the paper, we discuss the genericity of the regularity condition.",
      "authors": [
        "Vu Trung Hieu"
      ],
      "date": "2018-08-18",
      "tags": [
        "math.OC"
      ],
      "category": "general"
    },
    {
      "slug": "arxiv-caching-with-rental-cost-and-zapping",
      "title": "Caching with rental cost and zapping",
      "description": "The \\emph{file caching} problem is defined as follows. Given a cache of size $k$ (a positive integer), the goal is to minimize the total retrieval cost for the given sequence of requests to files. A...",
      "source": "arxiv",
      "sourceLabel": "arXiv",
      "sourceColor": "#b31b1b",
      "url": "https://arxiv.org/abs/1208.2724v5",
      "content": "## Abstract\n\nThe \\emph{file caching} problem is defined as follows. Given a cache of size $k$ (a positive integer), the goal is to minimize the total retrieval cost for the given sequence of requests to files. A file $f$ has size $size(f)$ (a positive integer) and retrieval cost $cost(f)$ (a non-negative number) for bringing the file into the cache. A \\emph{miss} or \\emph{fault} occurs when the requested file is not in the cache and the file has to be retrieved into the cache by paying the retrieval cost, and some other file may have to be removed (\\emph{evicted}) from the cache so that the total size of the files in the cache does not exceed $k$. We study the following variants of the online file caching problem. \\textbf{\\emph{Caching with Rental Cost} (or \\emph{Rental Caching})}: There is a rental cost $λ$ (a positive number) for each file in the cache at each time unit. The goal is to minimize the sum of the retrieval costs and the rental costs. \\textbf{\\emph{Caching with Zapping}}: A file can be \\emph{zapped} by paying a zapping cost $N \\ge 1$. Once a file is zapped, all future requests of the file don't incur any cost. The goal is to minimize the sum of the retrieval costs and the zapping costs. We study these two variants and also the variant which combines these two (rental caching with zapping). We present deterministic lower and upper bounds in the competitive-analysis framework. We study and extend the online covering algorithm from \\citep{young02online} to give deterministic online algorithms. We also present randomized lower and upper bounds for some of these problems.",
      "authors": [
        "Monik Khare",
        "Neal E. Young"
      ],
      "date": "2012-08-13",
      "tags": [
        "cs.DS"
      ],
      "category": "caching"
    },
    {
      "slug": "arxiv-struq-defending-against-prompt-injection-with-structured-queries",
      "title": "StruQ: Defending Against Prompt Injection with Structured Queries",
      "description": "Recent advances in Large Language Models (LLMs) enable exciting LLM-integrated applications, which perform text-based tasks by utilizing their advanced language understanding capabilities. However,...",
      "source": "arxiv",
      "sourceLabel": "arXiv",
      "sourceColor": "#b31b1b",
      "url": "https://arxiv.org/abs/2402.06363v2",
      "content": "## Abstract\n\nRecent advances in Large Language Models (LLMs) enable exciting LLM-integrated applications, which perform text-based tasks by utilizing their advanced language understanding capabilities. However, as LLMs have improved, so have the attacks against them. Prompt injection attacks are an important threat: they trick the model into deviating from the original application's instructions and instead follow user directives. These attacks rely on the LLM's ability to follow instructions and inability to separate prompts and user data. We introduce structured queries, a general approach to tackle this problem. Structured queries separate prompts and data into two channels. We implement a system that supports structured queries. This system is made of (1) a secure front-end that formats a prompt and user data into a special format, and (2) a specially trained LLM that can produce high-quality outputs from these inputs. The LLM is trained using a novel fine-tuning strategy: we convert a base (non-instruction-tuned) LLM to a structured instruction-tuned model that will only follow instructions in the prompt portion of a query. To do so, we augment standard instruction tuning datasets with examples that also include instructions in the data portion of the query, and fine-tune the model to ignore these. Our system significantly improves resistance to prompt injection attacks, with little or no impact on utility. Our code is released at https://github.com/Sizhe-Chen/StruQ.",
      "authors": [
        "Sizhe Chen",
        "Julien Piet",
        "Chawin Sitawarin",
        "David Wagner"
      ],
      "date": "2024-02-09",
      "tags": [
        "cs.CR"
      ],
      "category": "prompt-engineering"
    },
    {
      "slug": "arxiv-secalign-defending-against-prompt-injection-with-preference-optimization",
      "title": "SecAlign: Defending Against Prompt Injection with Preference Optimization",
      "description": "Large language models (LLMs) are becoming increasingly prevalent in modern software systems, interfacing between the user and the Internet to assist with tasks that require advanced language...",
      "source": "arxiv",
      "sourceLabel": "arXiv",
      "sourceColor": "#b31b1b",
      "url": "https://arxiv.org/abs/2410.05451v3",
      "content": "## Abstract\n\nLarge language models (LLMs) are becoming increasingly prevalent in modern software systems, interfacing between the user and the Internet to assist with tasks that require advanced language understanding. To accomplish these tasks, the LLM often uses external data sources such as user documents, web retrieval, results from API calls, etc. This opens up new avenues for attackers to manipulate the LLM via prompt injection. Adversarial prompts can be injected into external data sources to override the system's intended instruction and instead execute a malicious instruction. To mitigate this vulnerability, we propose a new defense called SecAlign based on the technique of preference optimization. Our defense first constructs a preference dataset with prompt-injected inputs, secure outputs (ones that respond to the legitimate instruction), and insecure outputs (ones that respond to the injection). We then perform preference optimization on this dataset to teach the LLM to prefer the secure output over the insecure one. This provides the first known method that reduces the success rates of various prompt injections to &lt;10%, even against attacks much more sophisticated than ones seen during training. This indicates our defense generalizes well against unknown and yet-to-come attacks. Also, SecAlign models are still practical with similar utility to the one before defensive training in our evaluations. Our code is at https://github.com/facebookresearch/SecAlign",
      "authors": [
        "Sizhe Chen",
        "Arman Zharmagambetov",
        "Saeed Mahloujifar",
        "Kamalika Chaudhuri",
        "David Wagner"
      ],
      "date": "2024-10-07",
      "tags": [
        "cs.CR",
        "cs.LG"
      ],
      "category": "rag"
    },
    {
      "slug": "arxiv-exploiting-context-to-identify-lexical-atoms-a-statistical-view-of-linguistic-co",
      "title": "Exploiting Context to Identify Lexical Atoms -- A Statistical View of Linguistic Context",
      "description": "Interpretation of natural language is inherently context-sensitive. Most words in natural language are ambiguous and their meanings are heavily dependent on the linguistic context in which they are...",
      "source": "arxiv",
      "sourceLabel": "arXiv",
      "sourceColor": "#b31b1b",
      "url": "https://arxiv.org/abs/cmp-lg/9701001v1",
      "content": "## Abstract\n\nInterpretation of natural language is inherently context-sensitive. Most words in natural language are ambiguous and their meanings are heavily dependent on the linguistic context in which they are used. The study of lexical semantics can not be separated from the notion of context. This paper takes a contextual approach to lexical semantics and studies the linguistic context of lexical atoms, or \"sticky\" phrases such as \"hot dog\". Since such lexical atoms may occur frequently in unrestricted natural language text, recognizing them is crucial for understanding naturally-occurring text. The paper proposes several heuristic approaches to exploiting the linguistic context to identify lexical atoms from arbitrary natural language text.",
      "authors": [
        "Chengxiang Zhai"
      ],
      "date": "1997-01-02",
      "tags": [
        "cs.CL"
      ],
      "category": "general"
    },
    {
      "slug": "arxiv-towards-adaptive-context-management-for-intelligent-conversational-question-answ",
      "title": "Towards Adaptive Context Management for Intelligent Conversational Question Answering",
      "description": "This particular paper introduces an Adaptive Context Management (ACM) framework for the Conversational Question Answering (ConvQA) systems. The key objective of the ACM framework is to optimize the...",
      "source": "arxiv",
      "sourceLabel": "arXiv",
      "sourceColor": "#b31b1b",
      "url": "https://arxiv.org/abs/2509.17829v1",
      "content": "## Abstract\n\nThis particular paper introduces an Adaptive Context Management (ACM) framework for the Conversational Question Answering (ConvQA) systems. The key objective of the ACM framework is to optimize the use of the conversation history by dynamically managing context for maximizing the relevant information provided to a ConvQA model within its token limit. Our approach incorporates a Context Manager (CM) Module, a Summarization (SM) Module, and an Entity Extraction (EE) Module in a bid to handle the conversation history efficaciously. The CM Module dynamically adjusts the context size, thereby preserving the most relevant and recent information within a model's token limit. The SM Module summarizes the older parts of the conversation history via a sliding window. When the summarization window exceeds its limit, the EE Module identifies and retains key entities from the oldest conversation turns. Experimental results demonstrate the effectiveness of our envisaged framework in generating accurate and contextually appropriate responses, thereby highlighting the potential of the ACM framework to enhance the robustness and scalability of the ConvQA systems.",
      "authors": [
        "Manoj Madushanka Perera",
        "Adnan Mahmood",
        "Kasun Eranda Wijethilake",
        "Quan Z. Sheng"
      ],
      "date": "2025-09-22",
      "tags": [
        "cs.CL"
      ],
      "category": "token-optimization"
    },
    {
      "slug": "arxiv-impacts-of-national-cultures-on-managerial-decisions-of-engaging-in-core-earning",
      "title": "Impacts of National Cultures on Managerial Decisions of Engaging in Core Earnings Management",
      "description": "This study investigates the impact of Hofstede's cultural dimensions on abnormal core earnings management in multiple national cultural contexts. We employ an Ordinary Least Squares (OLS) regression...",
      "source": "arxiv",
      "sourceLabel": "arXiv",
      "sourceColor": "#b31b1b",
      "url": "https://arxiv.org/abs/2407.16854v1",
      "content": "## Abstract\n\nThis study investigates the impact of Hofstede's cultural dimensions on abnormal core earnings management in multiple national cultural contexts. We employ an Ordinary Least Squares (OLS) regression model with abnormal core earnings as the dependent variable. The independent variables analyzed include Hofstede's dimensions: Power Distance Index (PDI), Individualism (IDV), Masculinity (MAS), and Uncertainty Avoidance Index (UAI). Our findings reveal that individualism is positively associated with abnormal core earnings, suggesting that cultures characterized by high individualism may encourage practices that inflate earnings due to the prominence of personal achievement and rewards. In contrast, masculinity negatively correlates with abnormal core earnings, indicating that the risk-taking attributes associated with masculine cultures may deter earnings management. Interestingly, uncertainty avoidance is positively linked to abnormal core earnings, supporting the notion that managers tend to engage more in earnings management to minimize fluctuations in financial reports in cultures with high uncertainty avoidance. The relationship between power distance and abnormal core earnings is found to be non-significant, indicating no substantial effect in this context. These findings contribute to the literature on cultural influences in financial reporting, providing valuable insights for policymakers and multinational firms concerning the cultural contexts within which financial decisions and reporting occur.",
      "authors": [
        "Muhammad Rofiqul Islam",
        "Abdullah Al Mehdi"
      ],
      "date": "2024-07-23",
      "tags": [
        "econ.GN"
      ],
      "category": "rag"
    },
    {
      "slug": "arxiv-evor-evolving-retrieval-for-code-generation",
      "title": "EVOR: Evolving Retrieval for Code Generation",
      "description": "Recently the retrieval-augmented generation (RAG) has been successfully applied in code generation. However, existing pipelines for retrieval-augmented code generation (RACG) employ static knowledge...",
      "source": "arxiv",
      "sourceLabel": "arXiv",
      "sourceColor": "#b31b1b",
      "url": "https://arxiv.org/abs/2402.12317v2",
      "content": "## Abstract\n\nRecently the retrieval-augmented generation (RAG) has been successfully applied in code generation. However, existing pipelines for retrieval-augmented code generation (RACG) employ static knowledge bases with a single source, limiting the adaptation capabilities of Large Language Models (LLMs) to domains they have insufficient knowledge of. In this work, we develop a novel pipeline, EVOR, that employs the synchronous evolution of both queries and diverse knowledge bases. On two realistic settings where the external knowledge is required to solve code generation tasks, we compile four new datasets associated with frequently updated libraries and long-tail programming languages, named EVOR-BENCH. Extensive experiments demonstrate that EVOR achieves two to four times of execution accuracy compared to other methods such as Reflexion (Shinn et al., 2024), DocPrompting (Zhou et al., 2023), etc. We demonstrate that EVOR is flexible and can be easily combined with them to achieve further improvement. Further analysis reveals that EVOR benefits from the synchronous evolution of queries and documents and the diverse information sources in the knowledge base. We hope that our studies will inspire more insights into the design of advanced RACG pipelines in future research. Our model, code, and data are available at https://arks-codegen.github.io.",
      "authors": [
        "Hongjin Su",
        "Shuyang Jiang",
        "Yuhang Lai",
        "Haoyuan Wu",
        "Boao Shi"
      ],
      "date": "2024-02-19",
      "tags": [
        "cs.CL",
        "cs.AI"
      ],
      "category": "rag"
    },
    {
      "slug": "arxiv-ar-rag-autoregressive-retrieval-augmentation-for-image-generation",
      "title": "AR-RAG: Autoregressive Retrieval Augmentation for Image Generation",
      "description": "We introduce Autoregressive Retrieval Augmentation (AR-RAG), a novel paradigm that enhances image generation by autoregressively incorporating knearest neighbor retrievals at the patch level. Unlike...",
      "source": "arxiv",
      "sourceLabel": "arXiv",
      "sourceColor": "#b31b1b",
      "url": "https://arxiv.org/abs/2506.06962v3",
      "content": "## Abstract\n\nWe introduce Autoregressive Retrieval Augmentation (AR-RAG), a novel paradigm that enhances image generation by autoregressively incorporating knearest neighbor retrievals at the patch level. Unlike prior methods that perform a single, static retrieval before generation and condition the entire generation on fixed reference images, AR-RAG performs context-aware retrievals at each generation step, using prior-generated patches as queries to retrieve and incorporate the most relevant patch-level visual references, enabling the model to respond to evolving generation needs while avoiding limitations (e.g., over-copying, stylistic bias, etc.) prevalent in existing methods. To realize AR-RAG, we propose two parallel frameworks: (1) Distribution-Augmentation in Decoding (DAiD), a training-free plug-and-use decoding strategy that directly merges the distribution of model-predicted patches with the distribution of retrieved patches, and (2) Feature-Augmentation in Decoding (FAiD), a parameter-efficient fine-tuning method that progressively smooths the features of retrieved patches via multi-scale convolution operations and leverages them to augment the image generation process. We validate the effectiveness of AR-RAG on widely adopted benchmarks, including Midjourney-30K, GenEval and DPG-Bench, demonstrating significant performance gains over state-of-the-art image generation models.",
      "authors": [
        "Jingyuan Qi",
        "Zhiyang Xu",
        "Qifan Wang",
        "Lifu Huang"
      ],
      "date": "2025-06-08",
      "tags": [
        "cs.CV"
      ],
      "category": "rag"
    },
    {
      "slug": "arxiv-intelligent-interaction-strategies-for-context-aware-cognitive-augmentation",
      "title": "Intelligent Interaction Strategies for Context-Aware Cognitive Augmentation",
      "description": "Human cognition is constrained by processing limitations, leading to cognitive overload and inefficiencies in knowledge synthesis and decision-making. Large Language Models (LLMs) present an...",
      "source": "arxiv",
      "sourceLabel": "arXiv",
      "sourceColor": "#b31b1b",
      "url": "https://arxiv.org/abs/2504.13684v1",
      "content": "## Abstract\n\nHuman cognition is constrained by processing limitations, leading to cognitive overload and inefficiencies in knowledge synthesis and decision-making. Large Language Models (LLMs) present an opportunity for cognitive augmentation, but their current reactive nature limits their real-world applicability. This position paper explores the potential of context-aware cognitive augmentation, where LLMs dynamically adapt to users' cognitive states and task environments to provide appropriate support. Through a think-aloud study in an exhibition setting, we examine how individuals interact with multi-modal information and identify key cognitive challenges in structuring, retrieving, and applying knowledge. Our findings highlight the need for AI-driven cognitive support systems that integrate real-time contextual awareness, personalized reasoning assistance, and socially adaptive interactions. We propose a framework for AI augmentation that seamlessly transitions between real-time cognitive support and post-experience knowledge organization, contributing to the design of more effective human-centered AI systems.",
      "authors": [
        "Xiangrong",
        "Zhu",
        "Yuan Xu",
        "Tianjian Liu",
        "Jingwei Sun"
      ],
      "date": "2025-04-18",
      "tags": [
        "cs.HC"
      ],
      "category": "general"
    },
    {
      "slug": "openai-how-to-work-with-large-language-models",
      "title": "How to work with large language models",
      "description": "[Large language models][Large language models Blog Post] are functions that map text to text. Given an input string of text, a large language model predicts the text that should come next.",
      "source": "openai",
      "sourceLabel": "OpenAI Cookbook",
      "sourceColor": "#10a37f",
      "url": "https://github.com/openai/openai-cookbook/blob/main/articles/how_to_work_with_large_language_models.md",
      "content": "# How to work with large language models\n\n## How large language models work\n\n[Large language models][Large language models Blog Post] are functions that map text to text. Given an input string of text, a large language model predicts the text that should come next.\n\nThe magic of large language models is that by being trained to minimize this prediction error over vast quantities of text, the models end up learning concepts useful for these predictions. For example, they learn:\n\n- how to spell\n- how grammar works\n- how to paraphrase\n- how to answer questions\n- how to hold a conversation\n- how to write in many languages\n- how to code\n- etc.\n\nThey do this by “reading” a large amount of existing text and learning how words tend to appear in context with other words, and uses what it has learned to predict the next most likely word that might appear in response to a user request, and each subsequent word after that.\n\nGPT-3 and GPT-4 power [many software products][OpenAI Customer Stories], including productivity apps, education apps, games, and more.\n\n## How to control a large language model\n\nOf all the inputs to a large language model, by far the most influential is the text prompt.\n\nLarge language models can be prompted to produce output in a few ways:\n\n- **Instruction**: Tell the model what you want\n- **Completion**: Induce the model to complete the beginning of what you want\n- **Scenario**: Give the model a situation to play out\n- **Demonstration**: Show the model what you want, with either:\n  - A few examples in the prompt\n  - Many hundreds or thousands of examples in a fine-tuning training dataset\n\nAn example of each is shown below.\n\n### Instruction prompts\n\nWrite your instruction at the top of the prompt (or at the bottom, or both), and the model will do its best to follow the instruction and then stop. Instructions can be detailed, so don't be afraid to write a paragraph explicitly detailing the output you want, just stay aware of how many [tokens](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them) the model can process.\n\nExample instruction prompt:\n\n```text\nExtract the name of the author from the quotation below.\n\n“Some humans theorize that intelligent species go extinct before they can expand into outer space. If they're correct, then the hush of the night sky is the silence of the graveyard.”\n― Ted Chiang, Exhalation\n```\n\nOutput:\n\n```text\nTed Chiang\n```\n\n### Completion prompt example\n\nCompletion-style prompts take advantage of how large language models try to write text they think is most likely to come next. To steer the model, try beginning a pattern or sentence that will be completed by the output you want to see. Relative to direct instructions, this mode of steering large language models can take more care and experimentation. In addition, the models won't necessarily know where to stop, so you will often need stop sequences or post-processing to cut off text generated beyond the desired output.\n\nExample completion prompt:\n\n```text\n“Some humans theorize that intelligent species go extinct before they can expand into outer space. If they're correct, then the hush of the night sky is the silence of the graveyard.”\n― Ted Chiang, Exhalation\n\nThe author of this quote is\n```\n\nOutput:\n\n```text\n Ted Chiang\n```\n\n### Scenario prompt example\n\nGiving the model a scenario to follow or role to play out can be helpful for complex queries or when seeking imaginative responses. When using a hypothetical prompt, you set up a situation, problem, or story, and then ask the model to respond as if it were a character in that scenario or an expert on the topic.\n\nExample scenario prompt:\n\n```text\nYour role is to extract the name of the author from any given text\n\n“Some humans theorize that intelligent species go extinct before they can expand into outer space. If they're correct, then the hush of the night sky is the silence of the graveyard.”\n― Ted Chiang, Exhalation\n```\n\nOutput:\n\n```text\n Ted Chiang\n```\n\n### Demonstration prompt example (few-shot learning)\n\nSimilar to completion-style prompts, demonstrations can show the model what you want it to do. This approach is sometimes called few-shot learning, as the model learns from a few examples provided in the prompt.\n\nExample demonstration prompt:\n\n```text\nQuote:\n“When the reasoning mind is forced to confront the impossible again and again, it has no choice but to adapt.”\n― N.K. Jemisin, The Fifth Season\nAuthor: N.K. Jemisin\n\nQuote:\n“Some humans theorize that intelligent species go extinct before they can expand into outer space. If they're correct, then the hush of the night sky is the silence of the graveyard.”\n― Ted Chiang, Exhalation\nAuthor:\n```\n\nOutput:\n\n```text\n Ted Chiang\n```\n\n### Fine-tuned prompt example\n\nWith enough training examples, you can [fine-tune][Fine Tuning Docs] a custom model. In this case, instructions become unnecessary, as the model can learn the task from the training data provided. However, it can be helpful to include separator sequences (e.g., `->` or `###` or any string that doesn't commonly appear in your inputs) to tell the model when the prompt has ended and the output should begin. Without separator sequences, there is a risk that the model continues elaborating on the input text rather than starting on the answer you want to see.\n\nExample fine-tuned prompt (for a model that has been custom trained on similar prompt-completion pairs):\n\n```text\n“Some humans theorize that intelligent species go extinct before they can expand into outer space. If they're correct, then the hush of the night sky is the silence of the graveyard.”\n― Ted Chiang, Exhalation\n\n###\n\n\n```\n\nOutput:\n\n```text\n Ted Chiang\n```\n\n## Code Capabilities\n\nLarge language models aren't only great at text - they can be great at code too. OpenAI's [GPT-4][GPT-4 and GPT-4 Turbo] model is a prime example.\n\nGPT-4 powers [numerous innovative products][OpenAI Customer Stories], including:\n\n- [GitHub Copilot] (autocompletes code in Visual Studio and other IDEs)\n- [Replit](https://replit.com/) (can complete, explain, edit and generate code)\n- [Cursor](https://cursor.sh/) (build software faster in an editor designed for pair-programming with AI)\n\nGPT-4 is more advanced than previous models like `gpt-3.5-turbo-instruct`. But, to get the best out of GPT-4 for coding tasks, it's still important to give clear and specific instructions. As a result, designing good prompts can take more care.\n\n### More prompt advice\n\nFor more prompt examples, visit [OpenAI Examples][OpenAI Examples].\n\nIn general, the input prompt is the best lever for improving model outputs. You can try tricks like:\n\n- **Be more specific** E.g., if you want the output to be a comma separated list, ask it to return a comma separated list. If you want it to say \"I don't know\" when it doesn't know the answer, tell it 'Say \"I don't know\" if you do not know the answer.' The more specific your instructions, the better the model can respond.\n- **Provide Context**: Help the model understand the bigger picture of your request. This could be background information, examples/demonstrations of what you want or explaining the purpose of your task.\n- **Ask the model to answer as if it was an expert.** Explicitly asking the model to produce high quality output or output as if it was written by an expert can induce the model to give higher quality answers that it thinks an expert would write. Phrases like \"Explain in detail\" or \"Describe step-by-step\" can be effective.\n- **Prompt the model to write down the series of steps explaining its reasoning.** If understanding the 'why' behind an answer is important, prompt the model to include its reasoning. This can be done by simply adding a line like \"[Let's think step by step](https://arxiv.org/abs/2205.11916)\" before each answer.\n\n[Fine Tuning Docs]: https://platform.openai.com/docs/guides/fine-tuning\n[OpenAI Customer Stories]: https://openai.com/customer-stories\n[Large language models Blog Post]: https://openai.com/research/better-language-models\n[GitHub Copilot]: https://github.com/features/copilot/\n[GPT-4 and GPT-4 Turbo]: https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo\n[GPT3 Apps Blog Post]: https://openai.com/blog/gpt-3-apps/\n[OpenAI Examples]: https://platform.openai.com/examples\n",
      "tags": [
        "tokens",
        "prompts"
      ],
      "category": "prompt-engineering"
    },
    {
      "slug": "openai-techniques-to-improve-reliability",
      "title": "Techniques to improve reliability",
      "description": "When GPT-3 fails on a task, what should you do?",
      "source": "openai",
      "sourceLabel": "OpenAI Cookbook",
      "sourceColor": "#10a37f",
      "url": "https://github.com/openai/openai-cookbook/blob/main/articles/techniques_to_improve_reliability.md",
      "content": "# Techniques to improve reliability\n\nWhen GPT-3 fails on a task, what should you do?\n\n- Search for a better prompt that elicits more reliable answers?\n- Invest in thousands of examples to fine-tune a custom model?\n- Assume the model is incapable of the task, and move on?\n\nThere is no simple answer - it depends. However, if your task involves logical reasoning or complexity, consider trying the techniques in this article to build more reliable, high-performing prompts.\n\n## Why GPT-3 fails on complex tasks\n\nIf you were asked to multiply 13 by 17, would the answer pop immediately into your mind? For most of us, probably not. Yet, that doesn't mean humans are incapable of two-digit multiplication. With a few seconds, and some pen and paper, it's not too taxing to work out that 13 x 17 = 130 + 70 + 21 = 221.\n\nSimilarly, if you give GPT-3 a task that's too complex to do in the time it takes to calculate its next token, it may confabulate an incorrect guess. Yet, akin to humans, that doesn't necessarily mean the model is incapable of the task. With some time and space to reason things out, the model still may be able to answer reliably.\n\nAs an example, if you ask `gpt-3.5-turbo-instruct` the following math problem about juggling balls, it answers incorrectly:\n\n```gpt-3.5-turbo-instruct\nQ: A juggler has 16 balls. Half of the balls are golf balls and half of the golf balls are blue. How many blue golf balls are there?\nA:\n```\n\n```gpt-3.5-turbo-instruct\nThere are 8 blue golf balls.\n```\n\nDoes this mean that GPT-3 cannot do simple math problems? No; in fact, it turns out that by prompting the model with `Let's think step by step`, the model solves the problem reliably:\n\n```gpt-3.5-turbo-instruct\nQ: A juggler has 16 balls. Half of the balls are golf balls and half of the golf balls are blue. How many blue golf balls are there?\nA: Let's think step by step.\n```\n\n```gpt-3.5-turbo-instruct\nThere are 16 balls in total.\nHalf of the balls are golf balls.\nThat means that there are 8 golf balls.\nHalf of the golf balls are blue.\nThat means that there are 4 blue golf balls.\n```\n\nOf course, it's hard to tell from only a single example whether this `Let's think step by step` trick actually works in general or just got lucky on this particular problem. But it really does work. On a benchmark of word math problems, the `Let's think step by step` trick raised GPT-3's solve rate massively, from a worthless 18% to a decent 79%!\n\n## Model capabilities depend on context\n\nWhen learning to work with GPT-3, one common conceptual mistake is to believe that its capabilities are fixed across all contexts. E.g., if GPT-3 gets a simple logic question wrong, then it must be incapable of simple logic.\n\nBut as the `Let's think step by step` example illustrates, apparent failures of GPT-3 can sometimes be remedied with a better prompt that helps the model steer itself toward the correct output.\n\n## How to improve reliability on complex tasks\n\nThe rest of this article shares techniques for improving reliability of large language models on complex tasks. Although some of the techniques are specific to certain types of problems, many of them are built upon general principles that can be applied to a wide range of tasks, e.g.:\n\n- Give clearer instructions\n- Split complex tasks into simpler subtasks\n- Structure the instruction to keep the model on task\n- Prompt the model to explain before answering\n- Ask for justifications of many possible answers, and then synthesize\n- Generate many outputs, and then use the model to pick the best one\n- Fine-tune custom models to maximize performance\n\n## Split complex tasks into simpler tasks\n\nOne way to give a model more time and space to think is to break tasks into simpler pieces.\n\nAs an example, consider a task where we ask the model a multiple-choice question about some text - in this case, a game of Clue. When asked directly, `gpt-3.5-turbo-instruct` isn't able to put clues 3 & 5 together, and answers incorrectly:\n\n```gpt-3.5-turbo-instruct\nUse the following clues to answer the following multiple-choice question.\n\nClues:\n1. Miss Scarlett was the only person in the lounge.\n2. The person with the pipe was in the kitchen.\n3. Colonel Mustard was the only person in the observatory.\n4. Professor Plum was not in the library nor the billiard room.\n5. The person with the candlestick was in the observatory.\n\nQuestion: Was Colonel Mustard in the observatory with the candlestick?\n(a) Yes; Colonel Mustard was in the observatory with the candlestick\n(b) No; Colonel Mustard was not in the observatory with the candlestick\n(c) Unknown; there is not enough information to determine whether Colonel Mustard was in the observatory with the candlestick\n\nSolution:\n```\n\n```gpt-3.5-turbo-instruct\n(c) Unknown; there is not enough information to determine whether Colonel Mustard was in the observatory with the candlestick\n```\n\nAlthough clues 3 and 5 establish that Colonel Mustard was the only person in the observatory and that the person in the observatory had the candlestick, the model fails to combine them into a correct answer of (a) Yes.\n\nHowever, instead of asking for the answer directly, we can split the task into three pieces:\n\n- First, go through the clues one by one and consider whether the clue is potentially relevant\n- Second, combine the relevant clues to reason out the answer to the question\n- Third, write the final answer: either (a), (b), or (c)\n\n```gpt-3.5-turbo-instruct\nUse the following clues to answer the following multiple-choice question, using the following procedure:\n(1) First, go through the clues one by one and consider whether the clue is potentially relevant\n(2) Second, combine the relevant clues to reason out the answer to the question\n(3) Third, map the answer to one of the multiple choice answers: either (a), (b), or (c)\n\nClues:\n1. Miss Scarlett was the only person in the lounge.\n2. The person with the pipe was in the kitchen.\n3. Colonel Mustard was the only person in the observatory.\n4. Professor Plum was not in the library nor the billiard room.\n5. The person with the candlestick was in the observatory.\n\nQuestion: Was Colonel Mustard in the observatory with the candlestick?\n(a) Yes; Colonel Mustard was in the observatory with the candlestick\n(b) No; Colonel Mustard was not in the observatory with the candlestick\n(c) Unknown; there is not enough information to determine whether Colonel Mustard was in the observatory with the candlestick\n\nSolution:\n(1) First, go through the clues one by one and consider whether the clue is potentially relevant:\n```\n\n```gpt-3.5-turbo-instruct\nClue 1: Miss Scarlett was the only person in the lounge. This clue is not relevant.\nClue 2: The person with the pipe was in the kitchen. This clue is not relevant.\nClue 3: Colonel Mustard was the only person in the observatory. This clue is relevant.\nClue 4: Professor Plum was not in the library nor the billiard room. This clue is not relevant.\nClue 5: The person with the candlestick was in the observatory. This clue is relevant.\n\n(2) Second, combine the relevant clues to reason out the answer to the question:\nClue 3 and Clue 5 both say that the person with the candlestick was in the observatory. Colonel Mustard was the only person in the observatory, so Colonel Mustard was the person with the candlestick.\n\n(3) Third, write the final answer:\n(a) Yes; Colonel Mustard was in the observatory with the candlestick\n```\n\nBy giving the model more time and space to think, and guiding it along a reasoning plan, it's able to figure out the correct answer of (a) Yes.\n\nAnother benefit of splitting complex instructions into smaller subtasks is that it can help keep the model focused on each subtask.\n\nFor example, if we ask `gpt-3.5-turbo-instruct` to summarize a text in its original language, the model can lapse back into English:\n\n```gpt-3.5-turbo-instruct\nSummarize the text using the original language of the text. The summary should be one sentence long.\n\nText:\n\"\"\"\nLa estadística (la forma femenina del término alemán Statistik, derivado a su vez del italiano statista, \"hombre de Estado\")​ es una ciencia que estudia la variabilidad, colección, organización, análisis, interpretación, y presentación de los datos, así como el proceso aleatorio que los genera siguiendo las leyes de la probabilidad.​ La estadística es una ciencia formal deductiva, con un conocimiento propio, dinámico y en continuo desarrollo obtenido a través del método científico formal. En ocasiones, las ciencias fácticas necesitan utilizar técnicas estadísticas durante su proceso de investigación factual, con el fin de obtener nuevos conocimientos basados en la experimentación y en la observación. En estos casos, la aplicación de la estadística permite el análisis de datos provenientes de una muestra representativa, que busca explicar las correlaciones y dependencias de un fenómeno físico o natural, de ocurrencia en forma aleatoria o condicional.\n\"\"\"\n\nSummary:\n```\n\n```gpt-3.5-turbo-instruct\nThe text explains that statistics is a science that studies the variability, collection, organization, analysis, interpretation, and presentation of data, as well as the random process that generates them following the laws of probability.\n```\n\nHowever, if we first ask the model to identify the language of the text, and then summarize the text, it becomes more reliable:\n\n```gpt-3.5-turbo-instruct\nFirst, identify the language of the text. Second, summarize the text using the original language of the text. The summary should be one sentence long.\n\nText:\n\"\"\"\nLa estadística (la forma femenina del término alemán Statistik, derivado a su vez del italiano statista, \"hombre de Estado\")​ es una ciencia que estudia la variabilidad, colección, organización, análisis, interpretación, y presentación de los datos, así como el proceso aleatorio que los genera siguiendo las leyes de la probabilidad.​ La estadística es una ciencia formal deductiva, con un conocimiento propio, dinámico y en continuo desarrollo obtenido a través del método científico formal. En ocasiones, las ciencias fácticas necesitan utilizar técnicas estadísticas durante su proceso de investigación factual, con el fin de obtener nuevos conocimientos basados en la experimentación y en la observación. En estos casos, la aplicación de la estadística permite el análisis de datos provenientes de una muestra representativa, que busca explicar las correlaciones y dependencias de un fenómeno físico o natural, de ocurrencia en forma aleatoria o condicional.\n\"\"\"\n\nLanguage:\n```\n\n```gpt-3.5-turbo-instruct\nSpanish\n\nLa estadística es una ciencia que estudia la variabilidad, colección, organización, análisis, interpretación, y presentación de los datos, así como el proceso aleatorio que los genera siguiendo las leyes de la probabilidad.\n```\n\n## Prompt the model to explain before answering\n\nAnother powerful technique for improving the reliability of answers is to prompt the model to gradually reason out the answer rather than jumping immediately to the final answer. By 'thinking aloud' the model can be far more likely to arrive at the correct answer.\n\n### Zero-shot\n\n#### Method\n\nPublished by [Takeshi Kojima et al. in 2022](https://arxiv.org/abs/2205.11916), the easiest way to prompt a model to reason out the answer is to simply prepend answers with `Let's think step by step.` Figure 2 illustrates an example:\n\n[![zero-shot reasoning example](/images/zero-shot_reasoners_fig2.png)\n<br>Source: _Large Language Models are Zero-Shot Reasoners_ by Takeshi Kojima et al. (2022).](https://arxiv.org/abs/2205.11916)\n\n#### Results\n\nApplying this simple trick to the MultiArith math dataset, the authors found `Let's think step by step` quadrupled the accuracy, from 18% to 79%!\n\n[![zero-shot reasoning example](/images/zero-shot_reasoners_tab5.png)\n<br>Source: _Large Language Models are Zero-Shot Reasoners_ by Takeshi Kojima et al. (2022).](https://arxiv.org/abs/2205.11916)\n\n#### Implications\n\nAlthough the `Let's think step by step` trick works well on math problems, it's not effective on all tasks. The authors found that it was most helpful for multi-step arithmetic problems, symbolic reasoning problems, strategy problems, and other reasoning problems. It didn't help with simple math problems or common sense questions, and presumably wouldn't help with many other non-reasoning tasks either.\n\n[![zero-shot reasoning example](/images/zero-shot_reasoners_tab1.png)\n<br>Source: _Large Language Models are Zero-Shot Reasoners_ by Takeshi Kojima et al. (2022).](https://arxiv.org/abs/2205.11916)\n\nTo learn more, read the [full paper](https://arxiv.org/abs/2205.11916).\n\nIf you apply this technique to your own tasks, don't be afraid to experiment with customizing the instruction. `Let's think step by step` is rather generic, so you may find better performance with instructions that hew to a stricter format customized to your use case. For example, you can try more structured variants like `First, think step by step about why X might be true. Second, think step by step about why Y might be true. Third, think step by step about whether X or Y makes more sense.`. And you can even give the model an example format to help keep it on track, e.g.:\n\n```gpt-3.5-turbo-instruct\nUsing the IRS guidance below, answer the following questions using this format:\n(1) For each criterion, determine whether it is met by the vehicle purchase\n- {Criterion} Let's think step by step. {explanation} {yes or no, or if the question does not apply then N/A}.\n(2) After considering each criterion in turn, phrase the final answer as \"Because of {reasons}, the answer is likely {yes or no}.\"\n\nIRS guidance:\n\"\"\"\nYou may be eligible for a federal tax credit under Section 30D if you purchased a car or truck that meets the following criteria:\n- Does the vehicle have at least four wheels?\n- Does the vehicle weigh less than 14,000 pounds?\n- Does the vehicle draw energy from a battery with at least 4 kilowatt hours that may be recharged from an external source?\n- Was the vehicle purchased in a year before 2022?\n  - If so, has the manufacturer sold less than 200,000 qualifying vehicles? (Tesla and GM have sold more than 200,000 qualifying vehicles.)\n- Was the vehicle purchased in a year after 2022?\n  - If so, is the vehicle present in the following list of North American-assembled vehicles? (The only electric vehicles assembled in North America are the Audi Q5, BMW 330e, BMW X5, Chevrolet Bolt EUV, Chevrolet Bolt EV, Chrysler Pacifica PHEV, Ford Escape PHEV, Ford F Series, Ford Mustang MACH E, Ford Transit Van, GMC Hummer Pickup, GMC Hummer SUV, Jeep Grand Cherokee PHEV, Jeep Wrangler PHEV, Lincoln Aviator PHEV, Lincoln Corsair Plug-in, Lucid Air, Nissan Leaf, Rivian EDV, Rivian R1S, Rivian R1T, Tesla Model 3, Tesla Model S, Tesla Model X, Tesla Model Y, Volvo S60, BMW 330e, Bolt EV, Cadillac Lyriq, Mercedes EQS SUV, and Nissan Leaf.)\n\"\"\"\n\nQuestion: Can I claim a federal tax credit for my Toyota Prius Prime bought in 2021?\n\nSolution:\n\n(1) For each criterion, determine whether it is met by the vehicle purchase\n- Does the vehicle have at least four wheels? Let's think step by step.\n```\n\n```gpt-3.5-turbo-instruct\n The Toyota Prius Prime has four wheels, so the answer is yes.\n- Does the vehicle weigh less than 14,000 pounds? Let's think step by step. The Toyota Prius Prime weighs less than 14,000 pounds, so the answer is yes.\n- Does the vehicle draw energy from a battery with at least 4 kilowatt hours that may be recharged from an external source? Let's think step by step. The Toyota Prius Prime has a battery with at least 4 kilowatt hours that may be recharged from an external source, so the answer is yes.\n- Was the vehicle purchased in a year before 2022? Let's think step by step. The Toyota Prius Prime was purchased in 2021, which is before 2022, so the answer is yes.\n- Was the vehicle purchased in a year after 2022? N/A\n- If so, is the vehicle present in the following list of North American-assembled vehicles? N/A\n\n(2) After considering each criterion in turn, phrase the final answer as \"Because of {reasons}, the answer is likely {yes or no}.\"\n\nBecause the Toyota Prius Prime meets all of the criteria for a federal tax credit, the answer is likely yes.\n```\n\n### Few-shot examples\n\n#### Method\n\nPrompting the model to reason out its answers can be done in many ways. One way is to demonstrate with a few examples ('few-shot'), as studied by [Jason Wei and Denny Zhou et al. from Google](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html). Here's an example few-shot chain-of-thought prompt:\n\n[![chain of thought example](/images/chain_of_thought_fig1.png)\n<br>Source: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Jason Wei and Denny Zhou et al. (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)\n\nMore demonstrations of reasoning chains written by human labelers:\n\n[![chain of thought example](/images/chain_of_thought_fig3.png)\n<br>Source: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Jason Wei and Denny Zhou et al. (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)\n\n[(Note that it has been called into question whether pears actually float)](https://twitter.com/Meaningness/status/1561062170074370048?s=20&t=mpHt8f3RRboztXxdhLFnWQ)\n\n#### Results\n\nTesting on grade school math problems, the authors found that chain of thought prompting tripled the solve rate, from 18% to 57%.\n\n[![chain of thought example](/images/chain_of_thought_fig5.png)\n<br>Source: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Jason Wei and Denny Zhou et al. (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)\n\nIn addition to math problems, chain of thought prompting also lifted performance on questions related to sports understanding, coin flip tracking, and last letter concatenation. In most cases, not many examples were need to saturate the performance gains (less than 8 or so).\n\n[![chain of thought example](/images/chain_of_thought_fig11.png)\n<br>Source: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Jason Wei and Denny Zhou et al. (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)\n\nTo learn more, read the [full paper](https://arxiv.org/abs/2201.11903).\n\n#### Implications\n\nOne advantage of the few-shot example-based approach relative to the `Let's think step by step` technique is that you can more easily specify the format, length, and style of reasoning that you want the model to perform before landing on its final answer. This can be particularly helpful in cases where the model isn't initially reasoning in the right way or depth.\n\n### Fine-tuned\n\n#### Method\n\nIn general, to eke out maximum performance on a task, you'll need to fine-tune a custom model. However, fine-tuning a model using explanations may take thousands of example explanations, which are costly to write.\n\nIn 2022, Eric Zelikman and Yuhuai Wu et al. published a clever procedure for using a few-shot prompt to generate a dataset of explanations that could be used to fine-tune a model. The idea is to use a few-shot prompt to generate candidate explanations, and only keep the explanations that produce the correct answer. Then, to get additional explanations for some of the incorrect answers, retry the few-shot prompt but with correct answers given as part of the question. The authors called their procedure STaR (Self-taught Reasoner):\n\n[![STaR procedure](/images/star_fig1.png)\n<br>Source: _STaR: Bootstrapping Reasoning With Reasoning_ by Eric Zelikman and Yujuai Wu et al. (2022)](https://arxiv.org/abs/2203.14465)\n\nWith this technique, you can combine the benefits of fine-tuning with the benefits of chain-of-thought prompting without needing to write thousands of example explanations.\n\n#### Results\n\nWhen the authors applied this technique to a Common Sense Q&A dataset, they found that STaR outperformed both chain-of-thought prompting alone (73% > 37%) and fine-tuning alone (73% > 60%):\n\n[![STaR results](/images/star_tab1.png)\n<br>Source: _STaR: Bootstrapping Reasoning With Reasoning_ by Eric Zelikman and Yujuai Wu et al. (2022)](https://arxiv.org/abs/2203.14465)\n\nTo learn more, read the [full paper](https://arxiv.org/abs/2203.14465).\n\n#### Implications\n\nUsing a few-shot prompt to extend or modify a fine-tuning dataset is an idea that can be generalized beyond explanation writing. For example, if you have large quantities of unstructured text that you want to train on, you may find opportunities to use a prompt to extract a structured dataset from your unstructured text, and then fine-tune a custom model on that structured dataset.\n\n## Extensions to chain-of-thought prompting\n\nA number of extensions of chain-of-thought prompting have been published as well.\n\n### Selection-inference prompting\n\n#### Method\n\nPublished by Antonia Creswell et al., one extension of the chain-of-thought technique is to split the single prompt for generating explanations and answers into smaller parts. First, a prompt selects a relevant subset of facts from the text ('selection prompt'). Then, a second prompt infers a conclusion from the selected facts ('inference prompt'). These prompts are then alternated in a loop to generate multiple steps of reasoning and eventually land on a final answer. The authors illustrate the idea in the following figure:\n\n[![Selection-inference prompting](/images/selection-inference_fig1.png)\n<br>Source: _Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning_ by Antonia Creswell et al. (2022)](https://arxiv.org/abs/2205.09712)\n\n#### Results\n\nWhen applied to a 7B-parameter model, the authors found that selection-inference prompting substantially improved performance relative to chain-of-thought prompting on the bAbi and Proof Writer benchmark tasks (both of which require longer sequences of reasoning steps). The best performance they achieved combined both selection-inference prompting with fine-tuning.\n\n[![Selection-inference prompting](/images/selection-inference_fig4.png)\n<br>Source: _Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning_ by Antonia Creswell et al. (2022)](https://arxiv.org/abs/2205.09712)\n\n#### Implications\n\nAlthough the gains on these benchmarks were large, these benchmarks were specifically chosen because they required longer sequences of reasoning. On problems that don't require reasoning with many steps, the gains are likely smaller.\n\nThe results highlight a couple of general lessons for working with large language models. One, splitting up complex tasks into smaller tasks is a great way to improve reliability and performance; the more atomic the task, the less room there is for the model to err. Two, getting maximum performance often means combining fine-tuning with whatever approach you've chosen.\n\nTo learn more, read the [full paper](https://arxiv.org/abs/2205.09712).\n\n### Faithful reasoning architecture\n\nA few months after publishing the selection-inference prompting technique, the authors extended the technique in a follow-up paper, with ideas for:\n\n- figuring out when the selection-inference cycle should stop or continue\n- adding a value function to help search over multiple reasoning paths\n- reducing hallucination of fake facts by fine-tuning a model to reason about sentence labels (e.g., sen1) rather than writing out the sentences themselves\n\n#### Method\n\nIn the original selection-inference technique, specialized 'selection' and 'inference' prompts are alternated to select facts and make inferences from those facts, combining to generate a sequence of reasoning steps.\n\nThe authors extend this technique with two additional components.\n\nFirst, the authors add a 'halter' model that, after each inference step, is asked whether the inferences thus far are sufficient to answer the question. If yes, then the model generates a final answer.\n\nThe halter models brings a couple of advantages:\n\n- it can tell the selection-inference process to stop or keep going, as necessary.\n- if the process never halts, you'll get no answer, which is often preferable to a hallucinated guess\n\n[![Faithful reasoning](/images/faithful-reasoning_fig3.png)\n<br>Source: _Faithful Reasoning Using Large Language Models_ by Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)\n\n[![Faithful reasoning](/images/faithful-reasoning_fig5.png)\n<br>Source: _Faithful Reasoning Using Large Language Models_ by Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)\n\nSecond, the authors add a value function, which is used to assess the quality of reasoning steps and search over multiple reasoning trajectories. This echoes a common theme for increasing reliability; instead of generating a single answer from the model, generate a set of answers and then use some type of value function / discriminator / verifier model to pick the best one.\n\n[![Faithful reasoning](/images/faithful-reasoning_fig7.png)\n<br>Source: _Faithful Reasoning Using Large Language Models_ by Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)\n\nIn addition to these two extensions, the authors also use a trick to reduce hallucination of fake facts. Rather than asking the model to write out factual sentences, they fine-tune a model to work with sentence labels (e.g., sen1) instead. This helps prevent the model from hallucinating fake facts not mentioned in the prompt context.\n\n[![Faithful reasoning](/images/faithful-reasoning_fig4.png)\n<br>Source: _Faithful Reasoning Using Large Language Models_ by Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)\n\n#### Results\n\nThe authors evaluated their technique on two benchmarks: the ProofWriter task (not shown) and [EntailmentBankQA](https://allenai.org/data/entailmentbank) (shown). The technique increased accuracy substantially, especially on harder reasoning problems.\n\n![Faithful reasoning](/images/faithful-reasoning_tab2.png)\n<br>Source: _Faithful Reasoning Using Large Language Models_ by Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)\n\nIn addition, their sentence label manipulation trick essentially eliminated hallucination!\n\n![Faithful reasoning](/images/faithful-reasoning_tab5.png)\n<br>Source: _Faithful Reasoning Using Large Language Models_ by Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)\n\n#### Implications\n\nThis paper illustrates a number of helpful lessons for improving the reliability of large language models:\n\n- Split complex tasks into smaller, more reliable subtasks\n- Generate your answer in a step-by-step fashion, evaluating it along the way\n- Generate many possible answers and use another model or function to pick the ones that look best\n- Reduce hallucination by constraining what the model can say (e.g., by using sentence labels instead of sentences)\n- Maximize performance of models by fine-tuning them on specialized tasks\n\nTo learn more, read the [full paper](https://arxiv.org/abs/2205.09712).\n\n### Least-to-most prompting\n\nIn addition to doing poorly on long reasoning chains (where selection-inference shines), chain-of-thought prompting can especially struggle when the examples are short but the task is long.\n\n#### Method\n\nLeast-to-most prompting is another technique that splits up reasoning tasks into smaller, more reliable subtasks. The idea is to elicit a subtask from the model by prompting it with something like `To solve {question}, we need to first solve: \"`. Then, with that subtask in hand, the model can generate a solution. The solution is appended to the original question and the process is repeated until a final answer is produced.\n\n[![Least-to-most prompting](/images/least-to-most_fig1.png)\n<br>Source: _Least-to-most Prompting Enables Complex Reasoning in Large Language Models_ by Denny Zhou et al. (2022)](https://arxiv.org/abs/2205.10625)\n\n#### Results\n\nWhen applied to benchmarks involving long reasoning chains using `code-davinci-002` (which is optimized for code but can still understand text), the authors measured gains as large as 16% -> 99.7%!\n\n[\n![Least-to-most prompting results on last-letter-concatenation task](/images/least-to-most_tab4.png)\n![Least-to-most prompting results on SCAN](/images/least-to-most_tab9.png)\n![Least-to-most prompting results on DROP numerical reasoning](/images/least-to-most_tab11.png)\n<br>Source: _Least-to-most Prompting Enables Complex Reasoning in Large Language Models_ by Denny Zhou et al. (2022)](https://arxiv.org/abs/2205.10625)\n\n#### Implications\n\nAlthough the above gains from least-to-most prompting are impressive, they are measured on a very narrow set of tasks that require long reasoning chains.\n\nStill, they illustrate a common theme: increase reliability by (a) breaking complex tasks into smaller subtasks and (b) giving the model more time and space to work out the answer.\n\nTo learn more, read the [full paper](https://arxiv.org/abs/2205.10625).\n\n## Related ideas\n\n### Maieutic prompting\n\n#### Method\n\nIn contrast to the previous techniques, which try to maximize the likelihood of correct answers, another approach is to use GPT-3 to generate a tree of possible explanations (both correct _and incorrect_), and then analyze their relationships to guess at which set is correct. This technique was coined maieutic prompting by [Jaehun Jung et al. in May 2022](https://arxiv.org/abs/2205.11822) (maieutic means relating to the Socratic method of asking questions to elicit ideas).\n\nThe method is complicated, and works as follows:\n\n- First, build a maieutic tree, where each node is a statement that could be true or false:\n  - Start with a multiple-choice question or true/false statement (e.g. `War cannot have a tie`)\n  - For each possible answer to the question, use the model to generate a corresponding explanation (with a prompt like `War cannot have a tie? True, because`)\n  - Then, prompt the model with the question and the generated explanation, and ask it to produce the answer. If reversing the explanation (with a prefix like `It is wrong to say that {explanation}`) reverses the answer, then the explanation is considered 'logically integral.'\n  - If an explanation is not logically integral, then repeat the above process recursively, with each explanation turned into a True or False question, and generate more explanations for each new question.\n  - After all of the recursive explaining is done, you end up with a tree of explanations, where each leaf on the tree has the property that reversing the explanation reverses the model's answer.\n- Second, convert the tree into a graph of relations:\n  - For each node in the tree, calculate the model's relative belief in each node (inferred from the probability of getting an answer of `True` to given an explanation)\n  - For each pair of nodes in the tree, use the model to identify whether they are entailed (implied) or contradicted\n- Third, find the most consistent set of beliefs and take those to be true:\n  - Specifically, using the strength of belief in each node and the logical relationships between them, formulate the problem as a weighted maximum satisfiability problem (MAX-SAT)\n  - Use a solver to the find the most self-consistent set of beliefs, and take those as true\n\n[\n![Maieutic prompting](/images/maieutic_fig2.png)\n![Maieutic prompting](/images/maieutic_fig6.png)\n<br>Source: _Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations_ by Jaehun Jung et al. (2022)](https://arxiv.org/abs/2205.11822)\n\n#### Results\n\n[![Maieutic prompting results](/images/maieutic_tab1.png)\n<br>Source: _Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations_ by Jaehun Jung et al. (2022)](https://arxiv.org/abs/2205.11822)\n\n#### Implications\n\nBeyond the complexity, one limitation of this method is that it appears to only apply to questions that can be posed as multiple-choice.\n\nTo learn more, read the [full paper](https://arxiv.org/abs/2205.11822).\n\n## Extensions\n\n### Self-consistency\n\n#### Method\n\nFor tasks with a discrete set of answers, one simple way to improve reliability is to sample multiple explanations & answers from the model (using a positive temperature) and then pick the final answer that appears most often.\n\n[![Self-consistency method](/images/self-consistency_fig1.png)\n<br>Source: _Self-Consistency Improves Chain of Thought Reasoning in Language Models_ by Xuezhi Wang et al. (2022)](https://arxiv.org/abs/2203.11171)\n\n#### Results\n\nThis technique lifted accuracies by anywhere from 1 to 24 percentage points on a suite of math and reasoning benchmarks. (Plotted below are results from Google's LaMDA model; using Google's larger PaLM model, the baselines were higher but the gains were a bit smaller.)\n\n[![Self-consistency results](/images/self-consistency_fig3.png)\n<br>Source: _Self-Consistency Improves Chain of Thought Reasoning in Language Models_ by Xuezhi Wang et al. (2022)](https://arxiv.org/abs/2203.11171)\n\n#### Implications\n\nAlthough this technique is simple to implement, it can be costly. Generating a set of 10 answers will increase your costs by 10x.\n\nAlso, as with many of these techniques, it applies only to tasks with a limited set of answers. For open-ended tasks where each answer is unique (such as writing a poem), it's not obvious what it would mean to pick the most common answer.\n\nLastly, this technique ought to be most beneficial when there are multiple paths or phrasings to reach an answer; if there's only one path, then the technique may not help at all. An extreme example: If the task was to generate a single token answer, then taking the most common token from 100 generations would be no different than taking the token with the highest logprobs (which you can get with a single generation at temperature=0).\n\n### Verifiers\n\nAnother key technique for improving task performance is to train a verifier or discriminator model to evaluate the outputs of the main generative model. If the discriminator rejects the output, then you can resample the generative model until you get an acceptable output. In many cases, it's easier to judge an answer than it is to create an answer, which helps explain the power of this method.\n\n#### Method\n\nIn 2021, OpenAI researchers applied this technique to grade school math problems, using the following procedure:\n\n- First, they fine-tuned a model on questions and solutions\n- For each problem in the training set, they generated 100 solutions\n- Each of those 100 solutions was automatically labeled as either correct or incorrect, based on whether the final answer was correct\n- Using those solutions, with some labeled correct and some labeled incorrect, they fine-tuned a verifier model to classify whether a question and candidate solution was correct or incorrect\n- Finally, at test time, the generative model creates 100 solutions to each problem, and the one with the highest score according to the verifier model is picked as the final answer\n\n[![Verifier method](/images/verifiers_fig3.png)\n<br>Source: _Training Verifiers to Solve Math Word Problems_ by Karl Cobbe et al. (2021)](https://arxiv.org/abs/2110.14168)\n\n#### Results\n\nWith a 175B GPT-3 model and 8,000 training examples, this technique substantially lifted grade school math accuracy from ~33% to ~55%.\n\n[![Verifier results](/images/verifiers_fig5.png)\n<br>Source: _Training Verifiers to Solve Math Word Problems_ by Karl Cobbe et al. (2021)](https://arxiv.org/abs/2110.14168)\n\n#### Implications\n\nSimilar to the self-consistency technique, this method can get expensive, as generating, say, 100 solutions per task will increase your costs by roughly ~100x.\n\n## Theories of reliability\n\nAlthough the techniques above vary in their approach, they all share the goal of improving reliability on complex tasks. Mainly they do this by:\n\n- decomposing unreliable operations into smaller, more reliable operations (e.g., selection-inference prompting)\n- using multiple steps or multiple relationships to make the system's reliability greater than any individual component (e.g., maieutic prompting)\n\n### Probabilistic graphical models\n\nThis paradigm of trying to build a reliable system out of less reliable components is reminiscent of probabilistic programming, and many of the analysis techniques of that field can be applied to this one.\n\nIn the paper _Language Model Cascades_, David Dohan et al. interpret the above techniques in the paradigm of probabilistic graphical models:\n\n#### Chain of thought prompting\n\n[![graphical model of chain of thought prompting](/images/lm_cascades_fig1.png)\n<br>Source: _Language Model Cascades_ by David Dohan et al. (2022)](https://arxiv.org/abs/2207.10342)\n\n#### Fine-tuned chain of thought prompting / Self-taught reasoner\n\n[![graphical model of fine-tuned chain of thought prompting](/images/lm_cascades_fig3.png)\n<br>Source: _Language Model Cascades_ by David Dohan et al. (2022)](https://arxiv.org/abs/2207.10342)\n\n#### Selection-inference prompting\n\n[![graphical model of selection-inference prompting](/images/lm_cascades_fig4.png)\n<br>Source: _Language Model Cascades_ by David Dohan et al. (2022)](https://arxiv.org/abs/2207.10342)\n\n#### Verifiers\n\n[![graphical model of verifiers](/images/lm_cascades_fig5.png)\n<br>Source: _Language Model Cascades_ by David Dohan et al. (2022)](https://arxiv.org/abs/2207.10342)\n\n#### Implications\n\nAlthough formulating these techniques as probabilistic graphical models may not be immediately useful for solving any particular problem, the framework may be helpful in selecting, combining, and discovering new techniques.\n\n## Closing thoughts\n\nResearch into large language models is very active and evolving rapidly. Not only do researchers continue to improve the models, they also continue to improve our understanding of how to best employ the models. To underscore the pace of these developments, note that all of the papers shared above were published within the past 12 months (as I write in Sep 2022).\n\nIn the future, expect better models and better techniques to be published. Even if the specific techniques here are eclipsed by future best practices, the general principles behind them will likely remain a key part of any expert user's toolkit.\n\n## Bibliography\n\n| Lesson                                                                                                                         | Paper                                                                                                                                     | Date     |\n| ------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------- | -------- |\n| Break complex tasks into simpler subtasks (and consider exposing the intermediate outputs to users)                            | [AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts](https://arxiv.org/abs/2110.01691) | 2021 Oct |\n| You can improve output by generating many candidates, and then picking the one that looks best                                 | [Training Verifiers to Solve Math Word Problems](https://arxiv.org/abs/2110.14168)                                                        | 2021 Oct |\n| On reasoning tasks, models do better when they reason step-by-step before answering                                            | [Chain of Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)                                 | 2022 Jan |\n| You can improve step-by-step reasoning by generating many explanation-answer outputs, and picking the most popular answer      | [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)                               | 2022 Mar |\n| If you want to fine-tune a step-by-step reasoner, you can do it with multiple-choice question & answer data alone              | [STaR: Bootstrapping Reasoning With Reasoning](https://arxiv.org/abs/2203.14465)                                                          | 2022 Mar |\n| The step-by-step reasoning method works great even with zero examples                                                          | [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)                                                         | 2022 May |\n| You can do better than step-by-step reasoning by alternating a ‘selection’ prompt and an ‘inference’ prompt                    | [Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning](https://arxiv.org/abs/2205.09712)             | 2022 May |\n| On long reasoning problems, you can improve step-by-step reasoning by splitting the problem into pieces to solve incrementally | [Least-to-most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.10625)                            | 2022 May |\n| You can have the model analyze both good and bogus explanations to figure out which set of explanations are most consistent    | [Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations](https://arxiv.org/abs/2205.11822)                        | 2022 May |\n| You can think about these techniques in terms of probabilistic programming, where systems comprise unreliable components       | [Language Model Cascades](https://arxiv.org/abs/2207.10342)                                                                               | 2022 Jul |\n| You can eliminate hallucination with sentence label manipulation, and you can reduce wrong answers with a 'halter' prompt      | [Faithful Reasoning Using Large Language Models](https://arxiv.org/abs/2208.14271)                                                        | 2022 Aug |\n",
      "tags": [
        "tokens",
        "prompts"
      ],
      "category": "prompt-engineering"
    },
    {
      "slug": "openai-related-resources-from-around-the-web",
      "title": "Related resources from around the web",
      "description": "People are writing great tools and papers for improving outputs from GPT. Here are some cool ones we've seen:",
      "source": "openai",
      "sourceLabel": "OpenAI Cookbook",
      "sourceColor": "#10a37f",
      "url": "https://github.com/openai/openai-cookbook/blob/main/articles/related_resources.md",
      "content": "# Related resources from around the web\n\nPeople are writing great tools and papers for improving outputs from GPT. Here are some cool ones we've seen:\n\n## Prompting libraries & tools (in alphabetical order)\n\n- [Arthur Shield](https://www.arthur.ai/get-started): A paid product for detecting toxicity, hallucination, prompt injection, etc.\n- [Baserun](https://baserun.ai/): A paid product for testing, debugging, and monitoring LLM-based apps\n- [Chainlit](https://docs.chainlit.io/overview): A Python library for making chatbot interfaces.\n- [ElatoAI](https://github.com/akdeb/ElatoAI): A platform for running OpenAI Realtime API Speech on ESP32 on Arduino using Deno Edge Runtime and Supabase.\n- [Embedchain](https://github.com/embedchain/embedchain): A Python library for managing and syncing unstructured data with LLMs.\n- [FLAML (A Fast Library for Automated Machine Learning & Tuning)](https://microsoft.github.io/FLAML/docs/Getting-Started/): A Python library for automating selection of models, hyperparameters, and other tunable choices.\n- [Guidance](https://github.com/microsoft/guidance): A handy looking Python library from Microsoft that uses Handlebars templating to interleave generation, prompting, and logical control.\n- [Haystack](https://github.com/deepset-ai/haystack): Open-source LLM orchestration framework to build customizable, production-ready LLM applications in Python.\n- [HoneyHive](https://honeyhive.ai): An enterprise platform to evaluate, debug, and monitor LLM apps.\n- [LangChain](https://github.com/hwchase17/langchain): A popular Python/JavaScript library for chaining sequences of language model prompts.\n- [LiteLLM](https://github.com/BerriAI/litellm): A minimal Python library for calling LLM APIs with a consistent format.\n- [LlamaIndex](https://github.com/jerryjliu/llama_index): A Python library for augmenting LLM apps with data.\n- [LLMOps Database](https://www.reddit.com/r/LocalLLaMA/comments/1h4u7au/a_nobs_database_of_how_companies_actually_deploy/): Database of how companies actually deploy LLMs in production.\n- [LMQL](https://lmql.ai): A programming language for LLM interaction with support for typed prompting, control flow, constraints, and tools.\n- [OpenAI Evals](https://github.com/openai/evals): An open-source library for evaluating task performance of language models and prompts.\n- [Outlines](https://github.com/normal-computing/outlines): A Python library that provides a domain-specific language to simplify prompting and constrain generation.\n- [Parea AI](https://www.parea.ai): A platform for debugging, testing, and monitoring LLM apps.\n- [Portkey](https://portkey.ai/): A platform for observability, model management, evals, and security for LLM apps.\n- [Promptify](https://github.com/promptslab/Promptify): A small Python library for using language models to perform NLP tasks.\n- [PromptPerfect](https://promptperfect.jina.ai/prompts): A paid product for testing and improving prompts.\n- [Prompttools](https://github.com/hegelai/prompttools): Open-source Python tools for testing and evaluating models, vector DBs, and prompts.\n- [Scale Spellbook](https://scale.com/spellbook): A paid product for building, comparing, and shipping language model apps.\n- [Semantic Kernel](https://github.com/microsoft/semantic-kernel): A Python/C#/Java library from Microsoft that supports prompt templating, function chaining, vectorized memory, and intelligent planning.\n- [Vellum](https://www.vellum.ai/): A paid AI product development platform to experiment with, evaluate, and deploy advanced LLM apps.\n- [Weights & Biases](https://wandb.ai/site/solutions/llmops): A paid product for tracking model training and prompt engineering experiments.\n- [YiVal](https://github.com/YiVal/YiVal): An open-source GenAI-Ops tool for tuning and evaluating prompts, retrieval configurations, and model parameters using customizable datasets, evaluation methods, and evolution strategies.\n\n## Prompting guides\n\n- [Brex's Prompt Engineering Guide](https://github.com/brexhq/prompt-engineering): Brex's introduction to language models and prompt engineering.\n- [learnprompting.org](https://learnprompting.org/): An introductory course to prompt engineering.\n- [Lil'Log Prompt Engineering](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/): An OpenAI researcher's review of the prompt engineering literature (as of March 2023).\n- [OpenAI Cookbook: Techniques to improve reliability](https://cookbook.openai.com/articles/techniques_to_improve_reliability): A slightly dated (Sep 2022) review of techniques for prompting language models.\n- [promptingguide.ai](https://www.promptingguide.ai/): A prompt engineering guide that demonstrates many techniques.\n- [Xavi Amatriain's Prompt Engineering 101 Introduction to Prompt Engineering](https://amatriain.net/blog/PromptEngineering) and [202 Advanced Prompt Engineering](https://amatriain.net/blog/prompt201): A basic but opinionated introduction to prompt engineering and a follow up collection with many advanced methods starting with CoT.\n\n## Video courses\n\n- [Andrew Ng's DeepLearning.AI](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/): A short course on prompt engineering for developers.\n- [Andrej Karpathy's Let's build GPT](https://www.youtube.com/watch?v=kCc8FmEb1nY): A detailed dive into the machine learning underlying GPT.\n- [Prompt Engineering by DAIR.AI](https://www.youtube.com/watch?v=dOxUroR57xs): A one-hour video on various prompt engineering techniques.\n- [Scrimba course about Assistants API](https://scrimba.com/learn/openaiassistants): A 30-minute interactive course about the Assistants API.\n- [LinkedIn course: Introduction to Prompt Engineering: How to talk to the AIs](https://www.linkedin.com/learning/prompt-engineering-how-to-talk-to-the-ais/talking-to-the-ais?u=0): Short video introduction to prompt engineering\n\n## Papers on advanced prompting to improve reasoning\n\n- [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (2022)](https://arxiv.org/abs/2201.11903): Using few-shot prompts to ask models to think step by step improves their reasoning. PaLM's score on math word problems (GSM8K) rises from 18% to 57%.\n- [Self-Consistency Improves Chain of Thought Reasoning in Language Models (2022)](https://arxiv.org/abs/2203.11171): Taking votes from multiple outputs improves accuracy even more. Voting across 40 outputs raises PaLM's score on math word problems further, from 57% to 74%, and `code-davinci-002`'s from 60% to 78%.\n- [Tree of Thoughts: Deliberate Problem Solving with Large Language Models (2023)](https://arxiv.org/abs/2305.10601): Searching over trees of step by step reasoning helps even more than voting over chains of thought. It lifts `GPT-4`'s scores on creative writing and crosswords.\n- [Language Models are Zero-Shot Reasoners (2022)](https://arxiv.org/abs/2205.11916): Telling instruction-following models to think step by step improves their reasoning. It lifts `text-davinci-002`'s score on math word problems (GSM8K) from 13% to 41%.\n- [Large Language Models Are Human-Level Prompt Engineers (2023)](https://arxiv.org/abs/2211.01910): Automated searching over possible prompts found a prompt that lifts scores on math word problems (GSM8K) to 43%, 2 percentage points above the human-written prompt in Language Models are Zero-Shot Reasoners.\n- [Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling (2023)](https://arxiv.org/abs/2305.09993): Automated searching over possible chain-of-thought prompts improved ChatGPT's scores on a few benchmarks by 0–20 percentage points.\n- [Faithful Reasoning Using Large Language Models (2022)](https://arxiv.org/abs/2208.14271): Reasoning can be improved by a system that combines: chains of thought generated by alternative selection and inference prompts, a halter model that chooses when to halt selection-inference loops, a value function to search over multiple reasoning paths, and sentence labels that help avoid hallucination.\n- [STaR: Bootstrapping Reasoning With Reasoning (2022)](https://arxiv.org/abs/2203.14465): Chain of thought reasoning can be baked into models via fine-tuning. For tasks with an answer key, example chains of thoughts can be generated by language models.\n- [ReAct: Synergizing Reasoning and Acting in Language Models (2023)](https://arxiv.org/abs/2210.03629): For tasks with tools or an environment, chain of thought works better if you prescriptively alternate between **Re**asoning steps (thinking about what to do) and **Act**ing (getting information from a tool or environment).\n- [Reflexion: an autonomous agent with dynamic memory and self-reflection (2023)](https://arxiv.org/abs/2303.11366): Retrying tasks with memory of prior failures improves subsequent performance.\n- [Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP (2023)](https://arxiv.org/abs/2212.14024): Models augmented with knowledge via a \"retrieve-then-read\" can be improved with multi-hop chains of searches.\n- [Improving Factuality and Reasoning in Language Models through Multiagent Debate (2023)](https://arxiv.org/abs/2305.14325): Generating debates between a few ChatGPT agents over a few rounds improves scores on various benchmarks. Math word problem scores rise from 77% to 85%.\n",
      "tags": [
        "prompts"
      ],
      "category": "prompt-engineering"
    },
    {
      "slug": "openai-how-to-count-tokens-with-tiktoken",
      "title": "How_to_count_tokens_with_tiktoken",
      "description": "{\n \"cells\":  \n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\":  \n    \"  How to count tokens with tiktoken\\n\",\n    \"\\n\",\n    \"  tiktoken  ...",
      "source": "openai",
      "sourceLabel": "OpenAI Cookbook",
      "sourceColor": "#10a37f",
      "url": "https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb",
      "content": "{\n \"cells\": [\n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# How to count tokens with tiktoken\\n\",\n    \"\\n\",\n    \"[`tiktoken`](https://github.com/openai/tiktoken/blob/main/README.md) is a fast open-source tokenizer by OpenAI.\\n\",\n    \"\\n\",\n    \"Given a text string (e.g., `\\\"tiktoken is great!\\\"`) and an encoding (e.g., `\\\"cl100k_base\\\"`), a tokenizer can split the text string into a list of tokens (e.g., `[\\\"t\\\", \\\"ik\\\", \\\"token\\\", \\\" is\\\", \\\" great\\\", \\\"!\\\"]`).\\n\",\n    \"\\n\",\n    \"Splitting text strings into tokens is useful because GPT models see text in the form of tokens. Knowing how many tokens are in a text string can tell you (a) whether the string is too long for a text model to process and (b) how much an OpenAI API call costs (as usage is priced by token).\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"## Encodings\\n\",\n    \"\\n\",\n    \"Encodings specify how text is converted into tokens. Different models use different encodings.\\n\",\n    \"\\n\",\n    \"`tiktoken` supports three encodings used by OpenAI models:\\n\",\n    \"\\n\",\n    \"| Encoding name           | OpenAI models                                       |\\n\",\n    \"|-------------------------|-----------------------------------------------------|\\n\",\n    \"| `o200k_base`            | `gpt-4o`, `gpt-4o-mini`                             |\\n\",\n    \"| `cl100k_base`           | `gpt-4-turbo`, `gpt-4`, `gpt-3.5-turbo`, `text-embedding-ada-002`, `text-embedding-3-small`, `text-embedding-3-large`  |\\n\",\n    \"| `p50k_base`             | Codex models, `text-davinci-002`, `text-davinci-003`|\\n\",\n    \"| `r50k_base` (or `gpt2`) | GPT-3 models like `davinci`                         |\\n\",\n    \"\\n\",\n    \"You can retrieve the encoding for a model using `tiktoken.encoding_for_model()` as follows:\\n\",\n    \"```python\\n\",\n    \"encoding = tiktoken.encoding_for_model('gpt-4o-mini')\\n\",\n    \"```\\n\",\n    \"\\n\",\n    \"Note that `p50k_base` overlaps substantially with `r50k_base`, and for non-code applications, they will usually give the same tokens.\\n\",\n    \"\\n\",\n    \"## Tokenizer libraries by language\\n\",\n    \"\\n\",\n    \"For `o200k_base`, `cl100k_base` and `p50k_base` encodings:\\n\",\n    \"- Python: [tiktoken](https://github.com/openai/tiktoken/blob/main/README.md)\\n\",\n    \"- .NET / C#: [SharpToken](https://github.com/dmitry-brazhenko/SharpToken), [TiktokenSharp](https://github.com/aiqinxuancai/TiktokenSharp)\\n\",\n    \"- Java: [jtokkit](https://github.com/knuddelsgmbh/jtokkit)\\n\",\n    \"- Golang: [tiktoken-go](https://github.com/pkoukk/tiktoken-go)\\n\",\n    \"- Rust: [tiktoken-rs](https://github.com/zurawiki/tiktoken-rs)\\n\",\n    \"\\n\",\n    \"For `r50k_base` (`gpt2`) encodings, tokenizers are available in many languages.\\n\",\n    \"- Python: [tiktoken](https://github.com/openai/tiktoken/blob/main/README.md) (or alternatively [GPT2TokenizerFast](https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2TokenizerFast))\\n\",\n    \"- JavaScript: [gpt-3-encoder](https://www.npmjs.com/package/gpt-3-encoder)\\n\",\n    \"- .NET / C#: [GPT Tokenizer](https://github.com/dluc/openai-tools)\\n\",\n    \"- Java: [gpt2-tokenizer-java](https://github.com/hyunwoongko/gpt2-tokenizer-java)\\n\",\n    \"- PHP: [GPT-3-Encoder-PHP](https://github.com/CodeRevolutionPlugins/GPT-3-Encoder-PHP)\\n\",\n    \"- Golang: [tiktoken-go](https://github.com/pkoukk/tiktoken-go)\\n\",\n    \"- Rust: [tiktoken-rs](https://github.com/zurawiki/tiktoken-rs)\\n\",\n    \"\\n\",\n    \"(OpenAI makes no endorsements or guarantees of third-party libraries.)\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"## How strings are typically tokenized\\n\",\n    \"\\n\",\n    \"In English, tokens commonly range in length from one character to one word (e.g., `\\\"t\\\"` or `\\\" great\\\"`), though in some languages tokens can be shorter than one character or longer than one word. Spaces are usually grouped with the starts of words (e.g., `\\\" is\\\"` instead of `\\\"is \\\"` or `\\\" \\\"`+`\\\"is\\\"`). You can quickly check how a string is tokenized at the [OpenAI Tokenizer](https://beta.openai.com/tokenizer), or the third-party [Tiktokenizer](https://tiktokenizer.vercel.app/) webapp.\"\n   ]\n  },\n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 0. Install `tiktoken`\\n\",\n    \"\\n\",\n    \"If needed, install `tiktoken` with `pip`:\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 1,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"\\n\",\n      \"\\u001b[1m[\\u001b[0m\\u001b[34;49mnotice\\u001b[0m\\u001b[1;39;49m]\\u001b[0m\\u001b[39;49m A new release of pip is available: \\u001b[0m\\u001b[31;49m24.0\\u001b[0m\\u001b[39;49m -> \\u001b[0m\\u001b[32;49m24.2\\u001b[0m\\n\",\n      \"\\u001b[1m[\\u001b[0m\\u001b[34;49mnotice\\u001b[0m\\u001b[1;39;49m]\\u001b[0m\\u001b[39;49m To update, run: \\u001b[0m\\u001b[32;49mpip install --upgrade pip\\u001b[0m\\n\",\n      \"Note: you may need to restart the kernel to use updated packages.\\n\",\n      \"\\n\",\n      \"\\u001b[1m[\\u001b[0m\\u001b[34;49mnotice\\u001b[0m\\u001b[1;39;49m]\\u001b[0m\\u001b[39;49m A new release of pip is available: \\u001b[0m\\u001b[31;49m24.0\\u001b[0m\\u001b[39;49m -> \\u001b[0m\\u001b[32;49m24.2\\u001b[0m\\n\",\n      \"\\u001b[1m[\\u001b[0m\\u001b[34;49mnotice\\u001b[0m\\u001b[1;39;49m]\\u001b[0m\\u001b[39;49m To update, run: \\u001b[0m\\u001b[32;49mpip install --upgrade pip\\u001b[0m\\n\",\n      \"Note: you may need to restart the kernel to use updated packages.\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"%pip install --upgrade tiktoken -q\\n\",\n    \"%pip install --upgrade openai -q\"\n   ]\n  },\n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 1. Import `tiktoken`\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 2,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import tiktoken\"\n   ]\n  },\n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 2. Load an encoding\\n\",\n    \"\\n\",\n    \"Use `tiktoken.get_encoding()` to load an encoding by name.\\n\",\n    \"\\n\",\n    \"The first time this runs, it will require an internet connection to download. Later runs won't need an internet connection.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 3,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"encoding = tiktoken.get_encoding(\\\"cl100k_base\\\")\\n\"\n   ]\n  },\n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Use `tiktoken.encoding_for_model()` to automatically load the correct encoding for a given model name.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 4,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"encoding = tiktoken.encoding_for_model(\\\"gpt-4o-mini\\\")\"\n   ]\n  },\n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 3. Turn text into tokens with `encoding.encode()`\\n\",\n    \"\\n\"\n   ]\n  },\n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"The `.encode()` method converts a text string into a list of token integers.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 5,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/plain\": [\n       \"[83, 8251, 2488, 382, 2212, 0]\"\n      ]\n     },\n     \"execution_count\": 5,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"encoding.encode(\\\"tiktoken is great!\\\")\"\n   ]\n  },\n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Count tokens by counting the length of the list returned by `.encode()`.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 6,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def num_tokens_from_string(string: str, encoding_name: str) -> int:\\n\",\n    \"    \\\"\\\"\\\"Returns the number of tokens in a text string.\\\"\\\"\\\"\\n\",\n    \"    encoding = tiktoken.get_encoding(encoding_name)\\n\",\n    \"    num_tokens = len(encoding.encode(string))\\n\",\n    \"    return num_tokens\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 7,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/plain\": [\n       \"6\"\n      ]\n     },\n     \"execution_count\": 7,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"num_tokens_from_string(\\\"tiktoken is great!\\\", \\\"o200k_base\\\")\"\n   ]\n  },\n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 4. Turn tokens into text with `encoding.decode()`\"\n   ]\n  },\n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"`.decode()` converts a list of token integers to a string.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 8,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/plain\": [\n       \"'tiktoken is great!'\"\n      ]\n     },\n     \"execution_count\": 8,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"encoding.decode([83, 8251, 2488, 382, 2212, 0])\"\n   ]\n  },\n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Warning: although `.decode()` can be applied to single tokens, beware that it can be lossy for tokens that aren't on utf-8 boundaries.\"\n   ]\n  },\n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"For single tokens, `.decode_single_token_bytes()` safely converts a single integer token to the bytes it represents.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 9,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/plain\": [\n       \"[b't', b'ikt', b'oken', b' is', b' great', b'!']\"\n      ]\n     },\n     \"execution_count\": 9,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"[encoding.decode_single_token_bytes(token) for token in [83, 8251, 2488, 382, 2212, 0]]\\n\"\n   ]\n  },\n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"(The `b` in front of the strings indicates that the strings are byte strings.)\"\n   ]\n  },\n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 5. Comparing encodings\\n\",\n    \"\\n\",\n    \"Different encodings vary in how they split words, group spaces, and handle non-English characters. Using the methods above, we can compare different encodings on a few example strings.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 10,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def compare_encodings(example_string: str) -> None:\\n\",\n    \"    \\\"\\\"\\\"Prints a comparison of three string encodings.\\\"\\\"\\\"\\n\",\n    \"    # print the example string\\n\",\n    \"    print(f'\\\\nExample string: \\\"{example_string}\\\"')\\n\",\n    \"    # for each encoding, print the # of tokens, the token integers, and the token bytes\\n\",\n    \"    for encoding_name in [\\\"r50k_base\\\", \\\"p50k_base\\\", \\\"cl100k_base\\\", \\\"o200k_base\\\"]:\\n\",\n    \"        encoding = tiktoken.get_encoding(encoding_name)\\n\",\n    \"        token_integers = encoding.encode(example_string)\\n\",\n    \"        num_tokens = len(token_integers)\\n\",\n    \"        token_bytes = [encoding.decode_single_token_bytes(token) for token in token_integers]\\n\",\n    \"        print()\\n\",\n    \"        print(f\\\"{encoding_name}: {num_tokens} tokens\\\")\\n\",\n    \"        print(f\\\"token integers: {token_integers}\\\")\\n\",\n    \"        print(f\\\"token bytes: {token_bytes}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 11,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"\\n\",\n      \"Example string: \\\"antidisestablishmentarianism\\\"\\n\",\n      \"\\n\",\n      \"r50k_base: 5 tokens\\n\",\n      \"token integers: [415, 29207, 44390, 3699, 1042]\\n\",\n      \"token bytes: [b'ant', b'idis', b'establishment', b'arian', b'ism']\\n\",\n      \"\\n\",\n      \"p50k_base: 5 tokens\\n\",\n      \"token integers: [415, 29207, 44390, 3699, 1042]\\n\",\n      \"token bytes: [b'ant', b'idis', b'establishment', b'arian', b'ism']\\n\",\n      \"\\n\",\n      \"cl100k_base: 6 tokens\\n\",\n      \"token integers: [519, 85342, 34500, 479, 8997, 2191]\\n\",\n      \"token bytes: [b'ant', b'idis', b'establish', b'ment', b'arian', b'ism']\\n\",\n      \"\\n\",\n      \"o200k_base: 6 tokens\\n\",\n      \"token integers: [493, 129901, 376, 160388, 21203, 2367]\\n\",\n      \"token bytes: [b'ant', b'idis', b'est', b'ablishment', b'arian', b'ism']\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"compare_encodings(\\\"antidisestablishmentarianism\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 12,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"\\n\",\n      \"Example string: \\\"2 + 2 = 4\\\"\\n\",\n      \"\\n\",\n      \"r50k_base: 5 tokens\\n\",\n      \"token integers: [17, 1343, 362, 796, 604]\\n\",\n      \"token bytes: [b'2', b' +', b' 2', b' =', b' 4']\\n\",\n      \"\\n\",\n      \"p50k_base: 5 tokens\\n\",\n      \"token integers: [17, 1343, 362, 796, 604]\\n\",\n      \"token bytes: [b'2', b' +', b' 2', b' =', b' 4']\\n\",\n      \"\\n\",\n      \"cl100k_base: 7 tokens\\n\",\n      \"token integers: [17, 489, 220, 17, 284, 220, 19]\\n\",\n      \"token bytes: [b'2', b' +', b' ', b'2', b' =', b' ', b'4']\\n\",\n      \"\\n\",\n      \"o200k_base: 7 tokens\\n\",\n      \"token integers: [17, 659, 220, 17, 314, 220, 19]\\n\",\n      \"token bytes: [b'2', b' +', b' ', b'2', b' =', b' ', b'4']\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"compare_encodings(\\\"2 + 2 = 4\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 13,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"\\n\",\n      \"Example string: \\\"お誕生日おめでとう\\\"\\n\",\n      \"\\n\",\n      \"r50k_base: 14 tokens\\n\",\n      \"token integers: [2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 29557]\\n\",\n      \"token bytes: [b'\\\\xe3\\\\x81', b'\\\\x8a', b'\\\\xe8\\\\xaa', b'\\\\x95', b'\\\\xe7\\\\x94\\\\x9f', b'\\\\xe6\\\\x97', b'\\\\xa5', b'\\\\xe3\\\\x81', b'\\\\x8a', b'\\\\xe3\\\\x82', b'\\\\x81', b'\\\\xe3\\\\x81\\\\xa7', b'\\\\xe3\\\\x81\\\\xa8', b'\\\\xe3\\\\x81\\\\x86']\\n\",\n      \"\\n\",\n      \"p50k_base: 14 tokens\\n\",\n      \"token integers: [2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 29557]\\n\",\n      \"token bytes: [b'\\\\xe3\\\\x81', b'\\\\x8a', b'\\\\xe8\\\\xaa', b'\\\\x95', b'\\\\xe7\\\\x94\\\\x9f', b'\\\\xe6\\\\x97', b'\\\\xa5', b'\\\\xe3\\\\x81', b'\\\\x8a', b'\\\\xe3\\\\x82', b'\\\\x81', b'\\\\xe3\\\\x81\\\\xa7', b'\\\\xe3\\\\x81\\\\xa8', b'\\\\xe3\\\\x81\\\\x86']\\n\",\n      \"\\n\",\n      \"cl100k_base: 9 tokens\\n\",\n      \"token integers: [33334, 45918, 243, 21990, 9080, 33334, 62004, 16556, 78699]\\n\",\n      \"token bytes: [b'\\\\xe3\\\\x81\\\\x8a', b'\\\\xe8\\\\xaa', b'\\\\x95', b'\\\\xe7\\\\x94\\\\x9f', b'\\\\xe6\\\\x97\\\\xa5', b'\\\\xe3\\\\x81\\\\x8a', b'\\\\xe3\\\\x82\\\\x81', b'\\\\xe3\\\\x81\\\\xa7', b'\\\\xe3\\\\x81\\\\xa8\\\\xe3\\\\x81\\\\x86']\\n\",\n      \"\\n\",\n      \"o200k_base: 8 tokens\\n\",\n      \"token integers: [8930, 9697, 243, 128225, 8930, 17693, 4344, 48669]\\n\",\n      \"token bytes: [b'\\\\xe3\\\\x81\\\\x8a', b'\\\\xe8\\\\xaa', b'\\\\x95', b'\\\\xe7\\\\x94\\\\x9f\\\\xe6\\\\x97\\\\xa5', b'\\\\xe3\\\\x81\\\\x8a', b'\\\\xe3\\\\x82\\\\x81', b'\\\\xe3\\\\x81\\\\xa7', b'\\\\xe3\\\\x81\\\\xa8\\\\xe3\\\\x81\\\\x86']\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"compare_encodings(\\\"お誕生日おめでとう\\\")\"\n   ]\n  },\n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 6. Counting tokens for chat completions API calls\\n\",\n    \"\\n\",\n    \"ChatGPT models like `gpt-4o-mini` and `gpt-4` use tokens in the same way as older completions models, but because of their message-based formatting, it's more difficult to count how many tokens will be used by a conversation.\\n\",\n    \"\\n\",\n    \"Below is an example function for counting tokens for messages passed to `gpt-3.5-turbo`, `gpt-4`, `gpt-4o` and `gpt-4o-mini`.\\n\",\n    \"\\n\",\n    \"Note that the exact way that tokens are counted from messages may change from model to model. Consider the counts from the function below an estimate, not a timeless guarantee.\\n\",\n    \"\\n\",\n    \"In particular, requests that use the optional functions input will consume extra tokens on top of the estimates calculated below.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 14,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def num_tokens_from_messages(messages, model=\\\"gpt-4o-mini-2024-07-18\\\"):\\n\",\n    \"    \\\"\\\"\\\"Return the number of tokens used by a list of messages.\\\"\\\"\\\"\\n\",\n    \"    try:\\n\",\n    \"        encoding = tiktoken.encoding_for_model(model)\\n\",\n    \"    except KeyError:\\n\",\n    \"        print(\\\"Warning: model not found. Using o200k_base encoding.\\\")\\n\",\n    \"        encoding = tiktoken.get_encoding(\\\"o200k_base\\\")\\n\",\n    \"    if model in {\\n\",\n    \"        \\\"gpt-3.5-turbo-0125\\\",\\n\",\n    \"        \\\"gpt-4-0314\\\",\\n\",\n    \"        \\\"gpt-4-32k-0314\\\",\\n\",\n    \"        \\\"gpt-4-0613\\\",\\n\",\n    \"        \\\"gpt-4-32k-0613\\\",\\n\",\n    \"        \\\"gpt-4o-mini-2024-07-18\\\",\\n\",\n    \"        \\\"gpt-4o-2024-08-06\\\"\\n\",\n    \"        }:\\n\",\n    \"        tokens_per_message = 3\\n\",\n    \"        tokens_per_name = 1\\n\",\n    \"    elif \\\"gpt-3.5-turbo\\\" in model:\\n\",\n    \"        print(\\\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0125.\\\")\\n\",\n    \"        return num_tokens_from_messages(messages, model=\\\"gpt-3.5-turbo-0125\\\")\\n\",\n    \"    elif \\\"gpt-4o-mini\\\" in model:\\n\",\n    \"        print(\\\"Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\\\")\\n\",\n    \"        return num_tokens_from_messages(messages, model=\\\"gpt-4o-mini-2024-07-18\\\")\\n\",\n    \"    elif \\\"gpt-4o\\\" in model:\\n\",\n    \"        print(\\\"Warning: gpt-4o and gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-2024-08-06.\\\")\\n\",\n    \"        return num_tokens_from_messages(messages, model=\\\"gpt-4o-2024-08-06\\\")\\n\",\n    \"    elif \\\"gpt-4\\\" in model:\\n\",\n    \"        print(\\\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\\\")\\n\",\n    \"        return num_tokens_from_messages(messages, model=\\\"gpt-4-0613\\\")\\n\",\n    \"    else:\\n\",\n    \"        raise NotImplementedError(\\n\",\n    \"            f\\\"\\\"\\\"num_tokens_from_messages() is not implemented for model {model}.\\\"\\\"\\\"\\n\",\n    \"        )\\n\",\n    \"    num_tokens = 0\\n\",\n    \"    for message in messages:\\n\",\n    \"        num_tokens += tokens_per_message\\n\",\n    \"        for key, value in message.items():\\n\",\n    \"            num_tokens += len(encoding.encode(value))\\n\",\n    \"            if key == \\\"name\\\":\\n\",\n    \"                num_tokens += tokens_per_name\\n\",\n    \"    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\\n\",\n    \"    return num_tokens\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 15,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"gpt-3.5-turbo\\n\",\n      \"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0125.\\n\",\n      \"129 prompt tokens counted by num_tokens_from_messages().\\n\",\n      \"129 prompt tokens counted by the OpenAI API.\\n\",\n      \"\\n\",\n      \"gpt-4-0613\\n\",\n      \"129 prompt tokens counted by num_tokens_from_messages().\\n\",\n      \"129 prompt tokens counted by the OpenAI API.\\n\",\n      \"\\n\",\n      \"gpt-4\\n\",\n      \"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\\n\",\n      \"129 prompt tokens counted by num_tokens_from_messages().\\n\",\n      \"129 prompt tokens counted by the OpenAI API.\\n\",\n      \"\\n\",\n      \"gpt-4o\\n\",\n      \"Warning: gpt-4o and gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-2024-08-06.\\n\",\n      \"124 prompt tokens counted by num_tokens_from_messages().\\n\",\n      \"124 prompt tokens counted by the OpenAI API.\\n\",\n      \"\\n\",\n      \"gpt-4o-mini\\n\",\n      \"Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\\n\",\n      \"124 prompt tokens counted by num_tokens_from_messages().\\n\",\n      \"124 prompt tokens counted by the OpenAI API.\\n\",\n      \"\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"# let's verify the function above matches the OpenAI API response\\n\",\n    \"\\n\",\n    \"from openai import OpenAI\\n\",\n    \"import os\\n\",\n    \"\\n\",\n    \"client = OpenAI(api_key=os.environ.get(\\\"OPENAI_API_KEY\\\", \\\"<your OpenAI API key if not set as env var>\\\"))\\n\",\n    \"\\n\",\n    \"example_messages = [\\n\",\n    \"    {\\n\",\n    \"        \\\"role\\\": \\\"system\\\",\\n\",\n    \"        \\\"content\\\": \\\"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\\\",\\n\",\n    \"    },\\n\",\n    \"    {\\n\",\n    \"        \\\"role\\\": \\\"system\\\",\\n\",\n    \"        \\\"name\\\": \\\"example_user\\\",\\n\",\n    \"        \\\"content\\\": \\\"New synergies will help drive top-line growth.\\\",\\n\",\n    \"    },\\n\",\n    \"    {\\n\",\n    \"        \\\"role\\\": \\\"system\\\",\\n\",\n    \"        \\\"name\\\": \\\"example_assistant\\\",\\n\",\n    \"        \\\"content\\\": \\\"Things working well together will increase revenue.\\\",\\n\",\n    \"    },\\n\",\n    \"    {\\n\",\n    \"        \\\"role\\\": \\\"system\\\",\\n\",\n    \"        \\\"name\\\": \\\"example_user\\\",\\n\",\n    \"        \\\"content\\\": \\\"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\\\",\\n\",\n    \"    },\\n\",\n    \"    {\\n\",\n    \"        \\\"role\\\": \\\"system\\\",\\n\",\n    \"        \\\"name\\\": \\\"example_assistant\\\",\\n\",\n    \"        \\\"content\\\": \\\"Let's talk later when we're less busy about how to do better.\\\",\\n\",\n    \"    },\\n\",\n    \"    {\\n\",\n    \"        \\\"role\\\": \\\"user\\\",\\n\",\n    \"        \\\"content\\\": \\\"This late pivot means we don't have time to boil the ocean for the client deliverable.\\\",\\n\",\n    \"    },\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"for model in [\\n\",\n    \"    \\\"gpt-3.5-turbo\\\",\\n\",\n    \"    \\\"gpt-4-0613\\\",\\n\",\n    \"    \\\"gpt-4\\\",\\n\",\n    \"    \\\"gpt-4o\\\",\\n\",\n    \"    \\\"gpt-4o-mini\\\"\\n\",\n    \"    ]:\\n\",\n    \"    print(model)\\n\",\n    \"    # example token count from the function defined above\\n\",\n    \"    print(f\\\"{num_tokens_from_messages(example_messages, model)} prompt tokens counted by num_tokens_from_messages().\\\")\\n\",\n    \"    # example token count from the OpenAI API\\n\",\n    \"    response = client.chat.completions.create(model=model,\\n\",\n    \"    messages=example_messages,\\n\",\n    \"    temperature=0,\\n\",\n    \"    max_tokens=1)\\n\",\n    \"    print(f'{response.usage.prompt_tokens} prompt tokens counted by the OpenAI API.')\\n\",\n    \"    print()\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 7. Counting tokens for chat completions with tool calls\\n\",\n    \"\\n\",\n    \"Next, we will look into how to apply this calculations to messages that may contain function calls. This is not immediately trivial, due to the formatting of the tools themselves. \\n\",\n    \"\\n\",\n    \"Below is an example function for counting tokens for messages that contain tools, passed to `gpt-3.5-turbo`, `gpt-4`, `gpt-4o` and `gpt-4o-mini`.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 16,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def num_tokens_for_tools(functions, messages, model):\\n\",\n    \"    \\n\",\n    \"    # Initialize function settings to 0\\n\",\n    \"    func_init = 0\\n\",\n    \"    prop_init = 0\\n\",\n    \"    prop_key = 0\\n\",\n    \"    enum_init = 0\\n\",\n    \"    enum_item = 0\\n\",\n    \"    func_end = 0\\n\",\n    \"    \\n\",\n    \"    if model in [\\n\",\n    \"        \\\"gpt-4o\\\",\\n\",\n    \"        \\\"gpt-4o-mini\\\"\\n\",\n    \"    ]:\\n\",\n    \"        \\n\",\n    \"        # Set function settings for the above models\\n\",\n    \"        func_init = 7\\n\",\n    \"        prop_init = 3\\n\",\n    \"        prop_key = 3\\n\",\n    \"        enum_init = -3\\n\",\n    \"        enum_item = 3\\n\",\n    \"        func_end = 12\\n\",\n    \"    elif model in [\\n\",\n    \"        \\\"gpt-3.5-turbo\\\",\\n\",\n    \"        \\\"gpt-4\\\"\\n\",\n    \"    ]:\\n\",\n    \"        # Set function settings for the above models\\n\",\n    \"        func_init = 10\\n\",\n    \"        prop_init = 3\\n\",\n    \"        prop_key = 3\\n\",\n    \"        enum_init = -3\\n\",\n    \"        enum_item = 3\\n\",\n    \"        func_end = 12\\n\",\n    \"    else:\\n\",\n    \"        raise NotImplementedError(\\n\",\n    \"            f\\\"\\\"\\\"num_tokens_for_tools() is not implemented for model {model}.\\\"\\\"\\\"\\n\",\n    \"        )\\n\",\n    \"    \\n\",\n    \"    try:\\n\",\n    \"        encoding = tiktoken.encoding_for_model(model)\\n\",\n    \"    except KeyError:\\n\",\n    \"        print(\\\"Warning: model not found. Using o200k_base encoding.\\\")\\n\",\n    \"        encoding = tiktoken.get_encoding(\\\"o200k_base\\\")\\n\",\n    \"    \\n\",\n    \"    func_token_count = 0\\n\",\n    \"    if len(functions) > 0:\\n\",\n    \"        for f in functions:\\n\",\n    \"            func_token_count += func_init  # Add tokens for start of each function\\n\",\n    \"            function = f[\\\"function\\\"]\\n\",\n    \"            f_name = function[\\\"name\\\"]\\n\",\n    \"            f_desc = function[\\\"description\\\"]\\n\",\n    \"            if f_desc.endswith(\\\".\\\"):\\n\",\n    \"                f_desc = f_desc[:-1]\\n\",\n    \"            line = f_name + \\\":\\\" + f_desc\\n\",\n    \"            func_token_count += len(encoding.encode(line))  # Add tokens for set name and description\\n\",\n    \"            if len(function[\\\"parameters\\\"][\\\"properties\\\"]) > 0:\\n\",\n    \"                func_token_count += prop_init  # Add tokens for start of each property\\n\",\n    \"                for key in list(function[\\\"parameters\\\"][\\\"properties\\\"].keys()):\\n\",\n    \"                    func_token_count += prop_key  # Add tokens for each set property\\n\",\n    \"                    p_name = key\\n\",\n    \"                    p_type = function[\\\"parameters\\\"][\\\"properties\\\"][key][\\\"type\\\"]\\n\",\n    \"                    p_desc = function[\\\"parameters\\\"][\\\"properties\\\"][key][\\\"description\\\"]\\n\",\n    \"                    if \\\"enum\\\" in function[\\\"parameters\\\"][\\\"properties\\\"][key].keys():\\n\",\n    \"                        func_token_count += enum_init  # Add tokens if property has enum list\\n\",\n    \"                        for item in function[\\\"parameters\\\"][\\\"properties\\\"][key][\\\"enum\\\"]:\\n\",\n    \"                            func_token_count += enum_item\\n\",\n    \"                            func_token_count += len(encoding.encode(item))\\n\",\n    \"                    if p_desc.endswith(\\\".\\\"):\\n\",\n    \"                        p_desc = p_desc[:-1]\\n\",\n    \"                    line = f\\\"{p_name}:{p_type}:{p_desc}\\\"\\n\",\n    \"                    func_token_count += len(encoding.encode(line))\\n\",\n    \"        func_token_count += func_end\\n\",\n    \"        \\n\",\n    \"    messages_token_count = num_tokens_from_messages(messages, model)\\n\",\n    \"    total_tokens = messages_token_count + func_token_count\\n\",\n    \"    \\n\",\n    \"    return total_tokens\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 17,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"gpt-3.5-turbo\\n\",\n      \"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0125.\\n\",\n      \"105 prompt tokens counted by num_tokens_for_tools().\\n\",\n      \"105 prompt tokens counted by the OpenAI API.\\n\",\n      \"\\n\",\n      \"gpt-4\\n\",\n      \"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\\n\",\n      \"105 prompt tokens counted by num_tokens_for_tools().\\n\",\n      \"105 prompt tokens counted by the OpenAI API.\\n\",\n      \"\\n\",\n      \"gpt-4o\\n\",\n      \"Warning: gpt-4o and gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-2024-08-06.\\n\",\n      \"101 prompt tokens counted by num_tokens_for_tools().\\n\",\n      \"101 prompt tokens counted by the OpenAI API.\\n\",\n      \"\\n\",\n      \"gpt-4o-mini\\n\",\n      \"Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\\n\",\n      \"101 prompt tokens counted by num_tokens_for_tools().\\n\",\n      \"101 prompt tokens counted by the OpenAI API.\\n\",\n      \"\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"tools = [\\n\",\n    \"  {\\n\",\n    \"    \\\"type\\\": \\\"function\\\",\\n\",\n    \"    \\\"function\\\": {\\n\",\n    \"      \\\"name\\\": \\\"get_current_weather\\\",\\n\",\n    \"      \\\"description\\\": \\\"Get the current weather in a given location\\\",\\n\",\n    \"      \\\"parameters\\\": {\\n\",\n    \"        \\\"type\\\": \\\"object\\\",\\n\",\n    \"        \\\"properties\\\": {\\n\",\n    \"          \\\"location\\\": {\\n\",\n    \"            \\\"type\\\": \\\"string\\\",\\n\",\n    \"            \\\"description\\\": \\\"The city and state, e.g. San Francisco, CA\\\",\\n\",\n    \"          },\\n\",\n    \"          \\\"unit\\\": {\\\"type\\\": \\\"string\\\", \\n\",\n    \"                   \\\"description\\\": \\\"The unit of temperature to return\\\",\\n\",\n    \"                   \\\"enum\\\": [\\\"celsius\\\", \\\"fahrenheit\\\"]},\\n\",\n    \"        },\\n\",\n    \"        \\\"required\\\": [\\\"location\\\"],\\n\",\n    \"      },\\n\",\n    \"    }\\n\",\n    \"  }\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"example_messages = [\\n\",\n    \"    {\\n\",\n    \"        \\\"role\\\": \\\"system\\\",\\n\",\n    \"        \\\"content\\\": \\\"You are a helpful assistant that can answer to questions about the weather.\\\",\\n\",\n    \"    },\\n\",\n    \"    {\\n\",\n    \"        \\\"role\\\": \\\"user\\\",\\n\",\n    \"        \\\"content\\\": \\\"What's the weather like in San Francisco?\\\",\\n\",\n    \"    },\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"for model in [\\n\",\n    \"    \\\"gpt-3.5-turbo\\\",\\n\",\n    \"    \\\"gpt-4\\\",\\n\",\n    \"    \\\"gpt-4o\\\",\\n\",\n    \"    \\\"gpt-4o-mini\\\"\\n\",\n    \"    ]:\\n\",\n    \"    print(model)\\n\",\n    \"    # example token count from the function defined above\\n\",\n    \"    print(f\\\"{num_tokens_for_tools(tools, example_messages, model)} prompt tokens counted by num_tokens_for_tools().\\\")\\n\",\n    \"    # example token count from the OpenAI API\\n\",\n    \"    response = client.chat.completions.create(model=model,\\n\",\n    \"          messages=example_messages,\\n\",\n    \"          tools=tools,\\n\",\n    \"          temperature=0)\\n\",\n    \"    print(f'{response.usage.prompt_tokens} prompt tokens counted by the OpenAI API.')\\n\",\n    \"    print()\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.11.7\"\n  },\n  \"vscode\": {\n   \"interpreter\": {\n    \"hash\": \"365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97\"\n   }\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 2\n}\n",
      "tags": [
        "tokens",
        "prompts",
        "embeddings",
        "tiktoken",
        "streaming"
      ],
      "category": "token-optimization"
    },
    {
      "slug": "openai-how-to-stream-completions",
      "title": "How_to_stream_completions",
      "description": "{\n \"cells\":  \n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\":  \n    \"  How to stream completions\\n\",\n    \"\\n\",\n    \"By default, when you request a completion...",
      "source": "openai",
      "sourceLabel": "OpenAI Cookbook",
      "sourceColor": "#10a37f",
      "url": "https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb",
      "content": "{\n \"cells\": [\n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# How to stream completions\\n\",\n    \"\\n\",\n    \"By default, when you request a completion from the OpenAI, the entire completion is generated before being sent back in a single response.\\n\",\n    \"\\n\",\n    \"If you're generating long completions, waiting for the response can take many seconds.\\n\",\n    \"\\n\",\n    \"To get responses sooner, you can 'stream' the completion as it's being generated. This allows you to start printing or processing the beginning of the completion before the full completion is finished.\\n\",\n    \"\\n\",\n    \"To stream completions, set `stream=True` when calling the chat completions or completions endpoints. This will return an object that streams back the response as [data-only server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format). Extract chunks from the `delta` field rather than the `message` field.\\n\",\n    \"\\n\",\n    \"## Downsides\\n\",\n    \"\\n\",\n    \"Note that using `stream=True` in a production application makes it more difficult to moderate the content of the completions, as partial completions may be more difficult to evaluate. This may have implications for [approved usage](https://beta.openai.com/docs/usage-guidelines).\\n\",\n    \"\\n\",\n    \"## Example code\\n\",\n    \"\\n\",\n    \"Below, this notebook shows:\\n\",\n    \"1. What a typical chat completion response looks like\\n\",\n    \"2. What a streaming chat completion response looks like\\n\",\n    \"3. How much time is saved by streaming a chat completion\\n\",\n    \"4. How to get token usage data for streamed chat completion response\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 1,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# !pip install openai\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 2,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# imports\\n\",\n    \"import time  # for measuring time duration of API calls\\n\",\n    \"from openai import OpenAI\\n\",\n    \"import os\\n\",\n    \"client = OpenAI(api_key=os.environ.get(\\\"OPENAI_API_KEY\\\", \\\"<your OpenAI API key if not set as env var>\\\"))\"\n   ]\n  },\n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### 1. What a typical chat completion response looks like\\n\",\n    \"\\n\",\n    \"With a typical ChatCompletions API call, the response is first computed and then returned all at once.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 3,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"Full response received 1.88 seconds after request\\n\",\n      \"Full response received:\\n\",\n      \"ChatCompletion(id='chatcmpl-9lMgdoiMfxVHPDNVCtvXuTWcQ2GGb', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100', role='assistant', function_call=None, tool_calls=None))], created=1721075651, model='gpt-july-test', object='chat.completion', system_fingerprint='fp_e9b8ed65d2', usage=CompletionUsage(completion_tokens=298, prompt_tokens=36, total_tokens=334))\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"# Example of an OpenAI ChatCompletion request\\n\",\n    \"# https://platform.openai.com/docs/guides/text-generation/chat-completions-api\\n\",\n    \"\\n\",\n    \"# record the time before the request is sent\\n\",\n    \"start_time = time.time()\\n\",\n    \"\\n\",\n    \"# send a ChatCompletion request to count to 100\\n\",\n    \"response = client.chat.completions.create(\\n\",\n    \"    model='gpt-4o-mini',\\n\",\n    \"    messages=[\\n\",\n    \"        {'role': 'user', 'content': 'Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}\\n\",\n    \"    ],\\n\",\n    \"    temperature=0,\\n\",\n    \")\\n\",\n    \"# calculate the time it took to receive the response\\n\",\n    \"response_time = time.time() - start_time\\n\",\n    \"\\n\",\n    \"# print the time delay and text received\\n\",\n    \"print(f\\\"Full response received {response_time:.2f} seconds after request\\\")\\n\",\n    \"print(f\\\"Full response received:\\\\n{response}\\\")\\n\"\n   ]\n  },\n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"The reply can be extracted with `response.choices[0].message`.\\n\",\n    \"\\n\",\n    \"The content of the reply can be extracted with `response.choices[0].message.content`.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 4,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"Extracted reply: \\n\",\n      \"ChatCompletionMessage(content='1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100', role='assistant', function_call=None, tool_calls=None)\\n\",\n      \"Extracted content: \\n\",\n      \"1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"reply = response.choices[0].message\\n\",\n    \"print(f\\\"Extracted reply: \\\\n{reply}\\\")\\n\",\n    \"\\n\",\n    \"reply_content = response.choices[0].message.content\\n\",\n    \"print(f\\\"Extracted content: \\\\n{reply_content}\\\")\\n\"\n   ]\n  },\n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### 2. How to stream a chat completion\\n\",\n    \"\\n\",\n    \"With a streaming API call, the response is sent back incrementally in chunks via an [event stream](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format). In Python, you can iterate over these events with a `for` loop.\\n\",\n    \"\\n\",\n    \"Let's see what it looks like:\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 5,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"ChatCompletionChunk(id='chatcmpl-9lMgfRSWPHcw51s6wxKT1YEO2CKpd', choices=[Choice(delta=ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721075653, model='gpt-july-test', object='chat.completion.chunk', system_fingerprint='fp_e9b8ed65d2', usage=None)\\n\",\n      \"\\n\",\n      \"****************\\n\",\n      \"ChatCompletionChunk(id='chatcmpl-9lMgfRSWPHcw51s6wxKT1YEO2CKpd', choices=[Choice(delta=ChoiceDelta(content='Two', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721075653, model='gpt-july-test', object='chat.completion.chunk', system_fingerprint='fp_e9b8ed65d2', usage=None)\\n\",\n      \"Two\\n\",\n      \"****************\\n\",\n      \"ChatCompletionChunk(id='chatcmpl-9lMgfRSWPHcw51s6wxKT1YEO2CKpd', choices=[Choice(delta=ChoiceDelta(content='.', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721075653, model='gpt-july-test', object='chat.completion.chunk', system_fingerprint='fp_e9b8ed65d2', usage=None)\\n\",\n      \".\\n\",\n      \"****************\\n\",\n      \"ChatCompletionChunk(id='chatcmpl-9lMgfRSWPHcw51s6wxKT1YEO2CKpd', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None), finish_reason='stop', index=0, logprobs=None)], created=1721075653, model='gpt-july-test', object='chat.completion.chunk', system_fingerprint='fp_e9b8ed65d2', usage=None)\\n\",\n      \"None\\n\",\n      \"****************\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"# Example of an OpenAI ChatCompletion request with stream=True\\n\",\n    \"# https://platform.openai.com/docs/api-reference/streaming#chat/create-stream\\n\",\n    \"\\n\",\n    \"# a ChatCompletion request\\n\",\n    \"response = client.chat.completions.create(\\n\",\n    \"    model='gpt-4o-mini',\\n\",\n    \"    messages=[\\n\",\n    \"        {'role': 'user', 'content': \\\"What's 1+1? Answer in one word.\\\"}\\n\",\n    \"    ],\\n\",\n    \"    temperature=0,\\n\",\n    \"    stream=True  # this time, we set stream=True\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"for chunk in response:\\n\",\n    \"    print(chunk)\\n\",\n    \"    print(chunk.choices[0].delta.content)\\n\",\n    \"    print(\\\"****************\\\")\"\n   ]\n  },\n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"As you can see above, streaming responses have a `delta` field rather than a `message` field. `delta` can hold things like:\\n\",\n    \"- a role token (e.g., `{\\\"role\\\": \\\"assistant\\\"}`)\\n\",\n    \"- a content token (e.g., `{\\\"content\\\": \\\"\\\\n\\\\n\\\"}`)\\n\",\n    \"- nothing (e.g., `{}`), when the stream is over\"\n   ]\n  },\n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### 3. How much time is saved by streaming a chat completion\\n\",\n    \"\\n\",\n    \"Now let's ask `gpt-4o-mini` to count to 100 again, and see how long it takes.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 6,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"Message received 1.14 seconds after request: \\n\",\n      \"Message received 1.14 seconds after request: 1\\n\",\n      \"Message received 1.14 seconds after request: ,\\n\",\n      \"Message received 1.14 seconds after request:  \\n\",\n      \"Message received 1.14 seconds after request: 2\\n\",\n      \"Message received 1.16 seconds after request: ,\\n\",\n      \"Message received 1.16 seconds after request:  \\n\",\n      \"Message received 1.16 seconds after request: 3\\n\",\n      \"Message received 1.35 seconds after request: ,\\n\",\n      \"Message received 1.35 seconds after request:  \\n\",\n      \"Message received 1.35 seconds after request: 4\\n\",\n      \"Message received 1.36 seconds after request: ,\\n\",\n      \"Message received 1.36 seconds after request:  \\n\",\n      \"Message received 1.36 seconds after request: 5\\n\",\n      \"Message received 1.36 seconds after request: ,\\n\",\n      \"Message received 1.36 seconds after request:  \\n\",\n      \"Message received 1.36 seconds after request: 6\\n\",\n      \"Message received 1.36 seconds after request: ,\\n\",\n      \"Message received 1.36 seconds after request:  \\n\",\n      \"Message received 1.36 seconds after request: 7\\n\",\n      \"Message received 1.36 seconds after request: ,\\n\",\n      \"Message received 1.36 seconds after request:  \\n\",\n      \"Message received 1.36 seconds after request: 8\\n\",\n      \"Message received 1.36 seconds after request: ,\\n\",\n      \"Message received 1.36 seconds after request:  \\n\",\n      \"Message received 1.36 seconds after request: 9\\n\",\n      \"Message received 1.36 seconds after request: ,\\n\",\n      \"Message received 1.36 seconds after request:  \\n\",\n      \"Message received 1.36 seconds after request: 10\\n\",\n      \"Message received 1.36 seconds after request: ,\\n\",\n      \"Message received 1.36 seconds after request:  \\n\",\n      \"Message received 1.36 seconds after request: 11\\n\",\n      \"Message received 1.36 seconds after request: ,\\n\",\n      \"Message received 1.36 seconds after request:  \\n\",\n      \"Message received 1.36 seconds after request: 12\\n\",\n      \"Message received 1.36 seconds after request: ,\\n\",\n      \"Message received 1.36 seconds after request:  \\n\",\n      \"Message received 1.45 seconds after request: 13\\n\",\n      \"Message received 1.45 seconds after request: ,\\n\",\n      \"Message received 1.45 seconds after request:  \\n\",\n      \"Message received 1.45 seconds after request: 14\\n\",\n      \"Message received 1.45 seconds after request: ,\\n\",\n      \"Message received 1.45 seconds after request:  \\n\",\n      \"Message received 1.45 seconds after request: 15\\n\",\n      \"Message received 1.45 seconds after request: ,\\n\",\n      \"Message received 1.45 seconds after request:  \\n\",\n      \"Message received 1.46 seconds after request: 16\\n\",\n      \"Message received 1.46 seconds after request: ,\\n\",\n      \"Message received 1.46 seconds after request:  \\n\",\n      \"Message received 1.47 seconds after request: 17\\n\",\n      \"Message received 1.47 seconds after request: ,\\n\",\n      \"Message received 1.47 seconds after request:  \\n\",\n      \"Message received 1.49 seconds after request: 18\\n\",\n      \"Message received 1.49 seconds after request: ,\\n\",\n      \"Message received 1.49 seconds after request:  \\n\",\n      \"Message received 1.52 seconds after request: 19\\n\",\n      \"Message received 1.52 seconds after request: ,\\n\",\n      \"Message received 1.52 seconds after request:  \\n\",\n      \"Message received 1.53 seconds after request: 20\\n\",\n      \"Message received 1.53 seconds after request: ,\\n\",\n      \"Message received 1.53 seconds after request:  \\n\",\n      \"Message received 1.55 seconds after request: 21\\n\",\n      \"Message received 1.55 seconds after request: ,\\n\",\n      \"Message received 1.55 seconds after request:  \\n\",\n      \"Message received 1.56 seconds after request: 22\\n\",\n      \"Message received 1.56 seconds after request: ,\\n\",\n      \"Message received 1.56 seconds after request:  \\n\",\n      \"Message received 1.58 seconds after request: 23\\n\",\n      \"Message received 1.58 seconds after request: ,\\n\",\n      \"Message received 1.58 seconds after request:  \\n\",\n      \"Message received 1.59 seconds after request: 24\\n\",\n      \"Message received 1.59 seconds after request: ,\\n\",\n      \"Message received 1.59 seconds after request:  \\n\",\n      \"Message received 1.62 seconds after request: 25\\n\",\n      \"Message received 1.62 seconds after request: ,\\n\",\n      \"Message received 1.62 seconds after request:  \\n\",\n      \"Message received 1.62 seconds after request: 26\\n\",\n      \"Message received 1.62 seconds after request: ,\\n\",\n      \"Message received 1.62 seconds after request:  \\n\",\n      \"Message received 1.65 seconds after request: 27\\n\",\n      \"Message received 1.65 seconds after request: ,\\n\",\n      \"Message received 1.65 seconds after request:  \\n\",\n      \"Message received 1.67 seconds after request: 28\\n\",\n      \"Message received 1.67 seconds after request: ,\\n\",\n      \"Message received 1.67 seconds after request:  \\n\",\n      \"Message received 1.69 seconds after request: 29\\n\",\n      \"Message received 1.69 seconds after request: ,\\n\",\n      \"Message received 1.69 seconds after request:  \\n\",\n      \"Message received 1.80 seconds after request: 30\\n\",\n      \"Message received 1.80 seconds after request: ,\\n\",\n      \"Message received 1.80 seconds after request:  \\n\",\n      \"Message received 1.80 seconds after request: 31\\n\",\n      \"Message received 1.80 seconds after request: ,\\n\",\n      \"Message received 1.80 seconds after request:  \\n\",\n      \"Message received 1.80 seconds after request: 32\\n\",\n      \"Message received 1.80 seconds after request: ,\\n\",\n      \"Message received 1.80 seconds after request:  \\n\",\n      \"Message received 1.80 seconds after request: 33\\n\",\n      \"Message received 1.80 seconds after request: ,\\n\",\n      \"Message received 1.80 seconds after request:  \\n\",\n      \"Message received 1.80 seconds after request: 34\\n\",\n      \"Message received 1.80 seconds after request: ,\\n\",\n      \"Message received 1.80 seconds after request:  \\n\",\n      \"Message received 1.80 seconds after request: 35\\n\",\n      \"Message received 1.80 seconds after request: ,\\n\",\n      \"Message received 1.80 seconds after request:  \\n\",\n      \"Message received 1.80 seconds after request: 36\\n\",\n      \"Message received 1.80 seconds after request: ,\\n\",\n      \"Message received 1.80 seconds after request:  \\n\",\n      \"Message received 1.82 seconds after request: 37\\n\",\n      \"Message received 1.82 seconds after request: ,\\n\",\n      \"Message received 1.82 seconds after request:  \\n\",\n      \"Message received 1.83 seconds after request: 38\\n\",\n      \"Message received 1.83 seconds after request: ,\\n\",\n      \"Message received 1.83 seconds after request:  \\n\",\n      \"Message received 1.84 seconds after request: 39\\n\",\n      \"Message received 1.84 seconds after request: ,\\n\",\n      \"Message received 1.84 seconds after request:  \\n\",\n      \"Message received 1.87 seconds after request: 40\\n\",\n      \"Message received 1.87 seconds after request: ,\\n\",\n      \"Message received 1.87 seconds after request:  \\n\",\n      \"Message received 1.88 seconds after request: 41\\n\",\n      \"Message received 1.88 seconds after request: ,\\n\",\n      \"Message received 1.88 seconds after request:  \\n\",\n      \"Message received 1.91 seconds after request: 42\\n\",\n      \"Message received 1.91 seconds after request: ,\\n\",\n      \"Message received 1.91 seconds after request:  \\n\",\n      \"Message received 1.93 seconds after request: 43\\n\",\n      \"Message received 1.93 seconds after request: ,\\n\",\n      \"Message received 1.93 seconds after request:  \\n\",\n      \"Message received 1.93 seconds after request: 44\\n\",\n      \"Message received 1.93 seconds after request: ,\\n\",\n      \"Message received 1.93 seconds after request:  \\n\",\n      \"Message received 1.95 seconds after request: 45\\n\",\n      \"Message received 1.95 seconds after request: ,\\n\",\n      \"Message received 1.95 seconds after request:  \\n\",\n      \"Message received 2.00 seconds after request: 46\\n\",\n      \"Message received 2.00 seconds after request: ,\\n\",\n      \"Message received 2.00 seconds after request:  \\n\",\n      \"Message received 2.00 seconds after request: 47\\n\",\n      \"Message received 2.00 seconds after request: ,\\n\",\n      \"Message received 2.00 seconds after request:  \\n\",\n      \"Message received 2.00 seconds after request: 48\\n\",\n      \"Message received 2.00 seconds after request: ,\\n\",\n      \"Message received 2.00 seconds after request:  \\n\",\n      \"Message received 2.00 seconds after request: 49\\n\",\n      \"Message received 2.00 seconds after request: ,\\n\",\n      \"Message received 2.00 seconds after request:  \\n\",\n      \"Message received 2.00 seconds after request: 50\\n\",\n      \"Message received 2.00 seconds after request: ,\\n\",\n      \"Message received 2.00 seconds after request:  \\n\",\n      \"Message received 2.00 seconds after request: 51\\n\",\n      \"Message received 2.00 seconds after request: ,\\n\",\n      \"Message received 2.04 seconds after request:  \\n\",\n      \"Message received 2.04 seconds after request: 52\\n\",\n      \"Message received 2.04 seconds after request: ,\\n\",\n      \"Message received 2.04 seconds after request:  \\n\",\n      \"Message received 2.04 seconds after request: 53\\n\",\n      \"Message received 2.04 seconds after request: ,\\n\",\n      \"Message received 2.13 seconds after request:  \\n\",\n      \"Message received 2.13 seconds after request: 54\\n\",\n      \"Message received 2.14 seconds after request: ,\\n\",\n      \"Message received 2.14 seconds after request:  \\n\",\n      \"Message received 2.14 seconds after request: 55\\n\",\n      \"Message received 2.14 seconds after request: ,\\n\",\n      \"Message received 2.14 seconds after request:  \\n\",\n      \"Message received 2.14 seconds after request: 56\\n\",\n      \"Message received 2.14 seconds after request: ,\\n\",\n      \"Message received 2.14 seconds after request:  \\n\",\n      \"Message received 2.16 seconds after request: 57\\n\",\n      \"Message received 2.16 seconds after request: ,\\n\",\n      \"Message received 2.16 seconds after request:  \\n\",\n      \"Message received 2.17 seconds after request: 58\\n\",\n      \"Message received 2.17 seconds after request: ,\\n\",\n      \"Message received 2.17 seconds after request:  \\n\",\n      \"Message received 2.19 seconds after request: 59\\n\",\n      \"Message received 2.19 seconds after request: ,\\n\",\n      \"Message received 2.19 seconds after request:  \\n\",\n      \"Message received 2.21 seconds after request: 60\\n\",\n      \"Message received 2.21 seconds after request: ,\\n\",\n      \"Message received 2.21 seconds after request:  \\n\",\n      \"Message received 2.34 seconds after request: 61\\n\",\n      \"Message received 2.34 seconds after request: ,\\n\",\n      \"Message received 2.34 seconds after request:  \\n\",\n      \"Message received 2.34 seconds after request: 62\\n\",\n      \"Message received 2.34 seconds after request: ,\\n\",\n      \"Message received 2.34 seconds after request:  \\n\",\n      \"Message received 2.34 seconds after request: 63\\n\",\n      \"Message received 2.34 seconds after request: ,\\n\",\n      \"Message received 2.34 seconds after request:  \\n\",\n      \"Message received 2.34 seconds after request: 64\\n\",\n      \"Message received 2.34 seconds after request: ,\\n\",\n      \"Message received 2.34 seconds after request:  \\n\",\n      \"Message received 2.34 seconds after request: 65\\n\",\n      \"Message received 2.34 seconds after request: ,\\n\",\n      \"Message received 2.34 seconds after request:  \\n\",\n      \"Message received 2.34 seconds after request: 66\\n\",\n      \"Message received 2.34 seconds after request: ,\\n\",\n      \"Message received 2.34 seconds after request:  \\n\",\n      \"Message received 2.34 seconds after request: 67\\n\",\n      \"Message received 2.34 seconds after request: ,\\n\",\n      \"Message received 2.34 seconds after request:  \\n\",\n      \"Message received 2.36 seconds after request: 68\\n\",\n      \"Message received 2.36 seconds after request: ,\\n\",\n      \"Message received 2.36 seconds after request:  \\n\",\n      \"Message received 2.36 seconds after request: 69\\n\",\n      \"Message received 2.36 seconds after request: ,\\n\",\n      \"Message received 2.36 seconds after request:  \\n\",\n      \"Message received 2.38 seconds after request: 70\\n\",\n      \"Message received 2.38 seconds after request: ,\\n\",\n      \"Message received 2.38 seconds after request:  \\n\",\n      \"Message received 2.39 seconds after request: 71\\n\",\n      \"Message received 2.39 seconds after request: ,\\n\",\n      \"Message received 2.39 seconds after request:  \\n\",\n      \"Message received 2.39 seconds after request: 72\\n\",\n      \"Message received 2.39 seconds after request: ,\\n\",\n      \"Message received 2.39 seconds after request:  \\n\",\n      \"Message received 2.39 seconds after request: 73\\n\",\n      \"Message received 2.39 seconds after request: ,\\n\",\n      \"Message received 2.39 seconds after request:  \\n\",\n      \"Message received 2.39 seconds after request: 74\\n\",\n      \"Message received 2.39 seconds after request: ,\\n\",\n      \"Message received 2.39 seconds after request:  \\n\",\n      \"Message received 2.39 seconds after request: 75\\n\",\n      \"Message received 2.39 seconds after request: ,\\n\",\n      \"Message received 2.40 seconds after request:  \\n\",\n      \"Message received 2.40 seconds after request: 76\\n\",\n      \"Message received 2.40 seconds after request: ,\\n\",\n      \"Message received 2.42 seconds after request:  \\n\",\n      \"Message received 2.42 seconds after request: 77\\n\",\n      \"Message received 2.42 seconds after request: ,\\n\",\n      \"Message received 2.51 seconds after request:  \\n\",\n      \"Message received 2.51 seconds after request: 78\\n\",\n      \"Message received 2.51 seconds after request: ,\\n\",\n      \"Message received 2.52 seconds after request:  \\n\",\n      \"Message received 2.52 seconds after request: 79\\n\",\n      \"Message received 2.52 seconds after request: ,\\n\",\n      \"Message received 2.52 seconds after request:  \\n\",\n      \"Message received 2.52 seconds after request: 80\\n\",\n      \"Message received 2.52 seconds after request: ,\\n\",\n      \"Message received 2.52 seconds after request:  \\n\",\n      \"Message received 2.52 seconds after request: 81\\n\",\n      \"Message received 2.52 seconds after request: ,\\n\",\n      \"Message received 2.52 seconds after request:  \\n\",\n      \"Message received 2.52 seconds after request: 82\\n\",\n      \"Message received 2.52 seconds after request: ,\\n\",\n      \"Message received 2.60 seconds after request:  \\n\",\n      \"Message received 2.60 seconds after request: 83\\n\",\n      \"Message received 2.60 seconds after request: ,\\n\",\n      \"Message received 2.64 seconds after request:  \\n\",\n      \"Message received 2.64 seconds after request: 84\\n\",\n      \"Message received 2.64 seconds after request: ,\\n\",\n      \"Message received 2.64 seconds after request:  \\n\",\n      \"Message received 2.64 seconds after request: 85\\n\",\n      \"Message received 2.64 seconds after request: ,\\n\",\n      \"Message received 2.64 seconds after request:  \\n\",\n      \"Message received 2.66 seconds after request: 86\\n\",\n      \"Message received 2.66 seconds after request: ,\\n\",\n      \"Message received 2.66 seconds after request:  \\n\",\n      \"Message received 2.66 seconds after request: 87\\n\",\n      \"Message received 2.66 seconds after request: ,\\n\",\n      \"Message received 2.66 seconds after request:  \\n\",\n      \"Message received 2.68 seconds after request: 88\\n\",\n      \"Message received 2.68 seconds after request: ,\\n\",\n      \"Message received 2.68 seconds after request:  \\n\",\n      \"Message received 2.69 seconds after request: 89\\n\",\n      \"Message received 2.69 seconds after request: ,\\n\",\n      \"Message received 2.69 seconds after request:  \\n\",\n      \"Message received 2.72 seconds after request: 90\\n\",\n      \"Message received 2.72 seconds after request: ,\\n\",\n      \"Message received 2.72 seconds after request:  \\n\",\n      \"Message received 2.82 seconds after request: 91\\n\",\n      \"Message received 2.82 seconds after request: ,\\n\",\n      \"Message received 2.82 seconds after request:  \\n\",\n      \"Message received 2.82 seconds after request: 92\\n\",\n      \"Message received 2.82 seconds after request: ,\\n\",\n      \"Message received 2.82 seconds after request:  \\n\",\n      \"Message received 2.82 seconds after request: 93\\n\",\n      \"Message received 2.82 seconds after request: ,\\n\",\n      \"Message received 2.82 seconds after request:  \\n\",\n      \"Message received 2.82 seconds after request: 94\\n\",\n      \"Message received 2.82 seconds after request: ,\\n\",\n      \"Message received 2.82 seconds after request:  \\n\",\n      \"Message received 2.82 seconds after request: 95\\n\",\n      \"Message received 2.82 seconds after request: ,\\n\",\n      \"Message received 2.82 seconds after request:  \\n\",\n      \"Message received 2.82 seconds after request: 96\\n\",\n      \"Message received 2.82 seconds after request: ,\\n\",\n      \"Message received 2.82 seconds after request:  \\n\",\n      \"Message received 2.82 seconds after request: 97\\n\",\n      \"Message received 2.82 seconds after request: ,\\n\",\n      \"Message received 2.82 seconds after request:  \\n\",\n      \"Message received 2.82 seconds after request: 98\\n\",\n      \"Message received 2.82 seconds after request: ,\\n\",\n      \"Message received 2.82 seconds after request:  \\n\",\n      \"Message received 2.82 seconds after request: 99\\n\",\n      \"Message received 2.82 seconds after request: ,\\n\",\n      \"Message received 2.82 seconds after request:  \\n\",\n      \"Message received 2.82 seconds after request: 100\\n\",\n      \"Message received 2.82 seconds after request: None\\n\",\n      \"Full response received 2.82 seconds after request\\n\",\n      \"Full conversation received: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"# Example of an OpenAI ChatCompletion request with stream=True\\n\",\n    \"# https://platform.openai.com/docs/api-reference/streaming#chat/create-stream\\n\",\n    \"\\n\",\n    \"# record the time before the request is sent\\n\",\n    \"start_time = time.time()\\n\",\n    \"\\n\",\n    \"# send a ChatCompletion request to count to 100\\n\",\n    \"response = client.chat.completions.create(\\n\",\n    \"    model='gpt-4o-mini',\\n\",\n    \"    messages=[\\n\",\n    \"        {'role': 'user', 'content': 'Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}\\n\",\n    \"    ],\\n\",\n    \"    temperature=0,\\n\",\n    \"    stream=True  # again, we set stream=True\\n\",\n    \")\\n\",\n    \"# create variables to collect the stream of chunks\\n\",\n    \"collected_chunks = []\\n\",\n    \"collected_messages = []\\n\",\n    \"# iterate through the stream of events\\n\",\n    \"for chunk in response:\\n\",\n    \"    chunk_time = time.time() - start_time  # calculate the time delay of the chunk\\n\",\n    \"    collected_chunks.append(chunk)  # save the event response\\n\",\n    \"    chunk_message = chunk.choices[0].delta.content  # extract the message\\n\",\n    \"    collected_messages.append(chunk_message)  # save the message\\n\",\n    \"    print(f\\\"Message received {chunk_time:.2f} seconds after request: {chunk_message}\\\")  # print the delay and text\\n\",\n    \"\\n\",\n    \"# print the time delay and text received\\n\",\n    \"print(f\\\"Full response received {chunk_time:.2f} seconds after request\\\")\\n\",\n    \"# clean None in collected_messages\\n\",\n    \"collected_messages = [m for m in collected_messages if m is not None]\\n\",\n    \"full_reply_content = ''.join(collected_messages)\\n\",\n    \"print(f\\\"Full conversation received: {full_reply_content}\\\")\\n\"\n   ]\n  },\n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Time comparison\\n\",\n    \"\\n\",\n    \"In the example above, both requests took about 4 to 5 seconds to fully complete. Request times will vary depending on load and other stochastic factors.\\n\",\n    \"\\n\",\n    \"However, with the streaming request, we received the first token after 0.1 seconds, and subsequent tokens every ~0.01-0.02 seconds.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### 4. How to get token usage data for streamed chat completion response\\n\",\n    \"\\n\",\n    \"You can get token usage statistics for your streamed response by setting `stream_options={\\\"include_usage\\\": True}`. When you do so, an extra chunk will be streamed as the final chunk. You can access the usage data for the entire request via the `usage` field on this chunk. A few important notes when you set `stream_options={\\\"include_usage\\\": True}`:\\n\",\n    \"* The value for the `usage` field on all chunks except for the last one will be null.\\n\",\n    \"* The `usage` field on the last chunk contains token usage statistics for the entire request.\\n\",\n    \"* The `choices` field on the last chunk will always be an empty array `[]`.\\n\",\n    \"\\n\",\n    \"Let's see how it works using the example in 2.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 7,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"choices: [Choice(delta=ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)]\\n\",\n      \"usage: None\\n\",\n      \"****************\\n\",\n      \"choices: [Choice(delta=ChoiceDelta(content='Two', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)]\\n\",\n      \"usage: None\\n\",\n      \"****************\\n\",\n      \"choices: [Choice(delta=ChoiceDelta(content='.', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)]\\n\",\n      \"usage: None\\n\",\n      \"****************\\n\",\n      \"choices: [Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None), finish_reason='stop', index=0, logprobs=None)]\\n\",\n      \"usage: None\\n\",\n      \"****************\\n\",\n      \"choices: []\\n\",\n      \"usage: CompletionUsage(completion_tokens=2, prompt_tokens=18, total_tokens=20)\\n\",\n      \"****************\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"# Example of an OpenAI ChatCompletion request with stream=True and stream_options={\\\"include_usage\\\": True}\\n\",\n    \"\\n\",\n    \"# a ChatCompletion request\\n\",\n    \"response = client.chat.completions.create(\\n\",\n    \"    model='gpt-4o-mini',\\n\",\n    \"    messages=[\\n\",\n    \"        {'role': 'user', 'content': \\\"What's 1+1? Answer in one word.\\\"}\\n\",\n    \"    ],\\n\",\n    \"    temperature=0,\\n\",\n    \"    stream=True,\\n\",\n    \"    stream_options={\\\"include_usage\\\": True}, # retrieving token usage for stream response\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"for chunk in response:\\n\",\n    \"    print(f\\\"choices: {chunk.choices}\\\\nusage: {chunk.usage}\\\")\\n\",\n    \"    print(\\\"****************\\\")\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3.9.9 ('openai')\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.11.8\"\n  },\n  \"orig_nbformat\": 4,\n  \"vscode\": {\n   \"interpreter\": {\n    \"hash\": \"365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97\"\n   }\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 2\n}\n",
      "tags": [
        "tokens",
        "prompts",
        "streaming"
      ],
      "category": "prompt-engineering"
    },
    {
      "slug": "anthropic-prompt-caching",
      "title": "Prompt Caching",
      "description": "Claude API Documentation",
      "source": "anthropic",
      "sourceLabel": "Anthropic",
      "sourceColor": "#d97706",
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching",
      "content": "Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...[](/docs)[](https://x.com/claudeai)[](https://www.linkedin.com/showcase/claude)[](https://instagram.com/claudeai)### Solutions\n\n- [AI agents](https://claude.com/solutions/agents)\n- [Code modernization](https://claude.com/solutions/code-modernization)\n- [Coding](https://claude.com/solutions/coding)\n- [Customer support](https://claude.com/solutions/customer-support)\n- [Education](https://claude.com/solutions/education)\n- [Financial services](https://claude.com/solutions/financial-services)\n- [Government](https://claude.com/solutions/government)\n- [Life sciences](https://claude.com/solutions/life-sciences)\n### Partners\n\n- [Amazon Bedrock](https://claude.com/partners/amazon-bedrock)\n- [Google Cloud&#x27;s Vertex AI](https://claude.com/partners/google-cloud-vertex-ai)\n### Learn\n\n- [Blog](https://claude.com/blog)\n- [Catalog](https://claude.ai/catalog/artifacts)\n- [Courses](https://www.anthropic.com/learn)\n- [Use cases](https://claude.com/resources/use-cases)\n- [Connectors](https://claude.com/partners/mcp)\n- [Customer stories](https://claude.com/customers)\n- [Engineering at Anthropic](https://www.anthropic.com/engineering)\n- [Events](https://www.anthropic.com/events)\n- [Powered by Claude](https://claude.com/partners/powered-by-claude)\n- [Service partners](https://claude.com/partners/services)\n- [Startups program](https://claude.com/programs/startups)\n### Company\n\n- [Anthropic](https://www.anthropic.com/company)\n- [Careers](https://www.anthropic.com/careers)\n- [Economic Futures](https://www.anthropic.com/economic-futures)\n- [Research](https://www.anthropic.com/research)\n- [News](https://www.anthropic.com/news)\n- [Responsible Scaling Policy](https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy)\n- [Security and compliance](https://trust.anthropic.com)\n- [Transparency](https://www.anthropic.com/transparency)\n### Learn\n\n- [Blog](https://claude.com/blog)\n- [Catalog](https://claude.ai/catalog/artifacts)\n- [Courses](https://www.anthropic.com/learn)\n- [Use cases](https://claude.com/resources/use-cases)\n- [Connectors](https://claude.com/partners/mcp)\n- [Customer stories](https://claude.com/customers)\n- [Engineering at Anthropic](https://www.anthropic.com/engineering)\n- [Events](https://www.anthropic.com/events)\n- [Powered by Claude](https://claude.com/partners/powered-by-claude)\n- [Service partners](https://claude.com/partners/services)\n- [Startups program](https://claude.com/programs/startups)\n### Help and security\n\n- [Availability](https://www.anthropic.com/supported-countries)\n- [Status](https://status.claude.com/)\n- [Support](https://support.claude.com/)\n- [Discord](https://www.anthropic.com/discord)\n### Terms and policies\n\n- [Privacy policy](https://www.anthropic.com/legal/privacy)\n- [Responsible disclosure policy](https://www.anthropic.com/responsible-disclosure-policy)\n- [Terms of service: Commercial](https://www.anthropic.com/legal/commercial-terms)\n- [Terms of service: Consumer](https://www.anthropic.com/legal/consumer-terms)\n- [Usage policy](https://www.anthropic.com/legal/aup)",
      "tags": [
        "caching",
        "optimization",
        "tokens"
      ],
      "category": "caching"
    },
    {
      "slug": "anthropic-prompt-engineering-overview",
      "title": "Prompt Engineering Overview",
      "description": "Claude API Documentation",
      "source": "anthropic",
      "sourceLabel": "Anthropic",
      "sourceColor": "#d97706",
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview",
      "content": "Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...[](/docs)[](https://x.com/claudeai)[](https://www.linkedin.com/showcase/claude)[](https://instagram.com/claudeai)### Solutions\n\n- [AI agents](https://claude.com/solutions/agents)\n- [Code modernization](https://claude.com/solutions/code-modernization)\n- [Coding](https://claude.com/solutions/coding)\n- [Customer support](https://claude.com/solutions/customer-support)\n- [Education](https://claude.com/solutions/education)\n- [Financial services](https://claude.com/solutions/financial-services)\n- [Government](https://claude.com/solutions/government)\n- [Life sciences](https://claude.com/solutions/life-sciences)\n### Partners\n\n- [Amazon Bedrock](https://claude.com/partners/amazon-bedrock)\n- [Google Cloud&#x27;s Vertex AI](https://claude.com/partners/google-cloud-vertex-ai)\n### Learn\n\n- [Blog](https://claude.com/blog)\n- [Catalog](https://claude.ai/catalog/artifacts)\n- [Courses](https://www.anthropic.com/learn)\n- [Use cases](https://claude.com/resources/use-cases)\n- [Connectors](https://claude.com/partners/mcp)\n- [Customer stories](https://claude.com/customers)\n- [Engineering at Anthropic](https://www.anthropic.com/engineering)\n- [Events](https://www.anthropic.com/events)\n- [Powered by Claude](https://claude.com/partners/powered-by-claude)\n- [Service partners](https://claude.com/partners/services)\n- [Startups program](https://claude.com/programs/startups)\n### Company\n\n- [Anthropic](https://www.anthropic.com/company)\n- [Careers](https://www.anthropic.com/careers)\n- [Economic Futures](https://www.anthropic.com/economic-futures)\n- [Research](https://www.anthropic.com/research)\n- [News](https://www.anthropic.com/news)\n- [Responsible Scaling Policy](https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy)\n- [Security and compliance](https://trust.anthropic.com)\n- [Transparency](https://www.anthropic.com/transparency)\n### Learn\n\n- [Blog](https://claude.com/blog)\n- [Catalog](https://claude.ai/catalog/artifacts)\n- [Courses](https://www.anthropic.com/learn)\n- [Use cases](https://claude.com/resources/use-cases)\n- [Connectors](https://claude.com/partners/mcp)\n- [Customer stories](https://claude.com/customers)\n- [Engineering at Anthropic](https://www.anthropic.com/engineering)\n- [Events](https://www.anthropic.com/events)\n- [Powered by Claude](https://claude.com/partners/powered-by-claude)\n- [Service partners](https://claude.com/partners/services)\n- [Startups program](https://claude.com/programs/startups)\n### Help and security\n\n- [Availability](https://www.anthropic.com/supported-countries)\n- [Status](https://status.claude.com/)\n- [Support](https://support.claude.com/)\n- [Discord](https://www.anthropic.com/discord)\n### Terms and policies\n\n- [Privacy policy](https://www.anthropic.com/legal/privacy)\n- [Responsible disclosure policy](https://www.anthropic.com/responsible-disclosure-policy)\n- [Terms of service: Commercial](https://www.anthropic.com/legal/commercial-terms)\n- [Terms of service: Consumer](https://www.anthropic.com/legal/consumer-terms)\n- [Usage policy](https://www.anthropic.com/legal/aup)",
      "tags": [
        "prompts",
        "engineering",
        "best-practices"
      ],
      "category": "prompt-engineering"
    },
    {
      "slug": "anthropic-chain-of-thought-prompting",
      "title": "Chain of Thought Prompting",
      "description": "Claude API Documentation",
      "source": "anthropic",
      "sourceLabel": "Anthropic",
      "sourceColor": "#d97706",
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/chain-of-thought",
      "content": "Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...[](/docs)[](https://x.com/claudeai)[](https://www.linkedin.com/showcase/claude)[](https://instagram.com/claudeai)### Solutions\n\n- [AI agents](https://claude.com/solutions/agents)\n- [Code modernization](https://claude.com/solutions/code-modernization)\n- [Coding](https://claude.com/solutions/coding)\n- [Customer support](https://claude.com/solutions/customer-support)\n- [Education](https://claude.com/solutions/education)\n- [Financial services](https://claude.com/solutions/financial-services)\n- [Government](https://claude.com/solutions/government)\n- [Life sciences](https://claude.com/solutions/life-sciences)\n### Partners\n\n- [Amazon Bedrock](https://claude.com/partners/amazon-bedrock)\n- [Google Cloud&#x27;s Vertex AI](https://claude.com/partners/google-cloud-vertex-ai)\n### Learn\n\n- [Blog](https://claude.com/blog)\n- [Catalog](https://claude.ai/catalog/artifacts)\n- [Courses](https://www.anthropic.com/learn)\n- [Use cases](https://claude.com/resources/use-cases)\n- [Connectors](https://claude.com/partners/mcp)\n- [Customer stories](https://claude.com/customers)\n- [Engineering at Anthropic](https://www.anthropic.com/engineering)\n- [Events](https://www.anthropic.com/events)\n- [Powered by Claude](https://claude.com/partners/powered-by-claude)\n- [Service partners](https://claude.com/partners/services)\n- [Startups program](https://claude.com/programs/startups)\n### Company\n\n- [Anthropic](https://www.anthropic.com/company)\n- [Careers](https://www.anthropic.com/careers)\n- [Economic Futures](https://www.anthropic.com/economic-futures)\n- [Research](https://www.anthropic.com/research)\n- [News](https://www.anthropic.com/news)\n- [Responsible Scaling Policy](https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy)\n- [Security and compliance](https://trust.anthropic.com)\n- [Transparency](https://www.anthropic.com/transparency)\n### Learn\n\n- [Blog](https://claude.com/blog)\n- [Catalog](https://claude.ai/catalog/artifacts)\n- [Courses](https://www.anthropic.com/learn)\n- [Use cases](https://claude.com/resources/use-cases)\n- [Connectors](https://claude.com/partners/mcp)\n- [Customer stories](https://claude.com/customers)\n- [Engineering at Anthropic](https://www.anthropic.com/engineering)\n- [Events](https://www.anthropic.com/events)\n- [Powered by Claude](https://claude.com/partners/powered-by-claude)\n- [Service partners](https://claude.com/partners/services)\n- [Startups program](https://claude.com/programs/startups)\n### Help and security\n\n- [Availability](https://www.anthropic.com/supported-countries)\n- [Status](https://status.claude.com/)\n- [Support](https://support.claude.com/)\n- [Discord](https://www.anthropic.com/discord)\n### Terms and policies\n\n- [Privacy policy](https://www.anthropic.com/legal/privacy)\n- [Responsible disclosure policy](https://www.anthropic.com/responsible-disclosure-policy)\n- [Terms of service: Commercial](https://www.anthropic.com/legal/commercial-terms)\n- [Terms of service: Consumer](https://www.anthropic.com/legal/consumer-terms)\n- [Usage policy](https://www.anthropic.com/legal/aup)",
      "tags": [
        "prompts",
        "reasoning",
        "chain-of-thought"
      ],
      "category": "prompt-engineering"
    },
    {
      "slug": "anthropic-context-windows",
      "title": "Context Windows",
      "description": "Claude API Documentation",
      "source": "anthropic",
      "sourceLabel": "Anthropic",
      "sourceColor": "#d97706",
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/context-windows",
      "content": "Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...[](/docs)[](https://x.com/claudeai)[](https://www.linkedin.com/showcase/claude)[](https://instagram.com/claudeai)### Solutions\n\n- [AI agents](https://claude.com/solutions/agents)\n- [Code modernization](https://claude.com/solutions/code-modernization)\n- [Coding](https://claude.com/solutions/coding)\n- [Customer support](https://claude.com/solutions/customer-support)\n- [Education](https://claude.com/solutions/education)\n- [Financial services](https://claude.com/solutions/financial-services)\n- [Government](https://claude.com/solutions/government)\n- [Life sciences](https://claude.com/solutions/life-sciences)\n### Partners\n\n- [Amazon Bedrock](https://claude.com/partners/amazon-bedrock)\n- [Google Cloud&#x27;s Vertex AI](https://claude.com/partners/google-cloud-vertex-ai)\n### Learn\n\n- [Blog](https://claude.com/blog)\n- [Catalog](https://claude.ai/catalog/artifacts)\n- [Courses](https://www.anthropic.com/learn)\n- [Use cases](https://claude.com/resources/use-cases)\n- [Connectors](https://claude.com/partners/mcp)\n- [Customer stories](https://claude.com/customers)\n- [Engineering at Anthropic](https://www.anthropic.com/engineering)\n- [Events](https://www.anthropic.com/events)\n- [Powered by Claude](https://claude.com/partners/powered-by-claude)\n- [Service partners](https://claude.com/partners/services)\n- [Startups program](https://claude.com/programs/startups)\n### Company\n\n- [Anthropic](https://www.anthropic.com/company)\n- [Careers](https://www.anthropic.com/careers)\n- [Economic Futures](https://www.anthropic.com/economic-futures)\n- [Research](https://www.anthropic.com/research)\n- [News](https://www.anthropic.com/news)\n- [Responsible Scaling Policy](https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy)\n- [Security and compliance](https://trust.anthropic.com)\n- [Transparency](https://www.anthropic.com/transparency)\n### Learn\n\n- [Blog](https://claude.com/blog)\n- [Catalog](https://claude.ai/catalog/artifacts)\n- [Courses](https://www.anthropic.com/learn)\n- [Use cases](https://claude.com/resources/use-cases)\n- [Connectors](https://claude.com/partners/mcp)\n- [Customer stories](https://claude.com/customers)\n- [Engineering at Anthropic](https://www.anthropic.com/engineering)\n- [Events](https://www.anthropic.com/events)\n- [Powered by Claude](https://claude.com/partners/powered-by-claude)\n- [Service partners](https://claude.com/partners/services)\n- [Startups program](https://claude.com/programs/startups)\n### Help and security\n\n- [Availability](https://www.anthropic.com/supported-countries)\n- [Status](https://status.claude.com/)\n- [Support](https://support.claude.com/)\n- [Discord](https://www.anthropic.com/discord)\n### Terms and policies\n\n- [Privacy policy](https://www.anthropic.com/legal/privacy)\n- [Responsible disclosure policy](https://www.anthropic.com/responsible-disclosure-policy)\n- [Terms of service: Commercial](https://www.anthropic.com/legal/commercial-terms)\n- [Terms of service: Consumer](https://www.anthropic.com/legal/consumer-terms)\n- [Usage policy](https://www.anthropic.com/legal/aup)",
      "tags": [
        "context",
        "windows",
        "tokens"
      ],
      "category": "context-management"
    },
    {
      "slug": "anthropic-long-context-window-tips",
      "title": "Long Context Window Tips",
      "description": "Claude API Documentation",
      "source": "anthropic",
      "sourceLabel": "Anthropic",
      "sourceColor": "#d97706",
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/long-context-tips",
      "content": "Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...[](/docs)[](https://x.com/claudeai)[](https://www.linkedin.com/showcase/claude)[](https://instagram.com/claudeai)### Solutions\n\n- [AI agents](https://claude.com/solutions/agents)\n- [Code modernization](https://claude.com/solutions/code-modernization)\n- [Coding](https://claude.com/solutions/coding)\n- [Customer support](https://claude.com/solutions/customer-support)\n- [Education](https://claude.com/solutions/education)\n- [Financial services](https://claude.com/solutions/financial-services)\n- [Government](https://claude.com/solutions/government)\n- [Life sciences](https://claude.com/solutions/life-sciences)\n### Partners\n\n- [Amazon Bedrock](https://claude.com/partners/amazon-bedrock)\n- [Google Cloud&#x27;s Vertex AI](https://claude.com/partners/google-cloud-vertex-ai)\n### Learn\n\n- [Blog](https://claude.com/blog)\n- [Catalog](https://claude.ai/catalog/artifacts)\n- [Courses](https://www.anthropic.com/learn)\n- [Use cases](https://claude.com/resources/use-cases)\n- [Connectors](https://claude.com/partners/mcp)\n- [Customer stories](https://claude.com/customers)\n- [Engineering at Anthropic](https://www.anthropic.com/engineering)\n- [Events](https://www.anthropic.com/events)\n- [Powered by Claude](https://claude.com/partners/powered-by-claude)\n- [Service partners](https://claude.com/partners/services)\n- [Startups program](https://claude.com/programs/startups)\n### Company\n\n- [Anthropic](https://www.anthropic.com/company)\n- [Careers](https://www.anthropic.com/careers)\n- [Economic Futures](https://www.anthropic.com/economic-futures)\n- [Research](https://www.anthropic.com/research)\n- [News](https://www.anthropic.com/news)\n- [Responsible Scaling Policy](https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy)\n- [Security and compliance](https://trust.anthropic.com)\n- [Transparency](https://www.anthropic.com/transparency)\n### Learn\n\n- [Blog](https://claude.com/blog)\n- [Catalog](https://claude.ai/catalog/artifacts)\n- [Courses](https://www.anthropic.com/learn)\n- [Use cases](https://claude.com/resources/use-cases)\n- [Connectors](https://claude.com/partners/mcp)\n- [Customer stories](https://claude.com/customers)\n- [Engineering at Anthropic](https://www.anthropic.com/engineering)\n- [Events](https://www.anthropic.com/events)\n- [Powered by Claude](https://claude.com/partners/powered-by-claude)\n- [Service partners](https://claude.com/partners/services)\n- [Startups program](https://claude.com/programs/startups)\n### Help and security\n\n- [Availability](https://www.anthropic.com/supported-countries)\n- [Status](https://status.claude.com/)\n- [Support](https://support.claude.com/)\n- [Discord](https://www.anthropic.com/discord)\n### Terms and policies\n\n- [Privacy policy](https://www.anthropic.com/legal/privacy)\n- [Responsible disclosure policy](https://www.anthropic.com/responsible-disclosure-policy)\n- [Terms of service: Commercial](https://www.anthropic.com/legal/commercial-terms)\n- [Terms of service: Consumer](https://www.anthropic.com/legal/consumer-terms)\n- [Usage policy](https://www.anthropic.com/legal/aup)",
      "tags": [
        "context",
        "long-context",
        "optimization"
      ],
      "category": "context-management"
    },
    {
      "slug": "anthropic-token-counting",
      "title": "Token Counting",
      "description": "Claude API Documentation",
      "source": "anthropic",
      "sourceLabel": "Anthropic",
      "sourceColor": "#d97706",
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/token-counting",
      "content": "Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...[](/docs)[](https://x.com/claudeai)[](https://www.linkedin.com/showcase/claude)[](https://instagram.com/claudeai)### Solutions\n\n- [AI agents](https://claude.com/solutions/agents)\n- [Code modernization](https://claude.com/solutions/code-modernization)\n- [Coding](https://claude.com/solutions/coding)\n- [Customer support](https://claude.com/solutions/customer-support)\n- [Education](https://claude.com/solutions/education)\n- [Financial services](https://claude.com/solutions/financial-services)\n- [Government](https://claude.com/solutions/government)\n- [Life sciences](https://claude.com/solutions/life-sciences)\n### Partners\n\n- [Amazon Bedrock](https://claude.com/partners/amazon-bedrock)\n- [Google Cloud&#x27;s Vertex AI](https://claude.com/partners/google-cloud-vertex-ai)\n### Learn\n\n- [Blog](https://claude.com/blog)\n- [Catalog](https://claude.ai/catalog/artifacts)\n- [Courses](https://www.anthropic.com/learn)\n- [Use cases](https://claude.com/resources/use-cases)\n- [Connectors](https://claude.com/partners/mcp)\n- [Customer stories](https://claude.com/customers)\n- [Engineering at Anthropic](https://www.anthropic.com/engineering)\n- [Events](https://www.anthropic.com/events)\n- [Powered by Claude](https://claude.com/partners/powered-by-claude)\n- [Service partners](https://claude.com/partners/services)\n- [Startups program](https://claude.com/programs/startups)\n### Company\n\n- [Anthropic](https://www.anthropic.com/company)\n- [Careers](https://www.anthropic.com/careers)\n- [Economic Futures](https://www.anthropic.com/economic-futures)\n- [Research](https://www.anthropic.com/research)\n- [News](https://www.anthropic.com/news)\n- [Responsible Scaling Policy](https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy)\n- [Security and compliance](https://trust.anthropic.com)\n- [Transparency](https://www.anthropic.com/transparency)\n### Learn\n\n- [Blog](https://claude.com/blog)\n- [Catalog](https://claude.ai/catalog/artifacts)\n- [Courses](https://www.anthropic.com/learn)\n- [Use cases](https://claude.com/resources/use-cases)\n- [Connectors](https://claude.com/partners/mcp)\n- [Customer stories](https://claude.com/customers)\n- [Engineering at Anthropic](https://www.anthropic.com/engineering)\n- [Events](https://www.anthropic.com/events)\n- [Powered by Claude](https://claude.com/partners/powered-by-claude)\n- [Service partners](https://claude.com/partners/services)\n- [Startups program](https://claude.com/programs/startups)\n### Help and security\n\n- [Availability](https://www.anthropic.com/supported-countries)\n- [Status](https://status.claude.com/)\n- [Support](https://support.claude.com/)\n- [Discord](https://www.anthropic.com/discord)\n### Terms and policies\n\n- [Privacy policy](https://www.anthropic.com/legal/privacy)\n- [Responsible disclosure policy](https://www.anthropic.com/responsible-disclosure-policy)\n- [Terms of service: Commercial](https://www.anthropic.com/legal/commercial-terms)\n- [Terms of service: Consumer](https://www.anthropic.com/legal/consumer-terms)\n- [Usage policy](https://www.anthropic.com/legal/aup)",
      "tags": [
        "tokens",
        "counting",
        "usage"
      ],
      "category": "token-optimization"
    },
    {
      "slug": "anthropic-use-xml-tags-in-prompts",
      "title": "Use XML Tags in Prompts",
      "description": "Claude API Documentation",
      "source": "anthropic",
      "sourceLabel": "Anthropic",
      "sourceColor": "#d97706",
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/use-xml-tags",
      "content": "Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...[](/docs)[](https://x.com/claudeai)[](https://www.linkedin.com/showcase/claude)[](https://instagram.com/claudeai)### Solutions\n\n- [AI agents](https://claude.com/solutions/agents)\n- [Code modernization](https://claude.com/solutions/code-modernization)\n- [Coding](https://claude.com/solutions/coding)\n- [Customer support](https://claude.com/solutions/customer-support)\n- [Education](https://claude.com/solutions/education)\n- [Financial services](https://claude.com/solutions/financial-services)\n- [Government](https://claude.com/solutions/government)\n- [Life sciences](https://claude.com/solutions/life-sciences)\n### Partners\n\n- [Amazon Bedrock](https://claude.com/partners/amazon-bedrock)\n- [Google Cloud&#x27;s Vertex AI](https://claude.com/partners/google-cloud-vertex-ai)\n### Learn\n\n- [Blog](https://claude.com/blog)\n- [Catalog](https://claude.ai/catalog/artifacts)\n- [Courses](https://www.anthropic.com/learn)\n- [Use cases](https://claude.com/resources/use-cases)\n- [Connectors](https://claude.com/partners/mcp)\n- [Customer stories](https://claude.com/customers)\n- [Engineering at Anthropic](https://www.anthropic.com/engineering)\n- [Events](https://www.anthropic.com/events)\n- [Powered by Claude](https://claude.com/partners/powered-by-claude)\n- [Service partners](https://claude.com/partners/services)\n- [Startups program](https://claude.com/programs/startups)\n### Company\n\n- [Anthropic](https://www.anthropic.com/company)\n- [Careers](https://www.anthropic.com/careers)\n- [Economic Futures](https://www.anthropic.com/economic-futures)\n- [Research](https://www.anthropic.com/research)\n- [News](https://www.anthropic.com/news)\n- [Responsible Scaling Policy](https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy)\n- [Security and compliance](https://trust.anthropic.com)\n- [Transparency](https://www.anthropic.com/transparency)\n### Learn\n\n- [Blog](https://claude.com/blog)\n- [Catalog](https://claude.ai/catalog/artifacts)\n- [Courses](https://www.anthropic.com/learn)\n- [Use cases](https://claude.com/resources/use-cases)\n- [Connectors](https://claude.com/partners/mcp)\n- [Customer stories](https://claude.com/customers)\n- [Engineering at Anthropic](https://www.anthropic.com/engineering)\n- [Events](https://www.anthropic.com/events)\n- [Powered by Claude](https://claude.com/partners/powered-by-claude)\n- [Service partners](https://claude.com/partners/services)\n- [Startups program](https://claude.com/programs/startups)\n### Help and security\n\n- [Availability](https://www.anthropic.com/supported-countries)\n- [Status](https://status.claude.com/)\n- [Support](https://support.claude.com/)\n- [Discord](https://www.anthropic.com/discord)\n### Terms and policies\n\n- [Privacy policy](https://www.anthropic.com/legal/privacy)\n- [Responsible disclosure policy](https://www.anthropic.com/responsible-disclosure-policy)\n- [Terms of service: Commercial](https://www.anthropic.com/legal/commercial-terms)\n- [Terms of service: Consumer](https://www.anthropic.com/legal/consumer-terms)\n- [Usage policy](https://www.anthropic.com/legal/aup)",
      "tags": [
        "prompts",
        "xml",
        "structure"
      ],
      "category": "prompt-engineering"
    },
    {
      "slug": "anthropic-extended-thinking",
      "title": "Extended Thinking",
      "description": "Claude API Documentation",
      "source": "anthropic",
      "sourceLabel": "Anthropic",
      "sourceColor": "#d97706",
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking",
      "content": "Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...Loading...[](/docs)[](https://x.com/claudeai)[](https://www.linkedin.com/showcase/claude)[](https://instagram.com/claudeai)### Solutions\n\n- [AI agents](https://claude.com/solutions/agents)\n- [Code modernization](https://claude.com/solutions/code-modernization)\n- [Coding](https://claude.com/solutions/coding)\n- [Customer support](https://claude.com/solutions/customer-support)\n- [Education](https://claude.com/solutions/education)\n- [Financial services](https://claude.com/solutions/financial-services)\n- [Government](https://claude.com/solutions/government)\n- [Life sciences](https://claude.com/solutions/life-sciences)\n### Partners\n\n- [Amazon Bedrock](https://claude.com/partners/amazon-bedrock)\n- [Google Cloud&#x27;s Vertex AI](https://claude.com/partners/google-cloud-vertex-ai)\n### Learn\n\n- [Blog](https://claude.com/blog)\n- [Catalog](https://claude.ai/catalog/artifacts)\n- [Courses](https://www.anthropic.com/learn)\n- [Use cases](https://claude.com/resources/use-cases)\n- [Connectors](https://claude.com/partners/mcp)\n- [Customer stories](https://claude.com/customers)\n- [Engineering at Anthropic](https://www.anthropic.com/engineering)\n- [Events](https://www.anthropic.com/events)\n- [Powered by Claude](https://claude.com/partners/powered-by-claude)\n- [Service partners](https://claude.com/partners/services)\n- [Startups program](https://claude.com/programs/startups)\n### Company\n\n- [Anthropic](https://www.anthropic.com/company)\n- [Careers](https://www.anthropic.com/careers)\n- [Economic Futures](https://www.anthropic.com/economic-futures)\n- [Research](https://www.anthropic.com/research)\n- [News](https://www.anthropic.com/news)\n- [Responsible Scaling Policy](https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy)\n- [Security and compliance](https://trust.anthropic.com)\n- [Transparency](https://www.anthropic.com/transparency)\n### Learn\n\n- [Blog](https://claude.com/blog)\n- [Catalog](https://claude.ai/catalog/artifacts)\n- [Courses](https://www.anthropic.com/learn)\n- [Use cases](https://claude.com/resources/use-cases)\n- [Connectors](https://claude.com/partners/mcp)\n- [Customer stories](https://claude.com/customers)\n- [Engineering at Anthropic](https://www.anthropic.com/engineering)\n- [Events](https://www.anthropic.com/events)\n- [Powered by Claude](https://claude.com/partners/powered-by-claude)\n- [Service partners](https://claude.com/partners/services)\n- [Startups program](https://claude.com/programs/startups)\n### Help and security\n\n- [Availability](https://www.anthropic.com/supported-countries)\n- [Status](https://status.claude.com/)\n- [Support](https://support.claude.com/)\n- [Discord](https://www.anthropic.com/discord)\n### Terms and policies\n\n- [Privacy policy](https://www.anthropic.com/legal/privacy)\n- [Responsible disclosure policy](https://www.anthropic.com/responsible-disclosure-policy)\n- [Terms of service: Commercial](https://www.anthropic.com/legal/commercial-terms)\n- [Terms of service: Consumer](https://www.anthropic.com/legal/consumer-terms)\n- [Usage policy](https://www.anthropic.com/legal/aup)",
      "tags": [
        "reasoning",
        "thinking",
        "chain-of-thought"
      ],
      "category": "prompt-engineering"
    },
    {
      "slug": "google-context-caching",
      "title": "Context Caching",
      "description": "Learn how to use Context Caching in the Gemini API",
      "source": "google",
      "sourceLabel": "Google AI",
      "sourceColor": "#4285f4",
      "url": "https://ai.google.dev/gemini-api/docs/caching",
      "content": "Home\n        \n  \n  \n      \n    \n  \n  \n  \n    \n      \n      \n    \n    \n    \n      \n        \n  \n    \n          Gemini API\n        \n  \n  \n      \n    \n  \n  \n  \n    \n      \n      \n    \n    \n    \n      \n        \n  \n    \n          Docs\n        \n  \n  \n      \n    \n  \n  \n\n    \n      \n    \n    \n  \n    \n  \n  \n    \n\n  \n  \n    \n    Send feedback\n  \n  \n\n  \n    \n      Context caching\n    \n  \n      \n    \n  \n  \n\n  \n  \n  \n    \n  \n\n  \n    \n    \n\n  Python\n  JavaScript\n  Go\n  REST\n\nIn a typical AI workflow, you might pass the same input tokens over and over to\na model. The Gemini API offers two different caching mechanisms:\n\n- Implicit caching (automatically enabled on most Gemini models, no cost saving guarantee)\n\n- Explicit caching (can be manually enabled on most models, cost saving guarantee)\n\nExplicit caching is useful in cases where you want to guarantee cost savings,\nbut with some added developer work.\n\n## Implicit caching\n\nImplicit caching is enabled by default and available for most Gemini models. We automatically\npass on cost savings if your request hits caches. There is nothing you need to do\nin order to enable this. It is effective as of May 8th, 2025. The minimum input\ntoken count for context caching is listed in the following table for each model:\n\nModel\nMin token limit\n\nGemini 3 Flash Preview\n1024\n\nGemini 3 Pro Preview\n4096\n\nGemini 2.5 Flash\n1024\n\nGemini 2.5 Pro\n4096\n\nTo increase the chance of an implicit cache hit:\n\n- Try putting large and common contents at the beginning of your prompt\n\n- Try to send requests with similar prefix in a short amount of time\n\nYou can see the number of tokens which were cache hits in the response object's\n`usage_metadata` field.\n\n## Explicit caching\n\nUsing the Gemini API explicit caching feature, you can pass some content\nto the model once, cache the input tokens, and then refer to the cached tokens\nfor subsequent requests. At certain volumes, using cached tokens is lower cost\nthan passing in the same corpus of tokens repeatedly.\n\nWhen you cache a set of tokens, you can choose how long you want the cache to\nexist before the tokens are automatically deleted. This caching duration is\ncalled the *time to live* (TTL). If not set, the TTL defaults to 1 hour. The\ncost for caching depends on the input token size and how long you want the\ntokens to persist.\n\nThis section assumes that you've installed a Gemini SDK (or have curl installed)\nand that you've configured an API key, as shown in the\n[quickstart](/gemini-api/docs/quickstart).\n\n### Generate content using a cache\n\nThe following example shows how to generate content using a cached system\ninstruction and video file.\n\n### Videos\n\n```\n`import os\nimport pathlib\nimport requests\nimport time\n\nfrom google import genai\nfrom google.genai import types\n\nclient = genai.Client()\n\n# Download a test video file and save it locally\nurl = 'https://storage.googleapis.com/generativeai-downloads/data/SherlockJr._10min.mp4'\npath_to_video_file = pathlib.Path('SherlockJr._10min.mp4')\nif not path_to_video_file.exists():\n    path_to_video_file.write_bytes(requests.get(url).content)\n\n# Upload the video using the Files API\nvideo_file = client.files.upload(file=path_to_video_file)\n\n# Wait for the file to finish processing\nwhile video_file.state.name == 'PROCESSING':\n    time.sleep(2.5)\n    video_file = client.files.get(name=video_file.name)\n\nprint(f'Video processing complete: {video_file.uri}')\n\nmodel='models/gemini-3-flash-preview'\n\n# Create a cache with a 5 minute TTL (300 seconds)\ncache = client.caches.create(\n    model=model,\n    config=types.CreateCachedContentConfig(\n        display_name='sherlock jr movie', # used to identify the cache\n        system_instruction=(\n            'You are an expert video analyzer, and your job is to answer '\n            'the user\\'s query based on the video file you have access to.'\n        ),\n        contents=[video_file],\n        ttl=\"300s\",\n    )\n)\n\nresponse = client.models.generate_content(\n    model = model,\n    contents= (\n    'Introduce different characters in the movie by describing '\n    'their personality, looks, and names. Also list the timestamps '\n    'they were introduced for the first time.'),\n    config=types.GenerateContentConfig(cached_content=cache.name)\n)\n\nprint(response.usage_metadata)\n\nprint(response.text)\n`\n```\n\n### PDFs\n\n```\n`from google import genai\nfrom google.genai import types\nimport io\nimport httpx\n\nclient = genai.Client()\n\nlong_context_pdf_path = \"https://sma.nasa.gov/SignificantIncidents/assets/a11_missionreport.pdf\"\n\n# Retrieve and upload the PDF using the File API\ndoc_io = io.BytesIO(httpx.get(long_context_pdf_path).content)\n\ndocument = client.files.upload(\n  file=doc_io,\n  config=dict(mime_type='application/pdf')\n)\n\nmodel_name = \"gemini-3-flash-preview\"\nsystem_instruction = \"You are an expert analyzing transcripts.\"\n\n# Create a cached content object\ncache = client.caches.create(\n    model=model_name,\n    config=types.CreateCachedContentConfig(\n      system_instruction=system_instruction,\n      contents=[document],\n    )\n)\n\nprint(f'{cache=}')\n\nresponse = client.models.generate_content(\n  model=model_name,\n  contents=\"Please summarize this transcript\",\n  config=types.GenerateContentConfig(\n    cached_content=cache.name\n  ))\n\nprint(f'{response.usage_metadata=}')\n\nprint('\\n\\n', response.text)\n`\n```\n\n### List caches\n\nIt's not possible to retrieve or view cached content, but you can retrieve\ncache metadata (`name`, `model`, `display_name`, `usage_metadata`,\n`create_time`, `update_time`, and `expire_time`).\n\nTo list metadata for all uploaded caches, use `CachedContent.list()`:\n\n```\n`for cache in client.caches.list():\n  print(cache)\n`\n```\n\nTo fetch the metadata for one cache object, if you know its name, use `get`:\n\n```\n`client.caches.get(name=name)\n`\n```\n\n### Update a cache\n\nYou can set a new `ttl` or `expire_time` for a cache. Changing anything else\nabout the cache isn't supported.\n\nThe following example shows how to update the `ttl` of a cache using\n`client.caches.update()`.\n\n```\n`from google import genai\nfrom google.genai import types\n\nclient.caches.update(\n  name = cache.name,\n  config  = types.UpdateCachedContentConfig(\n      ttl='300s'\n  )\n)\n`\n```\n\nTo set the expiry time, it will accepts either a `datetime` object\nor an ISO-formatted datetime string (`dt.isoformat()`, like\n`2025-01-27T16:02:36.473528+00:00`). Your time must include a time zone\n(`datetime.utcnow()` doesn't attach a time zone,\n`datetime.now(datetime.timezone.utc)` does attach a time zone).\n\n```\n`from google import genai\nfrom google.genai import types\nimport datetime\n\n# You must use a time zone-aware time.\nin10min = datetime.datetime.now(datetime.timezone.utc) + datetime.timedelta(minutes=10)\n\nclient.caches.update(\n  name = cache.name,\n  config  = types.UpdateCachedContentConfig(\n      expire_time=in10min\n  )\n)\n`\n```\n\n### Delete a cache\n\nThe caching service provides a delete operation for manually removing content\nfrom the cache. The following example shows how to delete a cache:\n\n```\n`client.caches.delete(cache.name)\n`\n```\n\n### Explicit caching using the OpenAI library\n\nIf you're using an [OpenAI library](/gemini-api/docs/openai), you can enable\nexplicit caching using the `cached_content` property on\n[`extra_body`](/gemini-api/docs/openai#extra-body).\n\n## When to use explicit caching\n\nContext caching is particularly well suited to scenarios where a substantial\ninitial context is referenced repeatedly by shorter requests. Consider using\ncontext caching for use cases such as:\n\n- Chatbots with extensive [system instructions](/gemini-api/docs/system-instructions)\n\n- Repetitive analysis of lengthy video files\n\n- Recurring queries against large document sets\n\n- Frequent code repository analysis or bug fixing\n\n### How explicit caching reduces costs\n\nContext caching is a paid feature designed to reduce cost. Billing is based on\nthe following factors:\n\n**Cache token count:** The number of input tokens cached, billed at a\nreduced rate when included in subsequent prompts.\n**Storage duration:** The amount of time cached tokens are stored (TTL),\nbilled based on the TTL duration of cached token count. There are no minimum\nor maximum bounds on the TTL.\n**Other factors:** Other charges apply, such as for non-cached input tokens\nand output tokens.\n\nFor up-to-date pricing details, refer to the Gemini API pricing\npage. To learn how to count tokens, see the Token\nguide.\n\n### Additional considerations\n\nKeep the following considerations in mind when using context caching:\n\nThe *minimum* input token count for context caching varies by model. The *maximum*\nis the same as the maximum for the given model. (For more on counting tokens,\nsee the [Token guide](/gemini-api/docs/tokens)).\nThe model doesn't make any distinction between cached tokens and regular\ninput tokens. Cached content is a prefix to the prompt.\nThere are no special rate or usage limits on context caching; the standard\nrate limits for `GenerateContent` apply, and token limits include cached\ntokens.\nThe number of cached tokens is returned in the `usage_metadata` from the\ncreate, get, and list operations of the cache service, and also in\n`GenerateContent` when using the cache.\n\n  \n\n  \n\n  \n    \n    \n      \n    \n    \n  \n       \n         \n\n  \n  \n    \n    Send feedback\n  \n  \n\n       \n    \n    \n  \n\n  \n\n  Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\n  Last updated 2026-01-16 UTC.\n\n  \n\n  \n    \n    \n    \n      \n\n  \n  \n    Need to tell us more?\n  \n  \n\n    \n  \n  \n    \n      [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the",
      "tags": [
        "caching",
        "context",
        "optimization"
      ],
      "category": "caching"
    },
    {
      "slug": "google-long-context",
      "title": "Long Context",
      "description": "در مورد نحوه شروع ساخت با متن طولانی (پنجره متن ۱ میلیون) در Gemini اطلاعات کسب کنید.",
      "source": "google",
      "sourceLabel": "Google AI",
      "sourceColor": "#4285f4",
      "url": "https://ai.google.dev/gemini-api/docs/long-context",
      "content": "این صفحه به‌وسیله [‏Cloud Translation API‏](//cloud.google.com/translate/?hl=fa) ترجمه شده است.\n          \n        \n        \n      \n    \n  \n\n  \n    \n    \n    \n  \n  \n    \n    \n    \n      \n        \n  \n    \n          صفحه اصلی\n        \n  \n  \n      \n    \n  \n  \n  \n    \n      \n      \n    \n    \n    \n      \n        \n  \n    \n          Gemini API\n        \n  \n  \n      \n    \n  \n  \n  \n    \n      \n      \n    \n    \n    \n      \n        \n  \n    \n          اسناد\n        \n  \n  \n      \n    \n  \n  \n\n    \n      \n    \n    \n  \n    \n  \n  \n    \n\n  \n  \n    \n    ارسال بازخورد\n  \n  \n\n  \n    \n      زمینه طولانی\n    \n  \n      \n    \n  \n  \n\n  \n  \n  \n    \n  \n\n  \n    \n    \n\nبسیاری از مدل‌های Gemini دارای پنجره‌های زمینه بزرگ با 1 میلیون توکن یا بیشتر هستند. از لحاظ تاریخی، مدل‌های زبان بزرگ (LLM) به میزان قابل توجهی توسط مقدار متن (یا نشانه‌هایی) که می‌توانستند در یک زمان به مدل منتقل شوند، محدود می‌شدند. پنجره زمینه طولانی Gemini بسیاری از موارد استفاده جدید و پارادایم های توسعه دهنده را باز می کند.\n\n کدی که قبلاً برای مواردی مانند [تولید متن](https://ai.google.dev/gemini-api/docs/text-generation?hl=fa) یا [ورودی‌های چندوجهی](https://ai.google.dev/gemini-api/docs/vision?hl=fa) استفاده می‌کنید بدون هیچ تغییری با زمینه طولانی کار می‌کند.\n\n این سند یک نمای کلی از آنچه می توانید با استفاده از مدل هایی با پنجره های زمینه 1M و بیشتر به دست آورید به شما ارائه می دهد. این صفحه یک نمای کلی از یک پنجره زمینه ارائه می‌دهد و به بررسی این موضوع می‌پردازد که چگونه توسعه‌دهندگان باید در مورد زمینه طولانی، موارد مختلف استفاده از دنیای واقعی برای زمینه طولانی، و راه‌هایی برای بهینه‌سازی استفاده از زمینه طولانی فکر کنند.\n\n برای اندازه‌های پنجره زمینه مدل‌های خاص، صفحه [مدل‌ها](https://ai.google.dev/gemini-api/docs/models?hl=fa) را ببینید.\n\n##  پنجره زمینه چیست؟\n\n روش اصلی استفاده از مدل‌های Gemini، انتقال اطلاعات (زمینه) به مدل است که متعاقباً یک پاسخ ایجاد می‌کند. یک قیاس برای پنجره زمینه، حافظه کوتاه مدت است. اطلاعات محدودی وجود دارد که می‌توان در حافظه کوتاه‌مدت شخص ذخیره کرد و همین امر در مورد مدل‌های تولیدی نیز صادق است.\n\n می‌توانید در [راهنمای مدل‌های تولیدی](https://ai.google.dev/gemini-api/docs/prompting-strategies?hl=fa#under-the-hood) ما درباره نحوه عملکرد مدل‌ها در زیر کاپوت بیشتر بخوانید.\n\n##  شروع با زمینه طولانی\n\n نسخه های قبلی مدل های مولد تنها قادر به پردازش 8000 توکن در یک زمان بودند. مدل های جدیدتر با پذیرش 32000 یا حتی 128000 توکن این امر را بیشتر پیش بردند. Gemini اولین مدلی است که توانایی پذیرش 1 میلیون توکن را دارد.\n\n در عمل، 1 میلیون توکن به صورت زیر است:\n\n-  50000 خط کد (با 80 کاراکتر استاندارد در هر خط)\n-  تمام پیامک هایی که در 5 سال گذشته ارسال کرده اید\n-  8 رمان انگلیسی متوسط\n-  رونوشت بیش از 200 قسمت پادکست با طول متوسط\n پنجره‌های زمینه محدودتر رایج در بسیاری از مدل‌های دیگر اغلب به استراتژی‌هایی مانند حذف دلخواه پیام‌های قدیمی، خلاصه کردن محتوا، استفاده از RAG با پایگاه‌های داده برداری، یا فیلتر کردن دستورات برای ذخیره توکن‌ها نیاز دارند.\n\n در حالی که این تکنیک ها در سناریوهای خاص ارزشمند باقی می مانند، پنجره زمینه گسترده Gemini رویکرد مستقیم تری را دعوت می کند: ارائه تمام اطلاعات مرتبط از قبل. از آنجایی که مدل‌های Gemini با قابلیت‌های زمینه عظیم ساخته شده‌اند، یادگیری درون زمینه‌ای قدرتمند را نشان می‌دهند. برای مثال، تنها با استفاده از مواد آموزشی درون متنی (یک دستور زبان مرجع 500 صفحه، یک فرهنگ لغت و 400 جمله موازی)، جمینی [یاد گرفت که از انگلیسی به کالامنگ - یک زبان پاپوآیی با کمتر از 200 سخنران - با کیفیتی مشابه با یک زبان آموز انسانی با استفاده از مطالب مشابه، ترجمه کند](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf) . این تغییر پارادایم را نشان می دهد که توسط زمینه طولانی Gemini فعال شده است، و امکانات جدید را از طریق یادگیری درون زمینه ای قوی تقویت می کند.\n\n##  موارد استفاده طولانی مدت\n\n در حالی که مورد استفاده استاندارد برای اکثر مدل‌های تولیدی همچنان ورودی متن است، خانواده مدل Gemini الگوی جدیدی از موارد استفاده چندوجهی را فعال می‌کند. این مدل ها می توانند متن، ویدئو، صدا و تصاویر را به صورت بومی درک کنند. آنها با [Gemini API همراه هستند که انواع فایل های چندوجهی را برای راحتی می پذیرد](https://ai.google.dev/gemini-api/docs/prompting_with_media?hl=fa) .\n\n###  متن فرم بلند\n\n ثابت شده است که متن لایه‌ای از هوش است که پشتوانه بسیاری از شتاب پیرامون LLM است. همانطور که قبلا ذکر شد، بسیاری از محدودیت های عملی LLM به دلیل نداشتن یک پنجره زمینه به اندازه کافی بزرگ برای انجام وظایف خاص بود. این منجر به پذیرش سریع تولید افزوده بازیابی (RAG) و سایر تکنیک‌هایی شد که به صورت پویا اطلاعات متنی مرتبط را به مدل ارائه می‌دهند. اکنون، با پنجره‌های زمینه بزرگ‌تر و بزرگ‌تر، تکنیک‌های جدیدی در دسترس هستند که موارد استفاده جدید را باز می‌کنند.\n\n برخی از موارد استفاده در حال ظهور و استاندارد برای زمینه طولانی مبتنی بر متن عبارتند از:\n\n-  خلاصه کردن مجموعه های بزرگ متن گزینه‌های خلاصه‌سازی قبلی با مدل‌های زمینه کوچک‌تر به یک پنجره کشویی یا تکنیک دیگری برای حفظ وضعیت بخش‌های قبلی با ارسال نشانه‌های جدید به مدل نیاز دارند.\n-  پرسش و پاسخ از نظر تاریخی، این تنها با RAG امکان پذیر بود، زیرا مقدار محدودی از زمینه و یادآوری واقعی مدل ها کم بود\n-  گردش کار نمایندگی متن زیربنای این است که چگونه عوامل وضعیت آنچه انجام داده اند و آنچه باید انجام دهند را حفظ می کنند. نداشتن اطلاعات کافی در مورد جهان و هدف عامل، محدودیت در قابلیت اطمینان عوامل است\n [یادگیری درون متنی چند شات](https://arxiv.org/pdf/2404.11018) یکی از منحصر به فردترین قابلیت هایی است که توسط مدل های زمینه طولانی باز شده است. تحقیقات نشان داده است که استفاده از پارادایم نمونه رایج «تک شات» یا «چند شات»، که در آن مدل با یک یا چند نمونه از یک کار ارائه می‌شود، و مقیاس‌بندی آن تا صدها، هزاران یا حتی صدها هزار نمونه، می‌تواند به قابلیت‌های مدل جدید منجر شود. همچنین نشان داده شده است که این رویکرد چندین شات مشابه مدل هایی است که برای یک کار خاص تنظیم شده اند. برای موارد استفاده که عملکرد مدل جمینی هنوز برای عرضه کافی نیست، می‌توانید رویکرد چند شات را امتحان کنید. همانطور که ممکن است بعداً در بخش بهینه‌سازی متن طولانی بررسی کنید، ذخیره‌سازی متن باعث می‌شود که این نوع حجم کار توکن ورودی بالا از نظر اقتصادی امکان‌پذیرتر باشد و حتی در برخی موارد تاخیر کمتری داشته باشد.\n\n###  ویدیوی طولانی\n\n ابزار محتوای ویدیویی مدت هاست که به دلیل عدم دسترسی به خود رسانه محدود شده است. مرور محتوا سخت بود، رونوشت‌ها اغلب نمی‌توانستند جزئیات یک ویدیو را به تصویر بکشند، و اکثر ابزارها تصویر، متن و صدا را با هم پردازش نمی‌کنند. با Gemini، قابلیت‌های متن طولانی به توانایی استدلال و پاسخ به سؤالات در مورد ورودی‌های چندوجهی با عملکرد پایدار تبدیل می‌شود.\n\n برخی از موارد استفاده در حال ظهور و استاندارد برای زمینه طولانی ویدیو عبارتند از:\n\n-  پرسش و پاسخ تصویری\n-  حافظه ویدیویی، همانطور که با [پروژه Astra گوگل](https://deepmind.google/technologies/gemini/project-astra/?hl=fa) نشان داده شده است\n-  زیرنویس ویدیو\n-  سیستم‌های توصیه ویدیویی، با غنی‌سازی ابرداده‌های موجود با درک چندوجهی جدید\n-  سفارشی‌سازی ویدیو، با مشاهده مجموعه‌ای از داده‌ها و فراداده‌های ویدیویی مرتبط و سپس حذف بخش‌هایی از ویدیوها که به بیننده مربوط نیستند.\n-  تعدیل محتوای ویدیویی\n-  پردازش ویدئو در زمان واقعی\n هنگام کار با ویدیوها، مهم است که نحوه [پردازش ویدیوها به توکن‌ها را](https://ai.google.dev/gemini-api/docs/tokens?hl=fa#media-token) در نظر بگیرید که بر روی صورت‌حساب و محدودیت‌های استفاده تأثیر می‌گذارد. می‌توانید درباره درخواست با فایل‌های ویدیویی در [راهنمای درخواست](https://ai.google.dev/gemini-api/docs/prompting_with_media?lang=python&hl=fa#prompting-with-videos) اطلاعات بیشتری کسب کنید.\n\n###  صوت فرم بلند\n\n مدل‌های Gemini اولین مدل‌های زبان بزرگ چندوجهی بومی بودند که می‌توانستند صدا را بفهمند. از لحاظ تاریخی، گردش کار توسعه‌دهنده معمولی شامل رشته‌بندی چندین مدل خاص دامنه، مانند مدل گفتار به متن و مدل متن به متن، به منظور پردازش صدا است. این منجر به تأخیر اضافی مورد نیاز با انجام چندین درخواست رفت و برگشت و کاهش عملکرد می‌شود که معمولاً به معماری‌های جداشده از راه‌اندازی مدل چندگانه نسبت داده می‌شود.\n\n برخی از موارد استفاده در حال ظهور و استاندارد برای زمینه صوتی عبارتند از:\n\n-  رونویسی و ترجمه در زمان واقعی\n-  پرسش و پاسخ پادکست / ویدئویی\n-  رونویسی و جمع بندی جلسه\n-  دستیارهای صوتی\n می‌توانید درباره درخواست با فایل‌های صوتی در [راهنمای درخواست](https://ai.google.dev/gemini-api/docs/prompting_with_media?lang=python&hl=fa#prompting-with-videos) اطلاعات بیشتری کسب کنید.\n\n##  بهینه سازی زمینه طولانی\n\n بهینه سازی اولیه هنگام کار با زمینه طولانی و مدل های Gemini، استفاده از [کش زمینه](https://ai.google.dev/gemini-api/docs/caching?hl=fa) است. فراتر از عدم امکان قبلی پردازش تعداد زیادی توکن در یک درخواست، محدودیت اصلی دیگر هزینه بود. اگر یک برنامه «چت با داده‌های خود» دارید که در آن کاربر 10 فایل PDF، یک ویدیو و برخی اسناد کاری را آپلود می‌کند، از لحاظ تاریخی باید با ابزار / چارچوب بازیابی پیچیده‌تر (RAG) کار کنید تا این درخواست‌ها را پردازش کنید و مبلغ قابل‌توجهی برای توکن‌های منتقل شده به پنجره زمینه بپردازید. اکنون می‌توانید فایل‌هایی را که کاربر آپلود می‌کند، کش کنید و برای ذخیره آن‌ها به صورت ساعتی هزینه پرداخت کنید. هزینه ورودی/خروجی به ازای هر درخواست برای مثال با Gemini Flash 4 برابر کمتر از هزینه ورودی/خروجی استاندارد است، بنابراین اگر کاربر به اندازه کافی با داده های خود چت کند، صرفه جویی زیادی در هزینه برای شما به عنوان توسعه دهنده خواهد شد.\n\n##  محدودیت های زمینه طولانی\n\n در بخش‌های مختلف این راهنما، ما در مورد چگونگی دستیابی مدل‌های Gemini به عملکرد بالا در ارزیابی‌های مختلف بازیابی سوزن در انبار کاه صحبت کردیم. این تست‌ها ابتدایی‌ترین راه‌اندازی را در نظر می‌گیرند، جایی که شما یک سوزن تنها دارید که به دنبال آن هستید. در مواردی که ممکن است چندین \"سوزن\" یا اطلاعات خاصی داشته باشید که به دنبال آن هستید، مدل با دقت یکسانی کار نمی کند. عملکرد می تواند تا حد زیادی بسته به زمینه متفاوت باشد. این مهم است که در نظر گرفته شود زیرا یک مبادله ذاتی بین بازیابی اطلاعات صحیح و هزینه وجود دارد. شما می توانید 99٪ در یک پرس و جو دریافت کنید، اما هر بار که آن درخواست را ارسال می کنید باید هزینه رمز ورودی را بپردازید. بنابراین برای بازیابی 100 قطعه اطلاعات، اگر به عملکرد 99 درصد نیاز داشتید، احتمالاً باید 100 درخواست ارسال کنید. این مثال خوبی از جایی است که ذخیره‌سازی متن می‌تواند هزینه‌های مرتبط با استفاده از مدل‌های Gemini را به میزان قابل توجهی کاهش دهد و در عین حال عملکرد بالا را حفظ کند.\n\n##  سوالات متداول \n\n###  بهترین مکان برای قرار دادن پرس و جو در پنجره زمینه کجاست؟\n\n در بیشتر موارد، به خصوص اگر متن کل طولانی باشد، اگر پرس و ",
      "tags": [
        "context",
        "long-context",
        "tokens"
      ],
      "category": "context-management"
    },
    {
      "slug": "google-tokens",
      "title": "Tokens",
      "description": "Home\n        \n  \n  \n      \n    \n  \n  \n  \n    \n      \n      \n    \n    \n    \n      \n        \n  \n    \n          Gemini API\n        \n  \n  \n      \n    \n  \n  \n  \n    \n      \n      \n    \n    \n    \n      \n  ...",
      "source": "google",
      "sourceLabel": "Google AI",
      "sourceColor": "#4285f4",
      "url": "https://ai.google.dev/gemini-api/docs/tokens",
      "content": "Home\n        \n  \n  \n      \n    \n  \n  \n  \n    \n      \n      \n    \n    \n    \n      \n        \n  \n    \n          Gemini API\n        \n  \n  \n      \n    \n  \n  \n  \n    \n      \n      \n    \n    \n    \n      \n        \n  \n    \n          Docs\n        \n  \n  \n      \n    \n  \n  \n\n    \n      \n    \n    \n  \n    \n  \n  \n    \n\n  \n  \n    \n    Send feedback\n  \n  \n\n  \n    \n      Understand and count tokens\n    \n  \n      \n    \n  \n  \n\n  \n  \n  \n    \n  \n\n  \n    \n    \n\nGemini and other generative AI models process input and output at a granularity\ncalled a *token*.\n\nFor Gemini models, a token is equivalent to about 4 characters.\n100 tokens is equal to about 60-80 English words.\n\n## About tokens\n\nTokens can be single characters like `z` or whole words like `cat`. Long words\nare broken up into several tokens. The set of all tokens used by the model is\ncalled the vocabulary, and the process of splitting text into tokens is called\n*tokenization*.\n\nWhen billing is enabled, the [cost of a call to the Gemini API](/pricing) is\ndetermined in part by the number of input and output tokens, so knowing how to\ncount tokens can be helpful.\n\n  Python\n  JavaScript\n  Go\n\n## Try out counting tokens in a Colab\n\nYou can try out counting tokens by using a Colab.\n\n  \n    [View on ai.google.dev](https://ai.google.dev/gemini-api/docs/tokens)\n  \n  \n  \n    [Try a Colab notebook](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Counting_Tokens.ipynb)\n  \n  \n    [View notebook on GitHub](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Counting_Tokens.ipynb)\n  \n\n## Context windows\n\nThe models available through the Gemini API have context windows that are\nmeasured in tokens. The context window defines how much input you can provide\nand how much output the model can generate. You can determine the size of the\ncontext window by calling the [getModels endpoint](/api/rest/v1/models/get) or\nby looking in the [models documentation](/gemini-api/docs/models/gemini).\n\nIn the following example, you can see that the `gemini-2.0-flash` model has an\ninput limit of about 1,000,000 tokens and an output limit of about 8,000 tokens,\nwhich means a context window is 1,000,000 tokens.\n\n```\n`from google import genai\n\nclient = genai.Client()\nmodel_info = client.models.get(model=\"gemini-2.0-flash\")\nprint(f\"{model_info.input_token_limit=}\")\nprint(f\"{model_info.output_token_limit=}\")\n# ( e.g., input_token_limit=30720, output_token_limit=2048 )[count_tokens.py](https://github.com/google-gemini/api-examples/blob/856e8a0f566a2810625cecabba6e2ab1fe97e496/python/count_tokens.py#L25-L31)\n`\n```\n\n## Count tokens\n\nAll input to and output from the Gemini API is tokenized, including text, image\nfiles, and other non-text modalities.\n\nYou can count tokens in the following ways:\n\n### Count text tokens\n\n```\n`from google import genai\n\nclient = genai.Client()\nprompt = \"The quick brown fox jumps over the lazy dog.\"\n\n# Count tokens using the new client method.\ntotal_tokens = client.models.count_tokens(\n    model=\"gemini-2.0-flash\", contents=prompt\n)\nprint(\"total_tokens: \", total_tokens)\n# ( e.g., total_tokens: 10 )\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash\", contents=prompt\n)\n\n# The usage_metadata provides detailed token counts.\nprint(response.usage_metadata)\n# ( e.g., prompt_token_count: 11, candidates_token_count: 73, total_token_count: 84 )[count_tokens.py](https://github.com/google-gemini/api-examples/blob/856e8a0f566a2810625cecabba6e2ab1fe97e496/python/count_tokens.py#L36-L54)\n`\n```\n\n### Count multi-turn (chat) tokens\n\n```\n`from google import genai\nfrom google.genai import types\n\nclient = genai.Client()\n\nchat = client.chats.create(\n    model=\"gemini-2.0-flash\",\n    history=[\n        types.Content(\n            role=\"user\", parts=[types.Part(text=\"Hi my name is Bob\")]\n        ),\n        types.Content(role=\"model\", parts=[types.Part(text=\"Hi Bob!\")]),\n    ],\n)\n# Count tokens for the chat history.\nprint(\n    client.models.count_tokens(\n        model=\"gemini-2.0-flash\", contents=chat.get_history()\n    )\n)\n# ( e.g., total_tokens: 10 )\n\nresponse = chat.send_message(\n    message=\"In one sentence, explain how a computer works to a young child.\"\n)\nprint(response.usage_metadata)\n# ( e.g., prompt_token_count: 25, candidates_token_count: 21, total_token_count: 46 )\n\n# You can count tokens for the combined history and a new message.\nextra = types.UserContent(\n    parts=[\n        types.Part(\n            text=\"What is the meaning of life?\",\n        )\n    ]\n)\nhistory = chat.get_history()\nhistory.append(extra)\nprint(client.models.count_tokens(model=\"gemini-2.0-flash\", contents=history))\n# ( e.g., total_tokens: 56 )[count_tokens.py](https://github.com/google-gemini/api-examples/blob/856e8a0f566a2810625cecabba6e2ab1fe97e496/python/count_tokens.py#L59-L98)\n`\n```\n\n### Count multimodal tokens\n\nAll input to the Gemini API is tokenized, including text, image files, and other\nnon-text modalities. Note the following high-level key points about tokenization\nof multimodal input during processing by the Gemini API:\n\nWith Gemini 2.0, image inputs with both dimensions <=384 pixels are counted as\n258 tokens. Images larger in one or both dimensions are cropped and scaled as\nneeded into tiles of 768x768 pixels, each counted as 258 tokens. Prior to Gemini\n2.0, images used a fixed 258 tokens.\n\nVideo and audio files are converted to tokens at the following fixed rates:\nvideo at 263 tokens per second and audio at 32 tokens per second.\n\n#### Media resolutions\n\nGemini 3 Pro Preview introduces granular control over multimodal vision processing with the\n`media_resolution` parameter. The `media_resolution` parameter determines the\n**maximum number of tokens allocated per input image or video frame.**\nHigher resolutions improve the model's ability to\nread fine text or identify small details, but increase token usage and latency.\n\nFor more details about the parameter and how it can impact token calculations,\nsee the [media resolution](/gemini-api/docs/media-resolution) guide.\n\n#### Image files\n\n**Note:** You'll get the same token count if you use a file uploaded using the\nFile API or you provide the file as inline data.\nExample that uses an uploaded image from the File API:\n\n```\n`from google import genai\n\nclient = genai.Client()\nprompt = \"Tell me about this image\"\nyour_image_file = client.files.upload(file=media / \"organ.jpg\")\n\nprint(\n    client.models.count_tokens(\n        model=\"gemini-2.0-flash\", contents=[prompt, your_image_file]\n    )\n)\n# ( e.g., total_tokens: 263 )\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash\", contents=[prompt, your_image_file]\n)\nprint(response.usage_metadata)\n# ( e.g., prompt_token_count: 264, candidates_token_count: 80, total_token_count: 345 )[count_tokens.py](https://github.com/google-gemini/api-examples/blob/856e8a0f566a2810625cecabba6e2ab1fe97e496/python/count_tokens.py#L127-L144)\n`\n```\n\nExample that provides the image as inline data:\n\n```\n`from google import genai\nimport PIL.Image\n\nclient = genai.Client()\nprompt = \"Tell me about this image\"\nyour_image_file = PIL.Image.open(media / \"organ.jpg\")\n\n# Count tokens for combined text and inline image.\nprint(\n    client.models.count_tokens(\n        model=\"gemini-2.0-flash\", contents=[prompt, your_image_file]\n    )\n)\n# ( e.g., total_tokens: 263 )\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash\", contents=[prompt, your_image_file]\n)\nprint(response.usage_metadata)\n# ( e.g., prompt_token_count: 264, candidates_token_count: 80, total_token_count: 345 )[count_tokens.py](https://github.com/google-gemini/api-examples/blob/856e8a0f566a2810625cecabba6e2ab1fe97e496/python/count_tokens.py#L103-L122)\n`\n```\n\n#### Video or audio files\n\nAudio and video are each converted to tokens at the following fixed rates:\n\n- Video: 263 tokens per second\n\n- Audio: 32 tokens per second\n\n**Note:** You'll get the same token count if you use a file uploaded using the File\nAPI or you provide the file as inline data.\n```\n`from google import genai\nimport time\n\nclient = genai.Client()\nprompt = \"Tell me about this video\"\nyour_file = client.files.upload(file=media / \"Big_Buck_Bunny.mp4\")\n\n# Poll until the video file is completely processed (state becomes ACTIVE).\nwhile not your_file.state or your_file.state.name != \"ACTIVE\":\n    print(\"Processing video...\")\n    print(\"File state:\", your_file.state)\n    time.sleep(5)\n    your_file = client.files.get(name=your_file.name)\n\nprint(\n    client.models.count_tokens(\n        model=\"gemini-2.0-flash\", contents=[prompt, your_file]\n    )\n)\n# ( e.g., total_tokens: 300 )\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash\", contents=[prompt, your_file]\n)\nprint(response.usage_metadata)\n# ( e.g., prompt_token_count: 301, candidates_token_count: 60, total_token_count: 361 )[count_tokens.py](https://github.com/google-gemini/api-examples/blob/856e8a0f566a2810625cecabba6e2ab1fe97e496/python/count_tokens.py#L149-L174)\n`\n```\n\n### System instructions and tools\n\nSystem instructions and tools also count towards the total token count for the\ninput.\n\nIf you use system instructions, the `total_tokens` count increases to\nreflect the addition of `system_instruction`.\n\nIf you use function calling, the `total_tokens` count increases to reflect the\naddition of `tools`.\n\n  \n\n  \n\n  \n    \n    \n      \n    \n    \n  \n       \n         \n\n  \n  \n    \n    Send feedback\n  \n  \n\n       \n    \n    \n  \n\n  \n\n  Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\n  Last updated 2025-12-18 UTC.\n\n  \n\n  \n    \n    \n    \n      \n\n  \n  \n    Need to tell us more?\n  \n  \n\n    \n  \n  \n",
      "tags": [
        "tokens",
        "counting",
        "usage"
      ],
      "category": "token-optimization"
    },
    {
      "slug": "google-prompting-strategies",
      "title": "Prompting Strategies",
      "description": "Strona główna\n        \n  \n  \n      \n    \n  \n  \n  \n    \n      \n      \n    \n    \n    \n      \n        \n  \n    \n          Gemini API\n        \n  \n  \n      \n    \n  \n  \n  \n    \n      \n      \n    \n    \n    \n...",
      "source": "google",
      "sourceLabel": "Google AI",
      "sourceColor": "#4285f4",
      "url": "https://ai.google.dev/gemini-api/docs/prompting-strategies",
      "content": "Strona główna\n        \n  \n  \n      \n    \n  \n  \n  \n    \n      \n      \n    \n    \n    \n      \n        \n  \n    \n          Gemini API\n        \n  \n  \n      \n    \n  \n  \n  \n    \n      \n      \n    \n    \n    \n      \n        \n  \n    \n          Dokumenty\n        \n  \n  \n      \n    \n  \n  \n\n    \n      \n    \n    \n  \n    \n  \n  \n    \n\n  \n  \n    \n    Prześlij opinię\n  \n  \n\n  \n    \n      Strategie projektowania promptów\n    \n  \n      \n    \n  \n  \n\n  \n  \n  \n    \n  \n\n  \n    \n    \n    \n\n*Projektowanie promptów* to proces tworzenia promptów, czyli zapytań w języku naturalnym, które pozwalają uzyskać dokładne i wysokiej jakości odpowiedzi od modelu językowego.\n\nNa tej stronie znajdziesz podstawowe koncepcje, strategie i sprawdzone metody, które pomogą Ci zacząć projektować prompty, aby w pełni wykorzystać możliwości modeli AI Gemini.\n\n**Uwaga:** tworzenie promptów to proces iteracyjny. Te wytyczne i szablony stanowią punkt wyjścia. Eksperymentuj i dostosowuj ustawienia do konkretnych przypadków użycia i obserwowanych odpowiedzi modelu.\n## Przewodniki po promptach dotyczące konkretnych tematów\n\nSzukasz bardziej szczegółowych strategii dotyczących promptów? Zapoznaj się z naszymi innymi przewodnikami po promptach:\n\n- [Promptowanie za pomocą plików multimedialnych](https://ai.google.dev/gemini-api/docs/files?hl=pl#prompt-guide)\n\n- Promptowanie w przypadku generowania obrazów za pomocą [Imagen](https://ai.google.dev/gemini-api/docs/imagen?hl=pl#imagen-prompt-guide) i [natywnego generowania obrazów w Gemini](https://ai.google.dev/gemini-api/docs/image-generation?hl=pl#prompt-guide)\n\n- [Promptowanie w celu generowania filmów](https://ai.google.dev/gemini-api/docs/video?hl=pl#prompt-guide)\n\nWięcej przykładowych promptów znajdziesz w [galerii promptów](https://ai.google.dev/gemini-api/prompts?hl=pl), która w interaktywny sposób prezentuje wiele koncepcji omówionych w tym przewodniku.\n\n## jasne i konkretne instrukcje,\n\nSkutecznym i wydajnym sposobem dostosowywania zachowania modelu jest podawanie mu jasnych i konkretnych instrukcji. Instrukcje mogą mieć formę pytania, zadań krok po kroku lub bardziej złożoną, np. mapowania doświadczeń i nastawienia użytkownika.\n\n### Dane wejściowe\n\nDane wejściowe to wymagany tekst w prompcie, na który model ma udzielić odpowiedzi. Dane wejściowe mogą być pytaniem, na które model odpowiada (dane wejściowe w postaci pytania), zadaniem, które model wykonuje (dane wejściowe w postaci zadania), obiektem, na którym model działa (dane wejściowe w postaci obiektu), lub częściowymi danymi wejściowymi, które model uzupełnia lub kontynuuje (dane wejściowe w postaci uzupełnienia).\n\n   \n     **Typ danych wejściowych**\n     **Prompt**\n     **Wygenerowane dane wyjściowe**\n   \n\n  \n    Pytanie\n    \n    \n```\n`\n    What's a good name for a flower shop that specializes in selling bouquets of\n    dried flowers? Create a list of 5 options with just the names.`\n```\n\n    \n    \n```\n`\n    Here are 5 names for a flower shop specializing in dried flowers:\n\n    1. Everlasting Blooms\n    2. Dried & Delightful\n    3. The Petal Preserve\n    4. Whispers of Wildflowers\n    5. Forever Blooms\n    `\n```\n\n    \n  \n  \n    Zadanie\n    \n    \n```\n\n    Give me a simple list of just the things that I must bring on a camping trip. The list should have 5 items.\n```\n\n    \n    \n```\n\n    1. **Tent:** Your shelter from the elements.\n    2. **Sleeping Bag:** Stay warm and comfortable.\n    3. **Sleeping Pad:** Insulation and cushioning for a good night's sleep.\n    4. **Headlamp or Flashlight:** Light for nighttime activities.\n    5. **First Aid Kit:** Be prepared for minor injuries.\n```\n\n    \n  \n    \n    Jednostka\n    \n    \n```\n`\n    Classify the following items as [large, small]:\nElephant\nMouse\nSnail`\n```\n\n    \n    \n```\n`\nHere's the classification:\n\n* **Elephant:** Large\n* **Mouse:** Small\n* **Snail:** Small\n    `\n```\n\n    \n  \n\n#### Częściowe uzupełnianie danych wejściowych\n\nGeneratywne modele językowe działają jak zaawansowane narzędzie do autouzupełniania. Gdy podasz część treści, model może dostarczyć resztę treści lub to, co uważa za jej kontynuację, w formie odpowiedzi. Jeśli podasz przykłady lub kontekst, model może je uwzględnić.\n\nPoniższy przykład zawiera prompt z instrukcją i danymi wejściowymi dotyczącymi jednostki:\n\n  **Prompt:**\n  \n```\n\nFor the given order, return a JSON object that has the fields cheeseburger, hamburger, fries, or\ndrink, with the value being the quantity.\n\nOrder: A burger and a drink.\n  \n```\n\n  **Odpowiedź:**\n  \n```\n\n{\n  \"cheeseburger\": 0,\n  \"hamburger\": 1,\n  \"fries\": 0,\n  \"drink\": 1\n}\n  \n```\n\n  (gemini-2.5-flash)\n\nModel wykonał zadanie zgodnie z prompterem, ale zapisanie instrukcji w języku naturalnym może być czasami trudne i pozostawia wiele miejsca na interpretację modelu.\nNa przykład menu restauracji może zawierać wiele pozycji. Aby zmniejszyć rozmiar odpowiedzi JSON, prawdopodobnie chcesz pominąć elementy, które nie zostały zamówione. W tym przypadku możesz podać przykład i [prefiks odpowiedzi](#prefixes), a model dokończy resztę:\n\n  **Prompt:**\n  \n```\n\nValid fields are cheeseburger, hamburger, fries, and drink.\nOrder: Give me a cheeseburger and fries\nOutput:\n```\n{\n  \"cheeseburger\": 1,\n  \"fries\": 1\n}\n```\nOrder: I want two burgers, a drink, and fries.\nOutput:\n  \n```\n\n  **Odpowiedź:**\n  \n```\n\n```\n{\n  \"hamburger\": 2,\n  \"drink\": 1,\n  \"fries\": 1\n}\n```\n  \n```\n\n  (gemini-2.5-flash)\n\nZwróć uwagę, że „cheeseburger” został wykluczony z danych wyjściowych, ponieważ nie był częścią zamówienia.\n\n### Ograniczenia\n\nOkreśl wszelkie ograniczenia dotyczące odczytywania prompta lub generowania odpowiedzi. Możesz powiedzieć modelowi, co ma robić, a czego nie. Na przykład w prompcie możesz określić ograniczenie dotyczące długości podsumowania:\n\n  \n  \n  \n    **Prompt:**\n    \n\n```\n\n**Summarize this text in one sentence:**\nText: A quantum computer exploits quantum mechanical phenomena to perform calculations exponentially\nfaster than any modern traditional computer. At very tiny scales, physical matter acts as both\nparticles and as waves, and quantum computing uses specialized hardware to leverage this behavior.\nThe operating principles of quantum devices is beyond the scope of classical physics. When deployed\nat scale, quantum computers could be used in a wide variety of applications such as: in\ncybersecurity to break existing encryption methods while helping researchers create new ones, in\nmeteorology to develop better weather forecasting etc. However, the current state of the art quantum\ncomputers are still largely experimental and impractical.\n    \n```\n\n  \n\n  \n    **Odpowiedź:**\n    \n\n```\n\n      Exploiting quantum mechanical phenomena, quantum computers can perform calculations exponentially faster than traditional computers for potential applications like cybersecurity and meteorology, although they are currently largely experimental and impractical.\n    \n```\n\n    (gemini-2.5-flash)\n  \n\n  \n  \n\n### Format odpowiedzi\n\nMożesz przekazać instrukcje określające format odpowiedzi. Możesz na przykład poprosić o sformatowanie odpowiedzi jako tabeli, listy punktowanej, krótkiej prezentacji, słów kluczowych, zdania lub akapitu. Ta instrukcja systemowa nakazuje modelowi udzielać bardziej konwersacyjnych odpowiedzi:\n\n  **Instrukcja systemowa**\n  \n```\n\n    All questions should be answered comprehensively with details, unless the user requests a concise response specifically.\n  \n```\n\n  **Prompt**\n  \n```\n\n    What is a smart way to make a business that sells DVD's in 2025?\n  \n```\n\n  **Odpowiedź:**\n  \n```\n\nStarting a business selling DVDs in 2025 presents significant challenges due to the overwhelming dominance of digital streaming and on-demand services; a \"smart\" approach would therefore require a highly specialized niche, a strong value proposition beyond just the disc, and likely a focus on community or collector markets rather than mainstream consumption.\n\nHere's a breakdown of what a smart DVD business in 2025 would likely entail:\n\n1. Extreme Niche Specialization: You cannot compete with streaming libraries. Your business must focus on what digital doesn't offer easily or at all. This could include:\n\n    * Out-of-Print & Rare Titles: Sourcing films or TV shows that have never been released digitally or are currently unavailable on major streaming platforms.\n    ....\n  \n```\n\n  (gemini-2.5-flash)\n\n#### Formatowanie odpowiedzi za pomocą strategii uzupełniania\n\n[Strategia uzupełniania](#completion) może też pomóc w formatowaniu odpowiedzi.\nTen przykład promptu zachęca model do utworzenia konspektu eseju:\n\n  **Prompt:**\n  \n```\n\nCreate an outline for an essay about hummingbirds.\n  \n```\n\n  **Odpowiedź:**\n  \n```\n\nHere is a possible outline for an essay about hummingbirds:\n\nI. Introduction\nA. Hook: Start with a captivating fact or description about hummingbirds (e.g., their speed, iridescent colors, or tiny size).\nB. Background: Briefly introduce hummingbirds – where they are found (Americas), their reputation as unique birds.\nC. Thesis Statement: State the main point of the essay, which could be about the remarkable adaptations, behaviors, and ecological importance of hummingbirds.\n\nII. Appearance and Identification\n.....\n  \n```\n\n  (gemini-2.5-flash)\n\nPrompt nie określał formatu konspektu, więc model wybrał go za Ciebie. Aby model zwrócił konspekt w określonym formacie, możesz dodać tekst, który będzie stanowił początek konspektu, i pozwolić modelowi dokończyć go na podstawie wzorca, który został przez Ciebie zainicjowany.\n\n  **Prompt:**\n  \n```\n\nCreate an outline for an essay about hummingbirds.\nI. Introduction\n   *\n  \n```\n\n  **Odpowiedź:**\n  \n```\n\nOkay, here is an outline for an essay about hummingbirds, starting from the introduction you provided:\n\nI. Introduction\n* Hook: Start with a captivating fact or description about hummingbirds (e.g., their speed, iridescent colors, or tiny size).\n* Background: Briefly introduce hummingbirds – where they are found (Americas), their reputation as unique birds, and their general characteristics.\n* Thesis Statement: State ",
      "tags": [
        "prompts",
        "strategies",
        "engineering"
      ],
      "category": "prompt-engineering"
    },
    {
      "slug": "google-system-instructions",
      "title": "System Instructions",
      "description": "Erste Schritte mit der Gemini API für die Entwicklung von Chat- und Textgenerierungs-Apps",
      "source": "google",
      "sourceLabel": "Google AI",
      "sourceColor": "#4285f4",
      "url": "https://ai.google.dev/gemini-api/docs/system-instructions",
      "content": "Startseite\n        \n  \n  \n      \n    \n  \n  \n  \n    \n      \n      \n    \n    \n    \n      \n        \n  \n    \n          Gemini API\n        \n  \n  \n      \n    \n  \n  \n  \n    \n      \n      \n    \n    \n    \n      \n        \n  \n    \n          Dokumentation\n        \n  \n  \n      \n    \n  \n  \n\n    \n      \n    \n    \n  \n    \n  \n  \n    \n\n  \n  \n    \n    Feedback geben\n  \n  \n\n  \n    \n      Textgenerierung\n    \n  \n      \n    \n  \n  \n\n  \n  \n  \n    \n  \n\n  \n    \n    \n    \n\nDie Gemini API kann Textausgabe aus Text-, Bild-, Video- und Audioeingaben generieren.\n\nHier ein einfaches Beispiel:\n\n### Python\n\n```\n`from google import genai\n\nclient = genai.Client()\n\nresponse = client.models.generate_content(\n    model=\"gemini-3-flash-preview\",\n    contents=\"How does AI work?\"\n)\nprint(response.text)\n`\n```\n\n### JavaScript\n\n```\n`import { GoogleGenAI } from \"@google/genai\";\n\nconst ai = new GoogleGenAI({});\n\nasync function main() {\n  const response = await ai.models.generateContent({\n    model: \"gemini-3-flash-preview\",\n    contents: \"How does AI work?\",\n  });\n  console.log(response.text);\n}\n\nawait main();\n`\n```\n\n### Ok\n\n```\n`package main\n\nimport (\n  \"context\"\n  \"fmt\"\n  \"os\"\n  \"google.golang.org/genai\"\n)\n\nfunc main() {\n\n  ctx := context.Background()\n  client, err := genai.NewClient(ctx, nil)\n  if err != nil {\n      log.Fatal(err)\n  }\n\n  result, _ := client.Models.GenerateContent(\n      ctx,\n      \"gemini-3-flash-preview\",\n      genai.Text(\"Explain how AI works in a few words\"),\n      nil,\n  )\n\n  fmt.Println(result.Text())\n}\n`\n```\n\n### Java\n\n```\n`import com.google.genai.Client;\nimport com.google.genai.types.GenerateContentResponse;\n\npublic class GenerateContentWithTextInput {\n  public static void main(String[] args) {\n\n    Client client = new Client();\n\n    GenerateContentResponse response =\n        client.models.generateContent(\"gemini-3-flash-preview\", \"How does AI work?\", null);\n\n    System.out.println(response.text());\n  }\n}\n`\n```\n\n### REST\n\n```\n`curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-3-flash-preview:generateContent\" \\\n  -H \"x-goog-api-key: $GEMINI_API_KEY\" \\\n  -H 'Content-Type: application/json' \\\n  -X POST \\\n  -d '{\n    \"contents\": [\n      {\n        \"parts\": [\n          {\n            \"text\": \"How does AI work?\"\n          }\n        ]\n      }\n    ]\n  }'\n`\n```\n\n### Apps Script\n\n```\n`// See https://developers.google.com/apps-script/guides/properties\n// for instructions on how to set the API key.\nconst apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n\nfunction main() {\n  const payload = {\n    contents: [\n      {\n        parts: [\n          { text: 'How AI does work?' },\n        ],\n      },\n    ],\n  };\n\n  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-3-flash-preview:generateContent';\n  const options = {\n    method: 'POST',\n    contentType: 'application/json',\n    headers: {\n      'x-goog-api-key': apiKey,\n    },\n    payload: JSON.stringify(payload)\n  };\n\n  const response = UrlFetchApp.fetch(url, options);\n  const data = JSON.parse(response);\n  const content = data['candidates'][0]['content']['parts'][0]['text'];\n  console.log(content);\n}\n`\n```\n\n## Mit Gemini Ideen entwickeln\n\nBei Gemini-Modellen ist [„Thinking“](https://ai.google.dev/gemini-api/docs/thinking?hl=de) (Denken) oft standardmäßig aktiviert. Dadurch kann das Modell Schlussfolgerungen ziehen, bevor es auf eine Anfrage antwortet.\n\nJedes Modell unterstützt unterschiedliche Denkkonfigurationen, sodass Sie Kosten, Latenz und Intelligenz steuern können. Weitere Informationen finden Sie im [Leitfaden zum Denken](https://ai.google.dev/gemini-api/docs/thinking?hl=de#set-budget).\n\n### Python\n\n```\n`from google import genai\nfrom google.genai import types\n\nclient = genai.Client()\n\nresponse = client.models.generate_content(\n    model=\"gemini-3-flash-preview\",\n    contents=\"How does AI work?\",\n    config=types.GenerateContentConfig(\n        thinking_config=types.ThinkingConfig(thinking_level=\"low\")\n    ),\n)\nprint(response.text)\n`\n```\n\n### JavaScript\n\n```\n`import { GoogleGenAI, ThinkingLevel } from \"@google/genai\";\n\nconst ai = new GoogleGenAI({});\n\nasync function main() {\n  const response = await ai.models.generateContent({\n    model: \"gemini-3-flash-preview\",\n    contents: \"How does AI work?\",\n    config: {\n      thinkingConfig: {\n        thinkingLevel: ThinkingLevel.LOW,\n      },\n    }\n  });\n  console.log(response.text);\n}\n\nawait main();\n`\n```\n\n### Ok\n\n```\n`package main\n\nimport (\n  \"context\"\n  \"fmt\"\n  \"os\"\n  \"google.golang.org/genai\"\n)\n\nfunc main() {\n\n  ctx := context.Background()\n  client, err := genai.NewClient(ctx, nil)\n  if err != nil {\n      log.Fatal(err)\n  }\n\n  thinkingLevelVal := \"low\"\n\n  result, _ := client.Models.GenerateContent(\n      ctx,\n      \"gemini-3-flash-preview\",\n      genai.Text(\"How does AI work?\"),\n      &genai.GenerateContentConfig{\n        ThinkingConfig: &genai.ThinkingConfig{\n            ThinkingLevel: &thinkingLevelVal,\n        },\n      }\n  )\n\n  fmt.Println(result.Text())\n}\n`\n```\n\n### Java\n\n```\n`import com.google.genai.Client;\nimport com.google.genai.types.GenerateContentConfig;\nimport com.google.genai.types.GenerateContentResponse;\nimport com.google.genai.types.ThinkingConfig;\nimport com.google.genai.types.ThinkingLevel;\n\npublic class GenerateContentWithThinkingConfig {\n  public static void main(String[] args) {\n\n    Client client = new Client();\n\n    GenerateContentConfig config =\n        GenerateContentConfig.builder()\n            .thinkingConfig(ThinkingConfig.builder().thinkingLevel(new ThinkingLevel(\"low\")))\n            .build();\n\n    GenerateContentResponse response =\n        client.models.generateContent(\"gemini-3-flash-preview\", \"How does AI work?\", config);\n\n    System.out.println(response.text());\n  }\n}\n`\n```\n\n### REST\n\n```\n`curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-3-flash-preview:generateContent\" \\\n  -H \"x-goog-api-key: $GEMINI_API_KEY\" \\\n  -H 'Content-Type: application/json' \\\n  -X POST \\\n  -d '{\n    \"contents\": [\n      {\n        \"parts\": [\n          {\n            \"text\": \"How does AI work?\"\n          }\n        ]\n      }\n    ],\n    \"generationConfig\": {\n      \"thinkingConfig\": {\n        \"thinkingLevel\": \"low\"\n      }\n    }\n  }'\n`\n```\n\n### Apps Script\n\n```\n`// See https://developers.google.com/apps-script/guides/properties\n// for instructions on how to set the API key.\nconst apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n\nfunction main() {\n  const payload = {\n    contents: [\n      {\n        parts: [\n          { text: 'How AI does work?' },\n        ],\n      },\n    ],\n    generationConfig: {\n      thinkingConfig: {\n        thinkingLevel: 'low'\n      }\n    }\n  };\n\n  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-3-flash-preview:generateContent';\n  const options = {\n    method: 'POST',\n    contentType: 'application/json',\n    headers: {\n      'x-goog-api-key': apiKey,\n    },\n    payload: JSON.stringify(payload)\n  };\n\n  const response = UrlFetchApp.fetch(url, options);\n  const data = JSON.parse(response);\n  const content = data['candidates'][0]['content']['parts'][0]['text'];\n  console.log(content);\n}\n`\n```\n\n## Systemanweisungen und andere Konfigurationen\n\nMit Systemanweisungen können Sie das Verhalten von Gemini-Modellen steuern. Übergeben Sie dazu ein [`GenerateContentConfig`](https://ai.google.dev/api/generate-content?hl=de#v1beta.GenerationConfig)-Objekt.\n\n### Python\n\n```\n`from google import genai\nfrom google.genai import types\n\nclient = genai.Client()\n\nresponse = client.models.generate_content(\n    model=\"gemini-3-flash-preview\",\n    config=types.GenerateContentConfig(\n        system_instruction=\"You are a cat. Your name is Neko.\"),\n    contents=\"Hello there\"\n)\n\nprint(response.text)\n`\n```\n\n### JavaScript\n\n```\n`import { GoogleGenAI } from \"@google/genai\";\n\nconst ai = new GoogleGenAI({});\n\nasync function main() {\n  const response = await ai.models.generateContent({\n    model: \"gemini-3-flash-preview\",\n    contents: \"Hello there\",\n    config: {\n      systemInstruction: \"You are a cat. Your name is Neko.\",\n    },\n  });\n  console.log(response.text);\n}\n\nawait main();\n`\n```\n\n### Ok\n\n```\n`package main\n\nimport (\n  \"context\"\n  \"fmt\"\n  \"os\"\n  \"google.golang.org/genai\"\n)\n\nfunc main() {\n\n  ctx := context.Background()\n  client, err := genai.NewClient(ctx, nil)\n  if err != nil {\n      log.Fatal(err)\n  }\n\n  config := &genai.GenerateContentConfig{\n      SystemInstruction: genai.NewContentFromText(\"You are a cat. Your name is Neko.\", genai.RoleUser),\n  }\n\n  result, _ := client.Models.GenerateContent(\n      ctx,\n      \"gemini-3-flash-preview\",\n      genai.Text(\"Hello there\"),\n      config,\n  )\n\n  fmt.Println(result.Text())\n}\n`\n```\n\n### Java\n\n```\n`import com.google.genai.Client;\nimport com.google.genai.types.Content;\nimport com.google.genai.types.GenerateContentConfig;\nimport com.google.genai.types.GenerateContentResponse;\nimport com.google.genai.types.Part;\n\npublic class GenerateContentWithSystemInstruction {\n  public static void main(String[] args) {\n\n    Client client = new Client();\n\n    GenerateContentConfig config =\n        GenerateContentConfig.builder()\n            .systemInstruction(\n                Content.fromParts(Part.fromText(\"You are a cat. Your name is Neko.\")))\n            .build();\n\n    GenerateContentResponse response =\n        client.models.generateContent(\"gemini-3-flash-preview\", \"Hello there\", config);\n\n    System.out.println(response.text());\n  }\n}\n`\n```\n\n### REST\n\n```\n`curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-3-flash-preview:generateContent\" \\\n  -H \"x-goog-api-key: $GEMINI_API_KEY\" \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"system_instruction\": {\n      \"parts\": [\n        {\n          \"text\": \"You are a cat. Your name is Neko.\"\n        }\n      ]\n    },\n    \"contents\": [\n      {\n        \"parts\": [\n          {\n            \"text\": \"Hello there\"\n          }\n        ]\n      }\n    ]\n  }'\n`\n```\n\n### Apps Script\n\n```\n`// See https://developers.google.com/a",
      "tags": [
        "system-prompts",
        "instructions",
        "engineering"
      ],
      "category": "prompt-engineering"
    },
    {
      "slug": "google-code-execution",
      "title": "Code Execution",
      "description": "জেমিনি এপিআই কোড এক্সিকিউশন ফিচার কীভাবে ব্যবহার করবেন তা জানুন।",
      "source": "google",
      "sourceLabel": "Google AI",
      "sourceColor": "#4285f4",
      "url": "https://ai.google.dev/gemini-api/docs/code-execution",
      "content": "এই পৃষ্ঠাটি [Cloud Translation API](//cloud.google.com/translate/?hl=bn) অনুবাদ করেছে।\n          \n        \n        \n      \n    \n  \n\n  \n    \n    \n    \n  \n  \n    \n    \n    \n      \n        \n  \n    \n          হোম\n        \n  \n  \n      \n    \n  \n  \n  \n    \n      \n      \n    \n    \n    \n      \n        \n  \n    \n          Gemini API\n        \n  \n  \n      \n    \n  \n  \n  \n    \n      \n      \n    \n    \n    \n      \n        \n  \n    \n          ডক্স\n        \n  \n  \n      \n    \n  \n  \n\n    \n      \n    \n    \n  \n    \n  \n  \n    \n\n  \n  \n    \n    মতামত জানান\n  \n  \n\n  \n    \n      কোড এক্সিকিউশন\n    \n  \n      \n    \n  \n  \n\n  \n  \n  \n    \n  \n\n  \n    \n    \n\nজেমিনি এপিআই একটি কোড এক্সিকিউশন টুল প্রদান করে যা মডেলটিকে পাইথন কোড তৈরি এবং চালাতে সক্ষম করে। এরপর মডেলটি কোড এক্সিকিউশন ফলাফল থেকে পুনরাবৃত্তিমূলকভাবে শিখতে পারে যতক্ষণ না এটি চূড়ান্ত আউটপুটে পৌঁছায়। আপনি কোড এক্সিকিউশন ব্যবহার করে এমন অ্যাপ্লিকেশন তৈরি করতে পারেন যা কোড-ভিত্তিক যুক্তি থেকে উপকৃত হয়। উদাহরণস্বরূপ, আপনি সমীকরণ সমাধান করতে বা পাঠ্য প্রক্রিয়া করতে কোড এক্সিকিউশন ব্যবহার করতে পারেন। আপনি আরও বিশেষায়িত কাজ সম্পাদনের জন্য কোড এক্সিকিউশন পরিবেশে অন্তর্ভুক্ত [লাইব্রেরিগুলিও](#supported-libraries) ব্যবহার করতে পারেন।\n\n জেমিনি কেবল পাইথনে কোড এক্সিকিউট করতে সক্ষম। আপনি এখনও জেমিনিকে অন্য ভাষায় কোড জেনারেট করতে বলতে পারেন, কিন্তু মডেলটি কোড এক্সিকিউটেশন টুল ব্যবহার করে এটি চালাতে পারবে না।\n\n##  কোড এক্সিকিউশন সক্ষম করুন\n\n কোড এক্সিকিউশন সক্ষম করতে, মডেলটিতে কোড এক্সিকিউশন টুলটি কনফিগার করুন। এটি মডেলটিকে কোড তৈরি এবং চালানোর অনুমতি দেয়। \n\n###  পাইথন \n\n```\n`from google import genai\nfrom google.genai import types\n\nclient = genai.Client()\n\nresponse = client.models.generate_content(\n    model=\"gemini-3-flash-preview\",\n    contents=\"What is the sum of the first 50 prime numbers? \"\n    \"Generate and run code for the calculation, and make sure you get all 50.\",\n    config=types.GenerateContentConfig(\n        tools=[types.Tool(code_execution=types.ToolCodeExecution)]\n    ),\n)\n\nfor part in response.candidates[0].content.parts:\n    if part.text is not None:\n        print(part.text)\n    if part.executable_code is not None:\n        print(part.executable_code.code)\n    if part.code_execution_result is not None:\n        print(part.code_execution_result.output)\n`\n```\n### জাভাস্ক্রিপ্ট \n\n```\n`import { GoogleGenAI } from \"@google/genai\";\n\nconst ai = new GoogleGenAI({});\n\nlet response = await ai.models.generateContent({\n  model: \"gemini-3-flash-preview\",\n  contents: [\n    \"What is the sum of the first 50 prime numbers? \" +\n      \"Generate and run code for the calculation, and make sure you get all 50.\",\n  ],\n  config: {\n    tools: [{ codeExecution: {} }],\n  },\n});\n\nconst parts = response?.candidates?.[0]?.content?.parts || [];\nparts.forEach((part) => {\n  if (part.text) {\n    console.log(part.text);\n  }\n\n  if (part.executableCode && part.executableCode.code) {\n    console.log(part.executableCode.code);\n  }\n\n  if (part.codeExecutionResult && part.codeExecutionResult.output) {\n    console.log(part.codeExecutionResult.output);\n  }\n});\n`\n```\n###  যাও \n\n```\n`package main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"os\"\n    \"google.golang.org/genai\"\n)\n\nfunc main() {\n\n    ctx := context.Background()\n    client, err := genai.NewClient(ctx, nil)\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    config := &genai.GenerateContentConfig{\n        Tools: []*genai.Tool{\n            {CodeExecution: &genai.ToolCodeExecution{}},\n        },\n    }\n\n    result, _ := client.Models.GenerateContent(\n        ctx,\n        \"gemini-3-flash-preview\",\n        genai.Text(\"What is the sum of the first 50 prime numbers? \" +\n                  \"Generate and run code for the calculation, and make sure you get all 50.\"),\n        config,\n    )\n\n    fmt.Println(result.Text())\n    fmt.Println(result.ExecutableCode())\n    fmt.Println(result.CodeExecutionResult())\n}\n`\n```\n###  বিশ্রাম \n\n```\n`curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-3-flash-preview:generateContent\" \\\n-H \"x-goog-api-key: $GEMINI_API_KEY\" \\\n-H 'Content-Type: application/json' \\\n-d ' {\"tools\": [{\"code_execution\": {}}],\n    \"contents\": {\n      \"parts\":\n        {\n            \"text\": \"What is the sum of the first 50 prime numbers? Generate and run code for the calculation, and make sure you get all 50.\"\n        }\n    },\n}'\n`\n```\n **দ্রষ্টব্য:** এই REST উদাহরণটি JSON প্রতিক্রিয়াকে বিশ্লেষণ করে না যেমনটি উদাহরণ আউটপুটে দেখানো হয়েছে। আউটপুটটি দেখতে নিচের মতো হতে পারে, যা পঠনযোগ্যতার জন্য ফর্ম্যাট করা হয়েছে:\n\n```\n`Okay, I need to calculate the sum of the first 50 prime numbers. Here's how I'll\napproach this:\n\n1.  **Generate Prime Numbers:** I'll use an iterative method to find prime\n    numbers. I'll start with 2 and check if each subsequent number is divisible\n    by any number between 2 and its square root. If not, it's a prime.\n2.  **Store Primes:** I'll store the prime numbers in a list until I have 50 of\n    them.\n3.  **Calculate the Sum:**  Finally, I'll sum the prime numbers in the list.\n\nHere's the Python code to do this:\n\ndef is_prime(n):\n  &#34;&#34;&#34;Efficiently checks if a number is prime.&#34;&#34;&#34;\n  if n <= 1:\n    return False\n  if n <= 3:\n    return True\n  if n % 2 == 0 or n % 3 == 0:\n    return False\n  i = 5\n  while i * i <= n:\n    if n % i == 0 or n % (i + 2) == 0:\n      return False\n    i += 6\n  return True\n\nprimes = []\nnum = 2\nwhile len(primes) < 50:\n  if is_prime(num):\n    primes.append(num)\n  num += 1\n\nsum_of_primes = sum(primes)\nprint(f'{primes=}')\nprint(f'{sum_of_primes=}')\n\nprimes=[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67,\n71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151,\n157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229]\nsum_of_primes=5117\n\nThe sum of the first 50 prime numbers is 5117.\n`\n```\n এই আউটপুটটি বেশ কয়েকটি কন্টেন্ট অংশকে একত্রিত করে যা কোড এক্সিকিউশন ব্যবহার করার সময় মডেলটি ফেরত দেয়:\n\n-  `text` : মডেল দ্বারা তৈরি ইনলাইন টেক্সট\n-  `executableCode` : যে মডেলটি কার্যকর করার জন্য তৈরি করা হয়েছে তার দ্বারা তৈরি কোড\n-  `codeExecutionResult` : এক্সিকিউটেবল কোডের ফলাফল\n এই অংশগুলির নামকরণের নিয়মগুলি প্রোগ্রামিং ভাষা অনুসারে পরিবর্তিত হয়।\n\n##  ছবি সহ কোড এক্সিকিউশন (মিথুন 3)\n\n জেমিনি ৩ ফ্ল্যাশ মডেলটি এখন পাইথন কোড লিখতে এবং কার্যকর করতে পারে যাতে সক্রিয়ভাবে ছবিগুলি পরিচালনা এবং পরিদর্শন করা যায়।\n\n **ব্যবহারের ক্ষেত্রে**\n\n-  **জুম করুন এবং পরিদর্শন করুন** : মডেলটি যখন বিবরণ খুব ছোট হয় (যেমন, দূরবর্তী গেজ পড়া) তখন অন্তর্নিহিতভাবে সনাক্ত করে এবং উচ্চ রেজোলিউশনে এলাকাটি ক্রপ এবং পুনরায় পরীক্ষা করার জন্য কোড লেখে।\n-  **ভিজ্যুয়াল ম্যাথ** : মডেলটি কোড ব্যবহার করে বহু-পদক্ষেপ গণনা চালাতে পারে (যেমন, একটি রসিদে লাইন আইটেমগুলির সারসংক্ষেপ)।\n-  **ছবির টীকা** : মডেলটি প্রশ্নের উত্তর দেওয়ার জন্য ছবি টীকা করতে পারে, যেমন সম্পর্ক দেখানোর জন্য তীর আঁকা।\n **দ্রষ্টব্য:** যদিও মডেলটি ছোট ছোট বিবরণের জন্য স্বয়ংক্রিয়ভাবে জুমিং পরিচালনা করে, আপনার এটিকে অন্যান্য কাজের জন্য কোড ব্যবহার করার জন্য স্পষ্টভাবে অনুরোধ করা উচিত, যেমন \"গিয়ারের সংখ্যা গণনা করার জন্য কোড লিখুন\" অথবা \"এই ছবিটিকে সোজা করে ঘোরান\"।###  ছবি সহ কোড এক্সিকিউশন সক্ষম করুন\n\n জেমিনি ৩ ফ্ল্যাশে ছবি সহ কোড এক্সিকিউশন আনুষ্ঠানিকভাবে সমর্থিত। আপনি কোড এক্সিকিউশনকে একটি টুল হিসেবে এবং চিন্তাভাবনা উভয়ই সক্ষম করে এই আচরণটি সক্রিয় করতে পারেন। \n\n###  পাইথন \n\n```\n`from google import genai\nfrom google.genai import types\nimport requests\nfrom PIL import Image\nimport io\n\nimage_path = \"https://goo.gle/instrument-img\"\nimage_bytes = requests.get(image_path).content\nimage = types.Part.from_bytes(\n  data=image_bytes, mime_type=\"image/jpeg\"\n)\n\n# Ensure you have your API key set\nclient = genai.Client()\n\nresponse = client.models.generate_content(\n    model=\"gemini-3-flash-preview\",\n    contents=[image, \"Zoom into the expression pedals and tell me how many pedals are there?\"],\n    config=types.GenerateContentConfig(\n        tools=[types.Tool(code_execution=types.ToolCodeExecution)]\n    ),\n)\n\nfor part in response.candidates[0].content.parts:\n    if part.text is not None:\n        print(part.text)\n    if part.executable_code is not None:\n        print(part.executable_code.code)\n    if part.code_execution_result is not None:\n        print(part.code_execution_result.output)\n    if part.as_image() is not None:\n        # display() is a standard function in Jupyter/Colab notebooks\n        display(Image.open(io.BytesIO(part.as_image().image_bytes)))\n`\n```\n###  জাভাস্ক্রিপ্ট \n\n```\n`async function main() {\n  const ai = new GoogleGenAI({ });\n\n  // 1. Prepare Image Data\n  const imageUrl = \"https://goo.gle/instrument-img\";\n  const response = await fetch(imageUrl);\n  const imageArrayBuffer = await response.arrayBuffer();\n  const base64ImageData = Buffer.from(imageArrayBuffer).toString('base64');\n\n  // 2. Call the API with Code Execution enabled\n  const result = await ai.models.generateContent({\n    model: \"gemini-3-flash-preview\",\n    contents: [\n      {\n        inlineData: {\n          mimeType: 'image/jpeg',\n          data: base64ImageData,\n        },\n      },\n      { text: \"Zoom into the expression pedals and tell me how many pedals are there?\" }\n    ],\n    config: {\n      tools: [{ codeExecution: {} }],\n    },\n  });\n\n  // 3. Process the response (Text, Code, and Execution Results)\n  const candidates = result.candidates;\n  if (candidates && candidates[0].content.parts) {\n    for (const part of candidates[0].content.parts) {\n      if (part.text) {\n        console.log(\"Text:\", part.text);\n      }\n      if (part.executableCode) {\n        console.log(`\\nGenerated Code (${part.executableCode.language}):\\n`, part.executableCode.code);\n      }\n      if (part.codeExecutionResult) {\n        console.log(`\\nExecution Output (${part.codeExecutionResult.outcome}):\\n`, part.codeExecutionResult.output);\n      }\n    }\n  }\n}\n\nmain();\n`\n```\n###  যাও \n\n```\n`package main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"io\"\n    \"log\"\n    \"net/http\"\n    \"os\"\n\n    \"google.golang.org/genai\"\n)\n\nfunc main() {\n    ctx := context.Background()\n    // Initialize Client (Reads GEMINI_API_KEY from env)\n    client, err := genai.NewClient(ctx, nil)\n    if err",
      "tags": [
        "code",
        "execution",
        "tools"
      ],
      "category": "tool-use"
    },
    {
      "slug": "builtin-progressive-disclosure",
      "title": "Progressive Disclosure",
      "description": "Instead of loading an entire codebase—which would immediately overwhelm the attention budget—modern agents use JIT context. The assistant dynamically loads only the necessary data at runtime.",
      "source": "builtin",
      "sourceLabel": "Built-in",
      "sourceColor": "#e3120b",
      "url": "",
      "content": "# Progressive Disclosure\n\nInstead of loading an entire codebase—which would immediately overwhelm the attention budget—modern agents use JIT context. The assistant dynamically loads only the necessary data at runtime.",
      "tags": [
        "context",
        "jit",
        "optimization"
      ],
      "category": "context-management"
    },
    {
      "slug": "builtin-lightweight-identifiers",
      "title": "Lightweight Identifiers",
      "description": "The assistant maintains references (file paths, stored queries) and dynamically loads only the necessary data at runtime using tools like grep, head, or tail.",
      "source": "builtin",
      "sourceLabel": "Built-in",
      "sourceColor": "#e3120b",
      "url": "",
      "content": "# Lightweight Identifiers\n\nThe assistant maintains references (file paths, stored queries) and dynamically loads only the necessary data at runtime using tools like grep, head, or tail.",
      "tags": [
        "context",
        "references",
        "efficiency"
      ],
      "category": "context-management"
    },
    {
      "slug": "builtin-compaction",
      "title": "Compaction",
      "description": "When a session nears its token limit, the assistant summarizes critical details—such as architectural decisions and unresolved bugs—while discarding redundant tool outputs.",
      "source": "builtin",
      "sourceLabel": "Built-in",
      "sourceColor": "#e3120b",
      "url": "",
      "content": "# Compaction\n\nWhen a session nears its token limit, the assistant summarizes critical details—such as architectural decisions and unresolved bugs—while discarding redundant tool outputs.",
      "tags": [
        "context",
        "compression",
        "long-horizon"
      ],
      "category": "context-management"
    },
    {
      "slug": "builtin-tool-result-clearing",
      "title": "Tool Result Clearing",
      "description": "A light touch form of compaction where the raw results of previous tool calls (like long terminal outputs) are cleared to save space.",
      "source": "builtin",
      "sourceLabel": "Built-in",
      "sourceColor": "#e3120b",
      "url": "",
      "content": "# Tool Result Clearing\n\nA light touch form of compaction where the raw results of previous tool calls (like long terminal outputs) are cleared to save space.",
      "tags": [
        "context",
        "tools",
        "optimization"
      ],
      "category": "context-management"
    },
    {
      "slug": "builtin-structured-note-taking",
      "title": "Structured Note-taking",
      "description": "The agent may maintain an external NOTES.md or a to-do list to track dependencies and progress across thousands of steps, which it can read back into its context after a reset.",
      "source": "builtin",
      "sourceLabel": "Built-in",
      "sourceColor": "#e3120b",
      "url": "",
      "content": "# Structured Note-taking\n\nThe agent may maintain an external NOTES.md or a to-do list to track dependencies and progress across thousands of steps, which it can read back into its context after a reset.",
      "tags": [
        "context",
        "persistence",
        "notes"
      ],
      "category": "context-management"
    },
    {
      "slug": "builtin-distractors",
      "title": "Distractors",
      "description": "Files or code snippets that are topically related to the query but do not contain the answer can cause the model to lose focus or hallucinate.",
      "source": "builtin",
      "sourceLabel": "Built-in",
      "sourceColor": "#e3120b",
      "url": "",
      "content": "# Distractors\n\nFiles or code snippets that are topically related to the query but do not contain the answer can cause the model to lose focus or hallucinate.",
      "tags": [
        "context",
        "pollution",
        "relevance"
      ],
      "category": "context-management"
    },
    {
      "slug": "builtin-context-rot",
      "title": "Context Rot",
      "description": "As more tokens are added, the model's ability to accurately retrieve needles of information from the haystack of the codebase decreases.",
      "source": "builtin",
      "sourceLabel": "Built-in",
      "sourceColor": "#e3120b",
      "url": "",
      "content": "# Context Rot\n\nAs more tokens are added, the model's ability to accurately retrieve needles of information from the haystack of the codebase decreases.",
      "tags": [
        "context",
        "degradation",
        "tokens"
      ],
      "category": "context-management"
    },
    {
      "slug": "builtin-xml-tagging",
      "title": "XML Tagging",
      "description": "Use tags like <background_information>, <tool_guidance>, <constraints> to clearly separate different types of instructions in system prompts.",
      "source": "builtin",
      "sourceLabel": "Built-in",
      "sourceColor": "#e3120b",
      "url": "",
      "content": "# XML Tagging\n\nUse tags like <background_information>, <tool_guidance>, <constraints> to clearly separate different types of instructions in system prompts.",
      "tags": [
        "prompts",
        "xml",
        "structure"
      ],
      "category": "prompt-engineering"
    },
    {
      "slug": "builtin-high-signal-tokens",
      "title": "High-Signal Tokens",
      "description": "The objective is to provide the smallest possible set of high-signal tokens that maximize the likelihood of the correct code generation.",
      "source": "builtin",
      "sourceLabel": "Built-in",
      "sourceColor": "#e3120b",
      "url": "",
      "content": "# High-Signal Tokens\n\nThe objective is to provide the smallest possible set of high-signal tokens that maximize the likelihood of the correct code generation.",
      "tags": [
        "tokens",
        "optimization",
        "quality"
      ],
      "category": "token-optimization"
    },
    {
      "slug": "builtin-structural-patterns",
      "title": "Structural Patterns",
      "description": "Research suggests that models often perform better on shuffled or unstructured context than on logically structured haystacks, impacting how they process long files.",
      "source": "builtin",
      "sourceLabel": "Built-in",
      "sourceColor": "#e3120b",
      "url": "",
      "content": "# Structural Patterns\n\nResearch suggests that models often perform better on shuffled or unstructured context than on logically structured haystacks, impacting how they process long files.",
      "tags": [
        "context",
        "structure",
        "research"
      ],
      "category": "context-management"
    }
  ],
  "fetchedAt": "2026-02-07T19:25:49.113Z",
  "stats": {
    "total": 44,
    "bySource": {
      "arxiv": 15,
      "openai": 5,
      "anthropic": 8,
      "google": 6,
      "builtin": 10
    },
    "byCategory": {
      "general": 5,
      "token-optimization": 7,
      "tool-use": 2,
      "caching": 3,
      "prompt-engineering": 12,
      "rag": 4,
      "context-management": 11
    }
  }
}